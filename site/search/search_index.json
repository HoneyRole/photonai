{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"How to start Installation You only need two things: Python 3 and your favourite Python IDE to get started. Then simply install via pip. # pip install photonai Build your pipeline 1. Setup New Analysis Start by creating a new Hyperpipe instance and giving it a name to identify your analysis. You can also set parameters to declare the output folder, where to save the results and the final model. ( pipe = Hyperpipe ( 'basic_pipe' , output_settings = OutputSettings ( project_folder = 'path/to/project' )) [comment]: < > ( ) [comment]: < > ( ) [comment]: < > (pipe = Hyperpipe('basic_pipe',) [comment]: < > ( output_settings=OutputSettings(project_folder='path/to/project'))) [comment]: < > ( ) 2. Customize the training and testing process Now you can select parameters to customize the training, hyperparameter optimization and testing procedure. Particularly, you can choose the hyperparameter optimization strategy, set parameters, choose performance metrics and choose the performance metric to minimize or maximize, respectively. pipe = Hyperpipe('basic_pipe', output_settings=OutputSettings(project_folder='path/to/project')) # choose hyperparameter optimization strategy optimizer='random_grid_search', # PHOTONAI automatically calculates your preferred metrics metrics=['mean_squared_error', 'pearson_correlation', 'mean_absolute_error', 'explained_variance'], # this metrics selects the best hyperparameter configuration # in this case mean squared error is minimized best_config_metric='mean_squared_error', # select cross validation strategies outer_cv=ShuffleSplit(n_splits=3, test_size=0.2), inner_cv=KFold(n_splits=10), 3. Build custom pipeline Select and arrange normalization, dimensionality reduction, feature selection, data augmentation, over- or undersampling algorithms in simple or parallel data streams. You can integrate custom algorithms or choose from our wide range of pre-registered algorithms from established toolboxes. # access the scikit-learn implementations via keywords # at the same time define the hyperparameters to optimize for each element pipe += PipelineElement('StandardScaler') pipe += PipelineElement('PCA', hyperparameters={'n_components': FloatRange(0.5, 0.8, step=0.1)}) pipe += PipelineElement('RandomForestRegressor', hyperparameters={'n_samples_split': IntegerRange(2, 10)}) 4. Feed Data and Train Load your data and start the (nested-) cross-validated hyperparameter optimization, training and evaluation procedure. You will see an extensive output to monitor the hyperparameter optimization progress, see the results and track the best performances so far. X, y = load_boston(return_X_y=True) pipe.fit(X, y) 5. Visualize Results Finally, you find all results conveniently stored in the output folder. You have a final model which is trained with the best hyperparameter configuration found to share and apply to new data. In addition, you find a .json file where all results from the training, optimization and testing procedure are stored. Take that file and drag it into the Explorer . You will see all available information conveniently visualized.","title":"Introduction"},{"location":"api/architecture/","text":"Architecture","title":"Architecture"},{"location":"api/architecture/#architecture","text":"","title":"Architecture"},{"location":"api/hyperpipe/","text":"Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. __init__ ( self , mongodb_connect_url = None , save_output = True , overwrite_results = False , generate_best_model = True , user_id = '' , wizard_object_id = '' , wizard_project_name = '' , project_folder = '' ) special Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"Hyperpipe"},{"location":"api/hyperpipe/#photonai.base.hyperpipe.OutputSettings","text":"Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances.","title":"photonai.base.hyperpipe.OutputSettings"},{"location":"api/hyperpipe/#photonai.base.hyperpipe.OutputSettings.__init__","text":"Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"__init__()"},{"location":"api/base/branch/","text":"Documentation for Branch A substream of pipeline elements that is encapsulated e.g. for parallelization. Examples: from photonai.base import Branch from photonai.optimization import IntegerRange tree_qua_branch = Branch('tree_branch') tree_qua_branch += PipelineElement('QuantileTransformer', n_quantiles=100) tree_qua_branch += PipelineElement('DecisionTreeClassifier', {'min_samples_split': IntegerRange(2, 4)}, criterion='gini') __iadd__ ( self , pipe_element ) special Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The object to add, being either a transformer or an estimator. \"\"\" super ( Branch , self ) . __iadd__ ( pipe_element ) self . _prepare_pipeline () return self __init__ ( self , name , elements = None ) special Initialize the object. Parameters: Name Type Description Default name str Name of the encapsulated item and/or summary of the encapsulated element`s functions. required elements List[photonai.base.photon_elements.PipelineElement] List of elements added one after another to the Branch. None Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None ): \"\"\" Initialize the object. Parameters: name: Name of the encapsulated item and/or summary of the encapsulated element`s functions. elements: List of elements added one after another to the Branch. \"\"\" super () . __init__ ( name , {}, test_disabled = False , disabled = False , base_element = True ) # in case any of the children needs y or covariates we need to request them self . needs_y = True self . needs_covariates = True self . elements = [] self . has_hyperparameters = True self . skip_caching = True self . identifier = \"BRANCH:\" # needed for caching on individual level self . fix_fold_id = False self . do_not_delete_cache_folder = False # add elements if elements : for element in elements : self . add ( element ) add ( self , pipe_element ) Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The object to add, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) fit ( self , X , y = None , ** kwargs ) Calls the fit function on all underlying base elements. Parameters: Name Type Description Default X ndarray The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function on all underlying base elements. Parameters: X: The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" self . base_element = Branch . sanity_check_pipeline ( self . base_element ) return super () . fit ( X , y , ** kwargs ) predict ( self , X , ** kwargs ) Calls the predict function on underlying base elements. Parameters: Name Type Description Default X The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y The truth array-like values with shape=[N], where N is the number of samples. required kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description ndarray Prediction. Source code in photonai/base/photon_elements.py def predict ( self , X , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements fit. Returns: Prediction. \"\"\" return super () . predict ( X , ** kwargs ) transform ( self , X , y = None , ** kwargs ) Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_elements predict/transform. {} Returns: Type Description ndarray Transformed/Predicted data. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements predict/transform. Returns: Transformed/Predicted data. \"\"\" if self . _estimator_type == 'classifier' or self . _estimator_type == 'regressor' : return super () . predict ( X ), y , kwargs return super () . transform ( X , y , ** kwargs )","title":"Branch"},{"location":"api/base/branch/#documentation-for-branch","text":"","title":"Documentation for Branch"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch","text":"A substream of pipeline elements that is encapsulated e.g. for parallelization. Examples: from photonai.base import Branch from photonai.optimization import IntegerRange tree_qua_branch = Branch('tree_branch') tree_qua_branch += PipelineElement('QuantileTransformer', n_quantiles=100) tree_qua_branch += PipelineElement('DecisionTreeClassifier', {'min_samples_split': IntegerRange(2, 4)}, criterion='gini')","title":"photonai.base.photon_elements.Branch"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.__iadd__","text":"Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The object to add, being either a transformer or an estimator. \"\"\" super ( Branch , self ) . __iadd__ ( pipe_element ) self . _prepare_pipeline () return self","title":"__iadd__()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.__init__","text":"Initialize the object. Parameters: Name Type Description Default name str Name of the encapsulated item and/or summary of the encapsulated element`s functions. required elements List[photonai.base.photon_elements.PipelineElement] List of elements added one after another to the Branch. None Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None ): \"\"\" Initialize the object. Parameters: name: Name of the encapsulated item and/or summary of the encapsulated element`s functions. elements: List of elements added one after another to the Branch. \"\"\" super () . __init__ ( name , {}, test_disabled = False , disabled = False , base_element = True ) # in case any of the children needs y or covariates we need to request them self . needs_y = True self . needs_covariates = True self . elements = [] self . has_hyperparameters = True self . skip_caching = True self . identifier = \"BRANCH:\" # needed for caching on individual level self . fix_fold_id = False self . do_not_delete_cache_folder = False # add elements if elements : for element in elements : self . add ( element )","title":"__init__()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.add","text":"Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The object to add, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element )","title":"add()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.fit","text":"Calls the fit function on all underlying base elements. Parameters: Name Type Description Default X ndarray The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function on all underlying base elements. Parameters: X: The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" self . base_element = Branch . sanity_check_pipeline ( self . base_element ) return super () . fit ( X , y , ** kwargs )","title":"fit()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.predict","text":"Calls the predict function on underlying base elements. Parameters: Name Type Description Default X The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y The truth array-like values with shape=[N], where N is the number of samples. required kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description ndarray Prediction. Source code in photonai/base/photon_elements.py def predict ( self , X , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements fit. Returns: Prediction. \"\"\" return super () . predict ( X , ** kwargs )","title":"predict()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.transform","text":"Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_elements predict/transform. {} Returns: Type Description ndarray Transformed/Predicted data. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements predict/transform. Returns: Transformed/Predicted data. \"\"\" if self . _estimator_type == 'classifier' or self . _estimator_type == 'regressor' : return super () . predict ( X ), y , kwargs return super () . transform ( X , y , ** kwargs )","title":"transform()"},{"location":"api/base/hyperpipe/","text":"Documentation for Hyperpipe The PHOTONAI Hyperpipe class enables you to create a custom pipeline. In addition it defines the relevant analysis\u2019 parameters such as the cross-validation scheme, the hyperparameter optimization strategy, and the performance metrics of interest. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods and algorithms and arranging them with the PHOTONAI classes, both simple and complex pipeline architectures can be designed rapidly. The PHOTONAI Hyperpipe automatizes the nested training, test and hyperparameter optimization procedures. The Hyperpipe monitors: the nested-cross-validated training and test procedure, communicates with the hyperparameter optimization strategy, streams information between the pipeline elements, logs all results obtained and evaluates the performance. guides the hyperparameter optimization process by a so-called best config metric which is used to select the best performing hyperparameter configuration. Attributes: Name Type Description optimum_pipe PhotonPipeline An sklearn pipeline object that is fitted to the training data according to the best hyperparameter configuration found. Currently, we don't create an ensemble of all best hyperparameter configs over all folds. We find the best config by comparing the test error across outer folds. The hyperparameter config of the best fold is used as the optimal model and is then trained on the complete set. best_config dict Dictionary containing the hyperparameters of the best configuration. Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value. results MDBHyperpipe Object containing all information about the for the performed hyperparameter search. Holds the training and test metrics for all outer folds, inner folds and configurations, as well as additional information. elements list Contains `all PipelineElement or Hyperpipe objects that are added to the pipeline. Examples: from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.model_selection import ShuffleSplit , KFold from sklearn.datasets import load_breast_cancer X , y = load_breast_cancer ( return_X_y = True ) hyperpipe = Hyperpipe ( 'myPipe' , optimizer = 'timeboxed_random_grid_search' , optimizer_params = { 'limit_in_minutes' : 2 }, outer_cv = ShuffleSplit ( test_size = 0.2 , n_splits = 3 ), inner_cv = KFold ( n_splits = 10 , shuffle = True ), metrics = [ 'accuracy' , 'precision' , 'recall' , \"f1_score\" ], best_config_metric = 'accuracy' , eval_final_performance = True , verbosity = 0 ) hyperpipe += PipelineElement ( \"SVC\" , hyperparameters = { \"C\" : FloatRange ( 1 , 100 )}) hyperpipe . fit ( X , y ) __init__ ( self , name , inner_cv = None , outer_cv = None , optimizer = 'grid_search' , optimizer_params = None , metrics = None , best_config_metric = None , use_test_set = True , test_size = 0.2 , project_folder = '' , calculate_metrics_per_fold = True , calculate_metrics_across_folds = False , random_seed = None , verbosity = 0 , learning_curves = False , learning_curves_cut = None , output_settings = None , performance_constraints = None , permutation_id = None , cache_folder = None , nr_of_processes = 1 , allow_multidim_targets = False ) special Initialize the object. Parameters: Name Type Description Default name Optional[str] Name of hyperpipe instance. required inner_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to test hyperparameter configurations, generates the validation set. None outer_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to use for the hyperparameter search itself, generates the test set. None optimizer str Hyperparameter optimization algorithm. In case a string literal is given: \"grid_search\": optimizer that iteratively tests all possible hyperparameter combinations \"random_grid_search\": a variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations \"timeboxed_random_grid_search\": randomly chooses hyperparameter combinations from the set of all possible hyperparameter combinations and tests until the given time limit is reached limit_in_minutes : int \"sk_opt\": Scikit-Optimize based on theories of Baysian optimization. \"random_search\": randomly chooses hyperparameter from grid-free domain. \"smac\": SMAC based on theories of Baysian optimization. \"nevergrad\": Nevergrad based on theories of evolutionary learning. In case an object is given: expects the object to have the following methods: next_config_generator : returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding model_name__parameter_name: parameter_value prepare : takes a list of pipeline elements and their particular hyperparameters to test evaluate_recent_performance : gets a tested config and the respective performance in order to calculate a smart next configuration to process 'grid_search' metrics Optional[List[Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str]]] Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own Metrics for classification : accuracy : sklearn.metrics.accuracy_score matthews_corrcoef : sklearn.metrics.matthews_corrcoef confusion_matrix : sklearn.metrics.confusion_matrix, f1_score : sklearn.metrics.f1_score hamming_loss : sklearn.metrics.hamming_loss log_loss : sklearn.metrics.log_loss precision : sklearn.metrics.precision_score recall : sklearn.metrics.recall_score Metrics for regression : mean_squared_error : sklearn.metrics.mean_squared_error mean_absolute_error : sklearn.metrics.mean_absolute_error explained_variance : sklearn.metrics.explained_variance_score r2 : sklearn.metrics.r2_score Other metrics pearson_correlation : photon_core.framework.Metrics.pearson_correlation variance_explained : photon_core.framework.Metrics.variance_explained_score categorical_accuracy : photon_core.framework.Metrics.categorical_accuracy_score None best_config_metric Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str] The metric that should be maximized or minimized in order to choose the best hyperparameter configuration None use_test_set bool If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. True project_folder str The output folder in which all files generated by the PHOTONAI project are saved to. '' test_size float The amount of the data that should be left out if no outer_cv is given and eval_final_perfomance is set to True. 0.2 calculate_metrics_per_fold bool If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. True calculate_metrics_across_folds bool If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. False random_seed int Random Seed. None verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug 0 learning_curves bool Enables larning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. False learning_curves_cut FloatRange The tested relativ cuts for data size. None performance_constraints list Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. None permutation_id str String identifier for permutation tests. None cache_folder str Folder path for multi-processing. None nr_of_processes int Determined the amount of simultaneous calculation of outer_folds. 1 allow_multidim_targets bool Allows multidimensional targets. False Source code in photonai/base/hyperpipe.py def __init__ ( self , name : Optional [ str ], inner_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits ] = None , outer_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits , None ] = None , optimizer : str = 'grid_search' , optimizer_params : dict = None , metrics : Optional [ List [ Union [ Scorer . Metric_Type , str ]]] = None , best_config_metric : Optional [ Union [ Scorer . Metric_Type , str ]] = None , use_test_set : bool = True , test_size : float = 0.2 , project_folder : str = '' , calculate_metrics_per_fold : bool = True , calculate_metrics_across_folds : bool = False , random_seed : int = None , verbosity : int = 0 , learning_curves : bool = False , learning_curves_cut : FloatRange = None , output_settings : OutputSettings = None , performance_constraints : list = None , permutation_id : str = None , cache_folder : str = None , nr_of_processes : int = 1 , allow_multidim_targets : bool = False ): \"\"\" Initialize the object. Parameters: name: Name of hyperpipe instance. inner_cv: Cross validation strategy to test hyperparameter configurations, generates the validation set. outer_cv: Cross validation strategy to use for the hyperparameter search itself, generates the test set. optimizer: Hyperparameter optimization algorithm. - In case a string literal is given: - \"grid_search\": optimizer that iteratively tests all possible hyperparameter combinations - \"random_grid_search\": a variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations - \"timeboxed_random_grid_search\": randomly chooses hyperparameter combinations from the set of all possible hyperparameter combinations and tests until the given time limit is reached - `limit_in_minutes`: int - \"sk_opt\": Scikit-Optimize based on theories of Baysian optimization. - \"random_search\": randomly chooses hyperparameter from grid-free domain. - \"smac\": SMAC based on theories of Baysian optimization. - \"nevergrad\": Nevergrad based on theories of evolutionary learning. - In case an object is given: expects the object to have the following methods: - `next_config_generator`: returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value` - `prepare`: takes a list of pipeline elements and their particular hyperparameters to test - `evaluate_recent_performance`: gets a tested config and the respective performance in order to calculate a smart next configuration to process metrics: Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own - Metrics for `classification`: - `accuracy`: sklearn.metrics.accuracy_score - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef - `confusion_matrix`: sklearn.metrics.confusion_matrix, - `f1_score`: sklearn.metrics.f1_score - `hamming_loss`: sklearn.metrics.hamming_loss - `log_loss`: sklearn.metrics.log_loss - `precision`: sklearn.metrics.precision_score - `recall`: sklearn.metrics.recall_score - Metrics for `regression`: - `mean_squared_error`: sklearn.metrics.mean_squared_error - `mean_absolute_error`: sklearn.metrics.mean_absolute_error - `explained_variance`: sklearn.metrics.explained_variance_score - `r2`: sklearn.metrics.r2_score - Other metrics - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation - `variance_explained`: photon_core.framework.Metrics.variance_explained_score - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score best_config_metric: The metric that should be maximized or minimized in order to choose the best hyperparameter configuration use_test_set [bool, default=True]: If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. project_folder: The output folder in which all files generated by the PHOTONAI project are saved to. test_size: The amount of the data that should be left out if no outer_cv is given and eval_final_perfomance is set to True. calculate_metrics_per_fold: If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. calculate_metrics_across_folds: If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. random_seed: Random Seed. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug learning_curves: Enables larning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. learning_curves_cut: The tested relativ cuts for data size. performance_constraints: Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. permutation_id: String identifier for permutation tests. cache_folder: Folder path for multi-processing. nr_of_processes: Determined the amount of simultaneous calculation of outer_folds. allow_multidim_targets: Allows multidimensional targets. \"\"\" self . name = re . sub ( r '\\W+' , '' , name ) # ====================== Cross Validation =========================== # check if both calculate_metrics_per_folds and calculate_metrics_across_folds is False if not calculate_metrics_across_folds and not calculate_metrics_per_fold : raise NotImplementedError ( \"Apparently, you've set calculate_metrics_across_folds=False and \" \"calculate_metrics_per_fold=False. In this case PHOTONAI does not calculate \" \"any metrics which doesn't make any sense. Set at least one to True.\" ) if inner_cv is None : msg = \"PHOTONAI requires an inner_cv split. Please enable inner cross-validation. As exmaple: Hyperpipe(...\" \\ \" inner_cv = KFold(n_splits = 3), ...). Ensure you import the cross_validation object first.\" logger . error ( msg ) raise AttributeError ( msg ) # use default cut 'FloatRange(0, 1, 'range', 0.2)' if learning_curves = True but learning_curves_cut is None if learning_curves and learning_curves_cut is None : learning_curves_cut = FloatRange ( 0 , 1 , 'range' , 0.2 ) elif not learning_curves and learning_curves_cut is not None : learning_curves_cut = None self . cross_validation = Hyperpipe . CrossValidation ( inner_cv = inner_cv , outer_cv = outer_cv , use_test_set = use_test_set , test_size = test_size , calculate_metrics_per_fold = calculate_metrics_per_fold , calculate_metrics_across_folds = calculate_metrics_across_folds , learning_curves = learning_curves , learning_curves_cut = learning_curves_cut ) # ====================== Data =========================== self . data = Hyperpipe . Data () # ====================== Output Folder and Log File Management =========================== if output_settings : self . output_settings = output_settings else : self . output_settings = OutputSettings () if project_folder == '' : self . project_folder = os . getcwd () else : self . project_folder = project_folder self . output_settings . set_project_folder ( self . project_folder ) # update output options to add pipe name and timestamp to results folder self . _verbosity = 0 self . verbosity = verbosity self . output_settings . set_log_file () # ====================== Result Logging =========================== self . results_handler = None self . results = None self . best_config = None # ====================== Pipeline =========================== self . elements = [] self . _pipe = None self . optimum_pipe = None self . preprocessing = None # ====================== Performance Optimization =========================== if optimizer_params is None : optimizer_params = {} self . optimization = Optimization ( metrics = metrics , best_config_metric = best_config_metric , optimizer_input = optimizer , optimizer_params = optimizer_params , performance_constraints = performance_constraints ) self . optimization . sanity_check_metrics () # ====================== Caching and Parallelization =========================== self . nr_of_processes = nr_of_processes if cache_folder : self . cache_folder = os . path . join ( cache_folder , self . name ) else : self . cache_folder = None # ====================== Internals =========================== self . permutation_id = permutation_id self . allow_multidim_targets = allow_multidim_targets self . is_final_fit = False # ====================== Random Seed =========================== self . random_state = random_seed if random_seed is not None : import random random . seed ( random_seed ) add ( self , pipe_element ) Add an element to the machine learning pipeline Returns self Parameters: Name Type Description Default pipe_element PipelineElement The object to add to the machine learning pipeline, being either a transformer or an estimator. required Source code in photonai/base/hyperpipe.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline Returns self Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) copy_me ( self ) Helper function to copy an entire Hyperpipe Returns: Type Description Hyperpipe Source code in photonai/base/hyperpipe.py def copy_me ( self ): \"\"\" Helper function to copy an entire Hyperpipe Returns: Hyperpipe \"\"\" signature = inspect . getfullargspec ( OutputSettings . __init__ )[ 0 ] settings = OutputSettings () for attr in signature : if hasattr ( self . output_settings , attr ): setattr ( settings , attr , getattr ( self . output_settings , attr )) self . output_settings . initialize_log_file () # create new Hyperpipe instance pipe_copy = Hyperpipe ( name = self . name , inner_cv = deepcopy ( self . cross_validation . inner_cv ), outer_cv = deepcopy ( self . cross_validation . outer_cv ), best_config_metric = self . optimization . best_config_metric , metrics = self . optimization . metrics , optimizer = self . optimization . optimizer_input_str , optimizer_params = self . optimization . optimizer_params , project_folder = self . project_folder , output_settings = settings ) signature = inspect . getfullargspec ( self . __init__ )[ 0 ] for attr in signature : if hasattr ( self , attr ) and attr != 'output_settings' : setattr ( pipe_copy , attr , getattr ( self , attr )) if hasattr ( self , 'preprocessing' ) and self . preprocessing : preprocessing = Preprocessing () for element in self . preprocessing . elements : preprocessing += element . copy_me () pipe_copy += preprocessing if hasattr ( self , 'elements' ): for element in self . elements : pipe_copy += element . copy_me () return pipe_copy fit ( self , data , targets , ** kwargs ) Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) requests new configurations from the hyperparameter search strategy, the optimizer, initializes the testing of a specific configuration, communicates the result to the optimizer, repeats 2-4 until optimizer delivers no more configurations to test finally searches for the best config in all tested configs, trains the pipeline with the best config and evaluates the performance on the test set Parameters: Name Type Description Default data ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required targets ndarray The truth array-like values with shape=[N], where N is the number of samples. required kwargs Keyword arguments, passed to Outer_Fold_Manager.fit. {} Returns: Type Description Fitted Hyperpipe. Source code in photonai/base/hyperpipe.py def fit ( self , data : np . ndarray , targets : np . ndarray , ** kwargs ): \"\"\" Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: 1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) 2. requests new configurations from the hyperparameter search strategy, the optimizer, 3. initializes the testing of a specific configuration, 4. communicates the result to the optimizer, 5. repeats 2-4 until optimizer delivers no more configurations to test 6. finally searches for the best config in all tested configs, 7. trains the pipeline with the best config and evaluates the performance on the test set Parameters: data: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. targets: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to Outer_Fold_Manager.fit. Returns: Fitted Hyperpipe. \"\"\" # switch to result output folder start = datetime . datetime . now () self . output_settings . update_settings ( self . name , start . strftime ( \"%Y-%m- %d _%H-%M-%S\" )) logger . photon_system_log ( '=' * 101 ) logger . photon_system_log ( 'PHOTONAI ANALYSIS: ' + self . name ) logger . photon_system_log ( '=' * 101 ) logger . info ( \"Preparing data and PHOTONAI objects for analysis...\" ) # loop over outer cross validation if self . nr_of_processes > 1 : hyperpipe_client = Client ( threads_per_worker = 1 , n_workers = self . nr_of_processes , processes = False ) try : # check data self . data . input_data_sanity_checks ( data , targets , ** kwargs ) # create photon pipeline self . _prepare_pipeline () # initialize the progress monitors self . _prepare_result_logging ( start ) # apply preprocessing self . preprocess_data () if not self . is_final_fit : # Outer Folds outer_folds = FoldInfo . generate_folds ( self . cross_validation . outer_cv , self . data . X , self . data . y , self . data . kwargs , self . cross_validation . use_test_set , self . cross_validation . test_size ) self . cross_validation . outer_folds = { f . fold_id : f for f in outer_folds } delayed_jobs = [] # Run Dummy Estimator dummy_estimator = self . _prepare_dummy_estimator () if self . cache_folder is not None : logger . info ( \"Removing cache files...\" ) CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) # loop over outer cross validation for i , outer_f in enumerate ( outer_folds ): # 1. generate OuterFolds Object outer_fold = MDBOuterFold ( fold_nr = outer_f . fold_nr ) outer_fold_computer = OuterFoldManager ( self . _pipe , self . optimization , outer_f . fold_id , self . cross_validation , cache_folder = self . cache_folder , cache_updater = self . recursive_cache_folder_propagation , dummy_estimator = dummy_estimator , result_obj = outer_fold ) # 2. monitor outputs self . results . outer_folds . append ( outer_fold ) if self . nr_of_processes > 1 : result = dask . delayed ( Hyperpipe . fit_outer_folds )( outer_fold_computer , self . data . X , self . data . y , self . data . kwargs , self . cache_folder ) delayed_jobs . append ( result ) else : try : # 3. fit outer_fold_computer . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # 4. save outer fold results self . results_handler . save () finally : # 5. clear cache CacheManager . clear_cache_files ( self . cache_folder ) if self . nr_of_processes > 1 : dask . compute ( * delayed_jobs ) self . results_handler . save () # evaluate hyperparameter optimization results for best config self . _finalize_optimization () # clear complete cache ? CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) ############################################################################################### else : self . preprocess_data () self . _pipe . fit ( self . data . X , self . data . y , ** kwargs ) except Exception as e : logger . error ( e ) logger . error ( traceback . format_exc ()) traceback . print_exc () raise e finally : if self . nr_of_processes > 1 : hyperpipe_client . close () return self inverse_transform_pipeline ( self , hyperparameters , data , targets , data_to_inverse ) Inverse transform data for a pipeline with specific hyperparameter configuration. Copy Sklearn Pipeline, Set Parameters Fit Pipeline to data and targets Inverse transform data with that pipeline Parameters: Name Type Description Default hyperparameters dict The concrete configuration settings for the pipeline elements. required data ndarray The training data to which the pipeline is fitted. required targets ndarray The truth values for training. required data_to_inverse ndarray The data that should be inversed after training. required Returns: Type Description ndarray Inversed data as array. Source code in photonai/base/hyperpipe.py def inverse_transform_pipeline ( self , hyperparameters : dict , data : np . ndarray , targets : np . ndarray , data_to_inverse : np . ndarray ) -> np . ndarray : \"\"\" Inverse transform data for a pipeline with specific hyperparameter configuration. 1. Copy Sklearn Pipeline, 2. Set Parameters 3. Fit Pipeline to data and targets 4. Inverse transform data with that pipeline Parameters: hyperparameters: The concrete configuration settings for the pipeline elements. data: The training data to which the pipeline is fitted. targets: The truth values for training. data_to_inverse: The data that should be inversed after training. Returns: Inversed data as array. \"\"\" copied_pipe = self . pipe . copy_me () copied_pipe . set_params ( ** hyperparameters ) copied_pipe . fit ( data , targets ) return copied_pipe . inverse_transform ( data_to_inverse ) load_optimum_pipe ( file , password = None ) staticmethod Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: Name Type Description Default file str File path specifying .photon file to load trained pipeline from zipped file. required password str Passcode for read file. None Returns: Type Description PhotonPipeline Returns pipeline with all trained PipelineElements. Source code in photonai/base/hyperpipe.py @staticmethod def load_optimum_pipe ( file : str , password : str = None ) -> PhotonPipeline : \"\"\" Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: file: File path specifying .photon file to load trained pipeline from zipped file. password: Passcode for read file. Returns: Returns pipeline with all trained PipelineElements. \"\"\" return PhotonModelPersistor . load_optimum_pipe ( file , password ) predict ( self , data , ** kwargs ) Use the optimum pipe to predict the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required kwargs Keyword arguments, passed to optimum_pipe.predict. {} Returns: Type Description ndarray Predicted targets calculated on input data with trained model. Source code in photonai/base/hyperpipe.py def predict ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Predicted targets calculated on input data with trained model. \"\"\" # Todo: if local_search = true then use optimized pipe here? if self . _pipe : return self . optimum_pipe . predict ( data , ** kwargs ) predict_proba ( self , data , ** kwargs ) Use the optimum pipe to predict the probabilities from the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required kwargs Keyword arguments, passed to optimum_pipe.predict_proba. {} Returns: Type Description ndarray Probabilities calculated from input data on fitted model. Source code in photonai/base/hyperpipe.py def predict_proba ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the probabilities from the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. kwargs: Keyword arguments, passed to optimum_pipe.predict_proba. Returns: Probabilities calculated from input data on fitted model. \"\"\" if self . _pipe : return self . optimum_pipe . predict_proba ( data , ** kwargs )","title":"Hyperpipe"},{"location":"api/base/hyperpipe/#documentation-for-hyperpipe","text":"","title":"Documentation for Hyperpipe"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe","text":"The PHOTONAI Hyperpipe class enables you to create a custom pipeline. In addition it defines the relevant analysis\u2019 parameters such as the cross-validation scheme, the hyperparameter optimization strategy, and the performance metrics of interest. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods and algorithms and arranging them with the PHOTONAI classes, both simple and complex pipeline architectures can be designed rapidly. The PHOTONAI Hyperpipe automatizes the nested training, test and hyperparameter optimization procedures. The Hyperpipe monitors: the nested-cross-validated training and test procedure, communicates with the hyperparameter optimization strategy, streams information between the pipeline elements, logs all results obtained and evaluates the performance. guides the hyperparameter optimization process by a so-called best config metric which is used to select the best performing hyperparameter configuration. Attributes: Name Type Description optimum_pipe PhotonPipeline An sklearn pipeline object that is fitted to the training data according to the best hyperparameter configuration found. Currently, we don't create an ensemble of all best hyperparameter configs over all folds. We find the best config by comparing the test error across outer folds. The hyperparameter config of the best fold is used as the optimal model and is then trained on the complete set. best_config dict Dictionary containing the hyperparameters of the best configuration. Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value. results MDBHyperpipe Object containing all information about the for the performed hyperparameter search. Holds the training and test metrics for all outer folds, inner folds and configurations, as well as additional information. elements list Contains `all PipelineElement or Hyperpipe objects that are added to the pipeline. Examples: from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.model_selection import ShuffleSplit , KFold from sklearn.datasets import load_breast_cancer X , y = load_breast_cancer ( return_X_y = True ) hyperpipe = Hyperpipe ( 'myPipe' , optimizer = 'timeboxed_random_grid_search' , optimizer_params = { 'limit_in_minutes' : 2 }, outer_cv = ShuffleSplit ( test_size = 0.2 , n_splits = 3 ), inner_cv = KFold ( n_splits = 10 , shuffle = True ), metrics = [ 'accuracy' , 'precision' , 'recall' , \"f1_score\" ], best_config_metric = 'accuracy' , eval_final_performance = True , verbosity = 0 ) hyperpipe += PipelineElement ( \"SVC\" , hyperparameters = { \"C\" : FloatRange ( 1 , 100 )}) hyperpipe . fit ( X , y )","title":"photonai.base.hyperpipe.Hyperpipe"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.__init__","text":"Initialize the object. Parameters: Name Type Description Default name Optional[str] Name of hyperpipe instance. required inner_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to test hyperparameter configurations, generates the validation set. None outer_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to use for the hyperparameter search itself, generates the test set. None optimizer str Hyperparameter optimization algorithm. In case a string literal is given: \"grid_search\": optimizer that iteratively tests all possible hyperparameter combinations \"random_grid_search\": a variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations \"timeboxed_random_grid_search\": randomly chooses hyperparameter combinations from the set of all possible hyperparameter combinations and tests until the given time limit is reached limit_in_minutes : int \"sk_opt\": Scikit-Optimize based on theories of Baysian optimization. \"random_search\": randomly chooses hyperparameter from grid-free domain. \"smac\": SMAC based on theories of Baysian optimization. \"nevergrad\": Nevergrad based on theories of evolutionary learning. In case an object is given: expects the object to have the following methods: next_config_generator : returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding model_name__parameter_name: parameter_value prepare : takes a list of pipeline elements and their particular hyperparameters to test evaluate_recent_performance : gets a tested config and the respective performance in order to calculate a smart next configuration to process 'grid_search' metrics Optional[List[Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str]]] Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own Metrics for classification : accuracy : sklearn.metrics.accuracy_score matthews_corrcoef : sklearn.metrics.matthews_corrcoef confusion_matrix : sklearn.metrics.confusion_matrix, f1_score : sklearn.metrics.f1_score hamming_loss : sklearn.metrics.hamming_loss log_loss : sklearn.metrics.log_loss precision : sklearn.metrics.precision_score recall : sklearn.metrics.recall_score Metrics for regression : mean_squared_error : sklearn.metrics.mean_squared_error mean_absolute_error : sklearn.metrics.mean_absolute_error explained_variance : sklearn.metrics.explained_variance_score r2 : sklearn.metrics.r2_score Other metrics pearson_correlation : photon_core.framework.Metrics.pearson_correlation variance_explained : photon_core.framework.Metrics.variance_explained_score categorical_accuracy : photon_core.framework.Metrics.categorical_accuracy_score None best_config_metric Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str] The metric that should be maximized or minimized in order to choose the best hyperparameter configuration None use_test_set bool If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. True project_folder str The output folder in which all files generated by the PHOTONAI project are saved to. '' test_size float The amount of the data that should be left out if no outer_cv is given and eval_final_perfomance is set to True. 0.2 calculate_metrics_per_fold bool If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. True calculate_metrics_across_folds bool If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. False random_seed int Random Seed. None verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug 0 learning_curves bool Enables larning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. False learning_curves_cut FloatRange The tested relativ cuts for data size. None performance_constraints list Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. None permutation_id str String identifier for permutation tests. None cache_folder str Folder path for multi-processing. None nr_of_processes int Determined the amount of simultaneous calculation of outer_folds. 1 allow_multidim_targets bool Allows multidimensional targets. False Source code in photonai/base/hyperpipe.py def __init__ ( self , name : Optional [ str ], inner_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits ] = None , outer_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits , None ] = None , optimizer : str = 'grid_search' , optimizer_params : dict = None , metrics : Optional [ List [ Union [ Scorer . Metric_Type , str ]]] = None , best_config_metric : Optional [ Union [ Scorer . Metric_Type , str ]] = None , use_test_set : bool = True , test_size : float = 0.2 , project_folder : str = '' , calculate_metrics_per_fold : bool = True , calculate_metrics_across_folds : bool = False , random_seed : int = None , verbosity : int = 0 , learning_curves : bool = False , learning_curves_cut : FloatRange = None , output_settings : OutputSettings = None , performance_constraints : list = None , permutation_id : str = None , cache_folder : str = None , nr_of_processes : int = 1 , allow_multidim_targets : bool = False ): \"\"\" Initialize the object. Parameters: name: Name of hyperpipe instance. inner_cv: Cross validation strategy to test hyperparameter configurations, generates the validation set. outer_cv: Cross validation strategy to use for the hyperparameter search itself, generates the test set. optimizer: Hyperparameter optimization algorithm. - In case a string literal is given: - \"grid_search\": optimizer that iteratively tests all possible hyperparameter combinations - \"random_grid_search\": a variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations - \"timeboxed_random_grid_search\": randomly chooses hyperparameter combinations from the set of all possible hyperparameter combinations and tests until the given time limit is reached - `limit_in_minutes`: int - \"sk_opt\": Scikit-Optimize based on theories of Baysian optimization. - \"random_search\": randomly chooses hyperparameter from grid-free domain. - \"smac\": SMAC based on theories of Baysian optimization. - \"nevergrad\": Nevergrad based on theories of evolutionary learning. - In case an object is given: expects the object to have the following methods: - `next_config_generator`: returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value` - `prepare`: takes a list of pipeline elements and their particular hyperparameters to test - `evaluate_recent_performance`: gets a tested config and the respective performance in order to calculate a smart next configuration to process metrics: Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own - Metrics for `classification`: - `accuracy`: sklearn.metrics.accuracy_score - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef - `confusion_matrix`: sklearn.metrics.confusion_matrix, - `f1_score`: sklearn.metrics.f1_score - `hamming_loss`: sklearn.metrics.hamming_loss - `log_loss`: sklearn.metrics.log_loss - `precision`: sklearn.metrics.precision_score - `recall`: sklearn.metrics.recall_score - Metrics for `regression`: - `mean_squared_error`: sklearn.metrics.mean_squared_error - `mean_absolute_error`: sklearn.metrics.mean_absolute_error - `explained_variance`: sklearn.metrics.explained_variance_score - `r2`: sklearn.metrics.r2_score - Other metrics - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation - `variance_explained`: photon_core.framework.Metrics.variance_explained_score - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score best_config_metric: The metric that should be maximized or minimized in order to choose the best hyperparameter configuration use_test_set [bool, default=True]: If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. project_folder: The output folder in which all files generated by the PHOTONAI project are saved to. test_size: The amount of the data that should be left out if no outer_cv is given and eval_final_perfomance is set to True. calculate_metrics_per_fold: If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. calculate_metrics_across_folds: If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. random_seed: Random Seed. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug learning_curves: Enables larning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. learning_curves_cut: The tested relativ cuts for data size. performance_constraints: Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. permutation_id: String identifier for permutation tests. cache_folder: Folder path for multi-processing. nr_of_processes: Determined the amount of simultaneous calculation of outer_folds. allow_multidim_targets: Allows multidimensional targets. \"\"\" self . name = re . sub ( r '\\W+' , '' , name ) # ====================== Cross Validation =========================== # check if both calculate_metrics_per_folds and calculate_metrics_across_folds is False if not calculate_metrics_across_folds and not calculate_metrics_per_fold : raise NotImplementedError ( \"Apparently, you've set calculate_metrics_across_folds=False and \" \"calculate_metrics_per_fold=False. In this case PHOTONAI does not calculate \" \"any metrics which doesn't make any sense. Set at least one to True.\" ) if inner_cv is None : msg = \"PHOTONAI requires an inner_cv split. Please enable inner cross-validation. As exmaple: Hyperpipe(...\" \\ \" inner_cv = KFold(n_splits = 3), ...). Ensure you import the cross_validation object first.\" logger . error ( msg ) raise AttributeError ( msg ) # use default cut 'FloatRange(0, 1, 'range', 0.2)' if learning_curves = True but learning_curves_cut is None if learning_curves and learning_curves_cut is None : learning_curves_cut = FloatRange ( 0 , 1 , 'range' , 0.2 ) elif not learning_curves and learning_curves_cut is not None : learning_curves_cut = None self . cross_validation = Hyperpipe . CrossValidation ( inner_cv = inner_cv , outer_cv = outer_cv , use_test_set = use_test_set , test_size = test_size , calculate_metrics_per_fold = calculate_metrics_per_fold , calculate_metrics_across_folds = calculate_metrics_across_folds , learning_curves = learning_curves , learning_curves_cut = learning_curves_cut ) # ====================== Data =========================== self . data = Hyperpipe . Data () # ====================== Output Folder and Log File Management =========================== if output_settings : self . output_settings = output_settings else : self . output_settings = OutputSettings () if project_folder == '' : self . project_folder = os . getcwd () else : self . project_folder = project_folder self . output_settings . set_project_folder ( self . project_folder ) # update output options to add pipe name and timestamp to results folder self . _verbosity = 0 self . verbosity = verbosity self . output_settings . set_log_file () # ====================== Result Logging =========================== self . results_handler = None self . results = None self . best_config = None # ====================== Pipeline =========================== self . elements = [] self . _pipe = None self . optimum_pipe = None self . preprocessing = None # ====================== Performance Optimization =========================== if optimizer_params is None : optimizer_params = {} self . optimization = Optimization ( metrics = metrics , best_config_metric = best_config_metric , optimizer_input = optimizer , optimizer_params = optimizer_params , performance_constraints = performance_constraints ) self . optimization . sanity_check_metrics () # ====================== Caching and Parallelization =========================== self . nr_of_processes = nr_of_processes if cache_folder : self . cache_folder = os . path . join ( cache_folder , self . name ) else : self . cache_folder = None # ====================== Internals =========================== self . permutation_id = permutation_id self . allow_multidim_targets = allow_multidim_targets self . is_final_fit = False # ====================== Random Seed =========================== self . random_state = random_seed if random_seed is not None : import random random . seed ( random_seed )","title":"__init__()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.add","text":"Add an element to the machine learning pipeline Returns self Parameters: Name Type Description Default pipe_element PipelineElement The object to add to the machine learning pipeline, being either a transformer or an estimator. required Source code in photonai/base/hyperpipe.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline Returns self Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element )","title":"add()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.copy_me","text":"Helper function to copy an entire Hyperpipe Returns: Type Description Hyperpipe Source code in photonai/base/hyperpipe.py def copy_me ( self ): \"\"\" Helper function to copy an entire Hyperpipe Returns: Hyperpipe \"\"\" signature = inspect . getfullargspec ( OutputSettings . __init__ )[ 0 ] settings = OutputSettings () for attr in signature : if hasattr ( self . output_settings , attr ): setattr ( settings , attr , getattr ( self . output_settings , attr )) self . output_settings . initialize_log_file () # create new Hyperpipe instance pipe_copy = Hyperpipe ( name = self . name , inner_cv = deepcopy ( self . cross_validation . inner_cv ), outer_cv = deepcopy ( self . cross_validation . outer_cv ), best_config_metric = self . optimization . best_config_metric , metrics = self . optimization . metrics , optimizer = self . optimization . optimizer_input_str , optimizer_params = self . optimization . optimizer_params , project_folder = self . project_folder , output_settings = settings ) signature = inspect . getfullargspec ( self . __init__ )[ 0 ] for attr in signature : if hasattr ( self , attr ) and attr != 'output_settings' : setattr ( pipe_copy , attr , getattr ( self , attr )) if hasattr ( self , 'preprocessing' ) and self . preprocessing : preprocessing = Preprocessing () for element in self . preprocessing . elements : preprocessing += element . copy_me () pipe_copy += preprocessing if hasattr ( self , 'elements' ): for element in self . elements : pipe_copy += element . copy_me () return pipe_copy","title":"copy_me()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.fit","text":"Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) requests new configurations from the hyperparameter search strategy, the optimizer, initializes the testing of a specific configuration, communicates the result to the optimizer, repeats 2-4 until optimizer delivers no more configurations to test finally searches for the best config in all tested configs, trains the pipeline with the best config and evaluates the performance on the test set Parameters: Name Type Description Default data ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required targets ndarray The truth array-like values with shape=[N], where N is the number of samples. required kwargs Keyword arguments, passed to Outer_Fold_Manager.fit. {} Returns: Type Description Fitted Hyperpipe. Source code in photonai/base/hyperpipe.py def fit ( self , data : np . ndarray , targets : np . ndarray , ** kwargs ): \"\"\" Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: 1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) 2. requests new configurations from the hyperparameter search strategy, the optimizer, 3. initializes the testing of a specific configuration, 4. communicates the result to the optimizer, 5. repeats 2-4 until optimizer delivers no more configurations to test 6. finally searches for the best config in all tested configs, 7. trains the pipeline with the best config and evaluates the performance on the test set Parameters: data: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. targets: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to Outer_Fold_Manager.fit. Returns: Fitted Hyperpipe. \"\"\" # switch to result output folder start = datetime . datetime . now () self . output_settings . update_settings ( self . name , start . strftime ( \"%Y-%m- %d _%H-%M-%S\" )) logger . photon_system_log ( '=' * 101 ) logger . photon_system_log ( 'PHOTONAI ANALYSIS: ' + self . name ) logger . photon_system_log ( '=' * 101 ) logger . info ( \"Preparing data and PHOTONAI objects for analysis...\" ) # loop over outer cross validation if self . nr_of_processes > 1 : hyperpipe_client = Client ( threads_per_worker = 1 , n_workers = self . nr_of_processes , processes = False ) try : # check data self . data . input_data_sanity_checks ( data , targets , ** kwargs ) # create photon pipeline self . _prepare_pipeline () # initialize the progress monitors self . _prepare_result_logging ( start ) # apply preprocessing self . preprocess_data () if not self . is_final_fit : # Outer Folds outer_folds = FoldInfo . generate_folds ( self . cross_validation . outer_cv , self . data . X , self . data . y , self . data . kwargs , self . cross_validation . use_test_set , self . cross_validation . test_size ) self . cross_validation . outer_folds = { f . fold_id : f for f in outer_folds } delayed_jobs = [] # Run Dummy Estimator dummy_estimator = self . _prepare_dummy_estimator () if self . cache_folder is not None : logger . info ( \"Removing cache files...\" ) CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) # loop over outer cross validation for i , outer_f in enumerate ( outer_folds ): # 1. generate OuterFolds Object outer_fold = MDBOuterFold ( fold_nr = outer_f . fold_nr ) outer_fold_computer = OuterFoldManager ( self . _pipe , self . optimization , outer_f . fold_id , self . cross_validation , cache_folder = self . cache_folder , cache_updater = self . recursive_cache_folder_propagation , dummy_estimator = dummy_estimator , result_obj = outer_fold ) # 2. monitor outputs self . results . outer_folds . append ( outer_fold ) if self . nr_of_processes > 1 : result = dask . delayed ( Hyperpipe . fit_outer_folds )( outer_fold_computer , self . data . X , self . data . y , self . data . kwargs , self . cache_folder ) delayed_jobs . append ( result ) else : try : # 3. fit outer_fold_computer . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # 4. save outer fold results self . results_handler . save () finally : # 5. clear cache CacheManager . clear_cache_files ( self . cache_folder ) if self . nr_of_processes > 1 : dask . compute ( * delayed_jobs ) self . results_handler . save () # evaluate hyperparameter optimization results for best config self . _finalize_optimization () # clear complete cache ? CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) ############################################################################################### else : self . preprocess_data () self . _pipe . fit ( self . data . X , self . data . y , ** kwargs ) except Exception as e : logger . error ( e ) logger . error ( traceback . format_exc ()) traceback . print_exc () raise e finally : if self . nr_of_processes > 1 : hyperpipe_client . close () return self","title":"fit()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.inverse_transform_pipeline","text":"Inverse transform data for a pipeline with specific hyperparameter configuration. Copy Sklearn Pipeline, Set Parameters Fit Pipeline to data and targets Inverse transform data with that pipeline Parameters: Name Type Description Default hyperparameters dict The concrete configuration settings for the pipeline elements. required data ndarray The training data to which the pipeline is fitted. required targets ndarray The truth values for training. required data_to_inverse ndarray The data that should be inversed after training. required Returns: Type Description ndarray Inversed data as array. Source code in photonai/base/hyperpipe.py def inverse_transform_pipeline ( self , hyperparameters : dict , data : np . ndarray , targets : np . ndarray , data_to_inverse : np . ndarray ) -> np . ndarray : \"\"\" Inverse transform data for a pipeline with specific hyperparameter configuration. 1. Copy Sklearn Pipeline, 2. Set Parameters 3. Fit Pipeline to data and targets 4. Inverse transform data with that pipeline Parameters: hyperparameters: The concrete configuration settings for the pipeline elements. data: The training data to which the pipeline is fitted. targets: The truth values for training. data_to_inverse: The data that should be inversed after training. Returns: Inversed data as array. \"\"\" copied_pipe = self . pipe . copy_me () copied_pipe . set_params ( ** hyperparameters ) copied_pipe . fit ( data , targets ) return copied_pipe . inverse_transform ( data_to_inverse )","title":"inverse_transform_pipeline()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.load_optimum_pipe","text":"Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: Name Type Description Default file str File path specifying .photon file to load trained pipeline from zipped file. required password str Passcode for read file. None Returns: Type Description PhotonPipeline Returns pipeline with all trained PipelineElements. Source code in photonai/base/hyperpipe.py @staticmethod def load_optimum_pipe ( file : str , password : str = None ) -> PhotonPipeline : \"\"\" Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: file: File path specifying .photon file to load trained pipeline from zipped file. password: Passcode for read file. Returns: Returns pipeline with all trained PipelineElements. \"\"\" return PhotonModelPersistor . load_optimum_pipe ( file , password )","title":"load_optimum_pipe()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.predict","text":"Use the optimum pipe to predict the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required kwargs Keyword arguments, passed to optimum_pipe.predict. {} Returns: Type Description ndarray Predicted targets calculated on input data with trained model. Source code in photonai/base/hyperpipe.py def predict ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Predicted targets calculated on input data with trained model. \"\"\" # Todo: if local_search = true then use optimized pipe here? if self . _pipe : return self . optimum_pipe . predict ( data , ** kwargs )","title":"predict()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.predict_proba","text":"Use the optimum pipe to predict the probabilities from the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required kwargs Keyword arguments, passed to optimum_pipe.predict_proba. {} Returns: Type Description ndarray Probabilities calculated from input data on fitted model. Source code in photonai/base/hyperpipe.py def predict_proba ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the probabilities from the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. kwargs: Keyword arguments, passed to optimum_pipe.predict_proba. Returns: Probabilities calculated from input data on fitted model. \"\"\" if self . _pipe : return self . optimum_pipe . predict_proba ( data , ** kwargs )","title":"predict_proba()"},{"location":"api/base/output_settings/","text":"Documentation for OutputSettings Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. __init__ ( self , mongodb_connect_url = None , save_output = True , overwrite_results = False , generate_best_model = True , user_id = '' , wizard_object_id = '' , wizard_project_name = '' , project_folder = '' ) special Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"OutputSettings"},{"location":"api/base/output_settings/#documentation-for-outputsettings","text":"","title":"Documentation for OutputSettings"},{"location":"api/base/output_settings/#photonai.base.hyperpipe.OutputSettings","text":"Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances.","title":"photonai.base.hyperpipe.OutputSettings"},{"location":"api/base/output_settings/#photonai.base.hyperpipe.OutputSettings.__init__","text":"Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"__init__()"},{"location":"api/base/pipeline_element/","text":"Documentation for PipelineElement PHOTONAI wrapper class for any transformer or predictor element in the pipeline. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods and algorithms and arranging them with the PHOTONAI classes, both simple and complex pipeline architectures can be designed rapidly. The PHOTONAI PipelineElement implements several helpful features: Saves the hyperparameters that are to be tested and creates a grid of all hyperparameter configurations. Enables fast and rapid instantiation of pipeline elements per string identifier, e.g 'svc' creates an sklearn.svm.SVC object. Attaches a \"disable\" switch to every element in the pipeline in order to test a complete disable __iadd__ ( self , pipe_element ) special Add an element to the element list Returns self Parameters: Name Type Description Default pipe_element The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element ): \"\"\" Add an element to the element list Returns self Parameters: pipe_element: The object to add, being either a transformer or an estimator. \"\"\" PipelineElement . sanity_check_element_type_for_building_photon_pipes ( pipe_element , type ( self )) # check if that exact instance has been added before already_added_objects = len ([ i for i in self . elements if i is pipe_element ]) if already_added_objects > 0 : error_msg = \"Cannot add the same instance twice to \" + self . name + \" - \" + str ( type ( self )) logger . error ( error_msg ) raise ValueError ( error_msg ) # check for doubled names: already_existing_element_with_that_name = len ([ i for i in self . elements if i . name == pipe_element . name ]) if already_existing_element_with_that_name > 0 : error_msg = \"Already added a pipeline element with the name \" + pipe_element . name + \" to \" + self . name logger . warning ( error_msg ) warnings . warn ( error_msg ) # check for other items that have been renamed nr_of_existing_elements_with_that_name = len ([ i for i in self . elements if i . name . startswith ( pipe_element . name )]) new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) while len ([ i for i in self . elements if i . name == new_name ]) > 0 : nr_of_existing_elements_with_that_name += 1 new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) msg = \"Renaming \" + pipe_element . name + \" in \" + self . name + \" to \" + new_name + \" in \" + self . name logger . warning ( msg ) warnings . warn ( msg ) pipe_element . name = new_name self . elements . append ( pipe_element ) return self __init__ ( self , name , hyperparameters = None , test_disabled = False , disabled = False , base_element = None , batch_size = 0 , ** kwargs ) special Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. None test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False batch_size int Size of the division on which is calculated separately. 0 kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , hyperparameters : dict = None , test_disabled : bool = False , disabled : bool = False , base_element = None , batch_size : int = 0 , ** kwargs ) -> None : \"\"\" Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. batch_size: Size of the division on which is calculated separately. kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. \"\"\" if hyperparameters is None : hyperparameters = {} if base_element is None : # Registering Pipeline Elements if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : registry = PhotonRegistry if name not in PhotonRegistry . ELEMENT_DICTIONARY : # try to reload PhotonRegistry . ELEMENT_DICTIONARY = PhotonRegistry () . get_package_info () if name in PhotonRegistry . ELEMENT_DICTIONARY : try : desired_class_info = PhotonRegistry . ELEMENT_DICTIONARY [ name ] desired_class_home = desired_class_info [ 0 ] desired_class_name = desired_class_info [ 1 ] imported_module = importlib . import_module ( desired_class_home ) desired_class = getattr ( imported_module , desired_class_name ) self . base_element = desired_class ( ** kwargs ) except AttributeError as ae : logger . error ( 'ValueError: Could not find according class:' + str ( PhotonRegistry . ELEMENT_DICTIONARY [ name ])) raise ValueError ( 'Could not find according class:' , PhotonRegistry . ELEMENT_DICTIONARY [ name ]) else : # if even after reload the element does not appear, it is not supported logger . error ( 'Element not supported right now:' + name ) raise NameError ( 'Element not supported right now:' , name ) else : self . base_element = base_element self . is_transformer = hasattr ( self . base_element , \"transform\" ) self . reduce_dimension = False # boolean - set on transform method self . is_estimator = hasattr ( self . base_element , \"predict\" ) self . _name = name self . initial_name = str ( name ) self . kwargs = kwargs self . current_config = None self . batch_size = batch_size self . test_disabled = test_disabled self . initial_hyperparameters = dict ( hyperparameters ) self . _sklearn_disabled = self . name + '__disabled' self . _hyperparameters = hyperparameters if len ( hyperparameters ) > 0 : key_0 = next ( iter ( hyperparameters )) if self . name not in key_0 : self . hyperparameters = hyperparameters else : self . hyperparameters = hyperparameters # self.initalize_hyperparameters = hyperparameters # check if hyperparameters are already in sklearn style # check if hyperparameters are members of the class if self . is_transformer or self . is_estimator : self . _check_hyperparameters ( BaseEstimator ) self . disabled = disabled # check if self.base element needs y for fitting and transforming if hasattr ( self . base_element , 'needs_y' ): self . needs_y = self . base_element . needs_y else : self . needs_y = False # or if it maybe needs covariates for fitting and transforming if hasattr ( self . base_element , 'needs_covariates' ): self . needs_covariates = self . base_element . needs_covariates else : self . needs_covariates = False self . _random_state = False create ( name , base_element , hyperparameters , test_disabled = False , disabled = False , ** kwargs ) classmethod Takes an instantiated object and encapsulates it into the PHOTONAI structure, add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. required test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Examples: class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) Source code in photonai/base/photon_elements.py @classmethod def create ( cls , name : str , base_element , hyperparameters : dict , test_disabled : bool = False , disabled : bool = False , ** kwargs ): \"\"\" Takes an instantiated object and encapsulates it into the PHOTONAI structure, add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. Example: ``` class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) ``` \"\"\" if isinstance ( base_element , type ): raise ValueError ( \"Base element should be an instance but is a class.\" ) return PipelineElement ( name , hyperparameters , test_disabled , disabled , base_element = base_element , ** kwargs ) fit ( self , X , y = None , ** kwargs ) Calls the fit function of the base element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_element.predict. Returns: Fitted self. \"\"\" if not self . disabled : obj = self . base_element arg_list = inspect . signature ( obj . fit ) if len ( arg_list . parameters ) > 2 : vals = arg_list . parameters . values () kwargs_param = list ( vals )[ - 1 ] if kwargs_param . kind == kwargs_param . VAR_KEYWORD : obj . fit ( X , y , ** kwargs ) return self obj . fit ( X , y ) return self inverse_transform ( self , X , y = None , ** kwargs ) Calls inverse_transform on the base element. For dimension preserving transformers without inverse, the value is returned untreated. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (X, y, kwargs) in back-transformed version. Source code in photonai/base/photon_elements.py def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls inverse_transform on the base element. For dimension preserving transformers without inverse, the value is returned untreated. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_element.transform. Raises: NotImplementedError: Thrown when there is a dimensional reduction but no inverse is defined. Returns: (X, y, kwargs) in back-transformed version. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) elif self . is_transformer and self . reduce_dimension : msg = \" {} has no inverse_transform, but element reduce dimesions.\" . format ( self . name ) logger . error ( msg ) raise NotImplementedError ( msg ) return X , y , kwargs predict ( self , X , ** kwargs ) Calls the predict function of the base element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description ndarray Predictions values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. kwargs: Keyword arguments, passed to base_element.predict. Returns: Predictions values. \"\"\" if self . batch_size == 0 : return self . __predict ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict , X , ** kwargs ) score ( self , X_test , y_test ) Calls the score function on the base element: Parameters: Name Type Description Default X_test ndarray Input test data to score on. required y_test ndarray Input true targets to score on. required Returns: Type Description A goodness of fit measure or a likelihood of unseen data. Source code in photonai/base/photon_elements.py def score ( self , X_test : np . ndarray , y_test : np . ndarray ): \"\"\" Calls the score function on the base element: Parameters: X_test: Input test data to score on. y_test: Input true targets to score on. Returns: A goodness of fit measure or a likelihood of unseen data. \"\"\" return self . base_element . score ( X_test , y_test ) transform ( self , X , y = None , ** kwargs ) Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (X, y, kwargs) in transformed version. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y, kwargs) in transformed version. \"\"\" if self . batch_size == 0 : Xt , yt , kwargs = self . __transform ( X , y , ** kwargs ) else : Xt , yt , kwargs = self . __batch_transform ( X , y , ** kwargs ) if all ( hasattr ( data , \"shape\" ) for data in [ X , Xt ]) and all ( len ( data . shape ) > 1 for data in [ X , Xt ]): self . reduce_dimension = ( Xt . shape [ 1 ] < X . shape [ 1 ]) return Xt , yt , kwargs","title":"PipelineElement"},{"location":"api/base/pipeline_element/#documentation-for-pipelineelement","text":"","title":"Documentation for PipelineElement"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement","text":"PHOTONAI wrapper class for any transformer or predictor element in the pipeline. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods and algorithms and arranging them with the PHOTONAI classes, both simple and complex pipeline architectures can be designed rapidly. The PHOTONAI PipelineElement implements several helpful features: Saves the hyperparameters that are to be tested and creates a grid of all hyperparameter configurations. Enables fast and rapid instantiation of pipeline elements per string identifier, e.g 'svc' creates an sklearn.svm.SVC object. Attaches a \"disable\" switch to every element in the pipeline in order to test a complete disable","title":"photonai.base.photon_elements.PipelineElement"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.__iadd__","text":"Add an element to the element list Returns self Parameters: Name Type Description Default pipe_element The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element ): \"\"\" Add an element to the element list Returns self Parameters: pipe_element: The object to add, being either a transformer or an estimator. \"\"\" PipelineElement . sanity_check_element_type_for_building_photon_pipes ( pipe_element , type ( self )) # check if that exact instance has been added before already_added_objects = len ([ i for i in self . elements if i is pipe_element ]) if already_added_objects > 0 : error_msg = \"Cannot add the same instance twice to \" + self . name + \" - \" + str ( type ( self )) logger . error ( error_msg ) raise ValueError ( error_msg ) # check for doubled names: already_existing_element_with_that_name = len ([ i for i in self . elements if i . name == pipe_element . name ]) if already_existing_element_with_that_name > 0 : error_msg = \"Already added a pipeline element with the name \" + pipe_element . name + \" to \" + self . name logger . warning ( error_msg ) warnings . warn ( error_msg ) # check for other items that have been renamed nr_of_existing_elements_with_that_name = len ([ i for i in self . elements if i . name . startswith ( pipe_element . name )]) new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) while len ([ i for i in self . elements if i . name == new_name ]) > 0 : nr_of_existing_elements_with_that_name += 1 new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) msg = \"Renaming \" + pipe_element . name + \" in \" + self . name + \" to \" + new_name + \" in \" + self . name logger . warning ( msg ) warnings . warn ( msg ) pipe_element . name = new_name self . elements . append ( pipe_element ) return self","title":"__iadd__()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.__init__","text":"Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. None test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False batch_size int Size of the division on which is calculated separately. 0 kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , hyperparameters : dict = None , test_disabled : bool = False , disabled : bool = False , base_element = None , batch_size : int = 0 , ** kwargs ) -> None : \"\"\" Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. batch_size: Size of the division on which is calculated separately. kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. \"\"\" if hyperparameters is None : hyperparameters = {} if base_element is None : # Registering Pipeline Elements if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : registry = PhotonRegistry if name not in PhotonRegistry . ELEMENT_DICTIONARY : # try to reload PhotonRegistry . ELEMENT_DICTIONARY = PhotonRegistry () . get_package_info () if name in PhotonRegistry . ELEMENT_DICTIONARY : try : desired_class_info = PhotonRegistry . ELEMENT_DICTIONARY [ name ] desired_class_home = desired_class_info [ 0 ] desired_class_name = desired_class_info [ 1 ] imported_module = importlib . import_module ( desired_class_home ) desired_class = getattr ( imported_module , desired_class_name ) self . base_element = desired_class ( ** kwargs ) except AttributeError as ae : logger . error ( 'ValueError: Could not find according class:' + str ( PhotonRegistry . ELEMENT_DICTIONARY [ name ])) raise ValueError ( 'Could not find according class:' , PhotonRegistry . ELEMENT_DICTIONARY [ name ]) else : # if even after reload the element does not appear, it is not supported logger . error ( 'Element not supported right now:' + name ) raise NameError ( 'Element not supported right now:' , name ) else : self . base_element = base_element self . is_transformer = hasattr ( self . base_element , \"transform\" ) self . reduce_dimension = False # boolean - set on transform method self . is_estimator = hasattr ( self . base_element , \"predict\" ) self . _name = name self . initial_name = str ( name ) self . kwargs = kwargs self . current_config = None self . batch_size = batch_size self . test_disabled = test_disabled self . initial_hyperparameters = dict ( hyperparameters ) self . _sklearn_disabled = self . name + '__disabled' self . _hyperparameters = hyperparameters if len ( hyperparameters ) > 0 : key_0 = next ( iter ( hyperparameters )) if self . name not in key_0 : self . hyperparameters = hyperparameters else : self . hyperparameters = hyperparameters # self.initalize_hyperparameters = hyperparameters # check if hyperparameters are already in sklearn style # check if hyperparameters are members of the class if self . is_transformer or self . is_estimator : self . _check_hyperparameters ( BaseEstimator ) self . disabled = disabled # check if self.base element needs y for fitting and transforming if hasattr ( self . base_element , 'needs_y' ): self . needs_y = self . base_element . needs_y else : self . needs_y = False # or if it maybe needs covariates for fitting and transforming if hasattr ( self . base_element , 'needs_covariates' ): self . needs_covariates = self . base_element . needs_covariates else : self . needs_covariates = False self . _random_state = False","title":"__init__()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.create","text":"Takes an instantiated object and encapsulates it into the PHOTONAI structure, add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. required test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Examples: class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) Source code in photonai/base/photon_elements.py @classmethod def create ( cls , name : str , base_element , hyperparameters : dict , test_disabled : bool = False , disabled : bool = False , ** kwargs ): \"\"\" Takes an instantiated object and encapsulates it into the PHOTONAI structure, add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. Example: ``` class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) ``` \"\"\" if isinstance ( base_element , type ): raise ValueError ( \"Base element should be an instance but is a class.\" ) return PipelineElement ( name , hyperparameters , test_disabled , disabled , base_element = base_element , ** kwargs )","title":"create()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.fit","text":"Calls the fit function of the base element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_element.predict. Returns: Fitted self. \"\"\" if not self . disabled : obj = self . base_element arg_list = inspect . signature ( obj . fit ) if len ( arg_list . parameters ) > 2 : vals = arg_list . parameters . values () kwargs_param = list ( vals )[ - 1 ] if kwargs_param . kind == kwargs_param . VAR_KEYWORD : obj . fit ( X , y , ** kwargs ) return self obj . fit ( X , y ) return self","title":"fit()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.inverse_transform","text":"Calls inverse_transform on the base element. For dimension preserving transformers without inverse, the value is returned untreated. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (X, y, kwargs) in back-transformed version. Source code in photonai/base/photon_elements.py def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls inverse_transform on the base element. For dimension preserving transformers without inverse, the value is returned untreated. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_element.transform. Raises: NotImplementedError: Thrown when there is a dimensional reduction but no inverse is defined. Returns: (X, y, kwargs) in back-transformed version. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) elif self . is_transformer and self . reduce_dimension : msg = \" {} has no inverse_transform, but element reduce dimesions.\" . format ( self . name ) logger . error ( msg ) raise NotImplementedError ( msg ) return X , y , kwargs","title":"inverse_transform()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.predict","text":"Calls the predict function of the base element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description ndarray Predictions values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. kwargs: Keyword arguments, passed to base_element.predict. Returns: Predictions values. \"\"\" if self . batch_size == 0 : return self . __predict ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict , X , ** kwargs )","title":"predict()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.score","text":"Calls the score function on the base element: Parameters: Name Type Description Default X_test ndarray Input test data to score on. required y_test ndarray Input true targets to score on. required Returns: Type Description A goodness of fit measure or a likelihood of unseen data. Source code in photonai/base/photon_elements.py def score ( self , X_test : np . ndarray , y_test : np . ndarray ): \"\"\" Calls the score function on the base element: Parameters: X_test: Input test data to score on. y_test: Input true targets to score on. Returns: A goodness of fit measure or a likelihood of unseen data. \"\"\" return self . base_element . score ( X_test , y_test )","title":"score()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.transform","text":"Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (X, y, kwargs) in transformed version. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y, kwargs) in transformed version. \"\"\" if self . batch_size == 0 : Xt , yt , kwargs = self . __transform ( X , y , ** kwargs ) else : Xt , yt , kwargs = self . __batch_transform ( X , y , ** kwargs ) if all ( hasattr ( data , \"shape\" ) for data in [ X , Xt ]) and all ( len ( data . shape ) > 1 for data in [ X , Xt ]): self . reduce_dimension = ( Xt . shape [ 1 ] < X . shape [ 1 ]) return Xt , yt , kwargs","title":"transform()"},{"location":"api/base/preprocessing/","text":"Documentation for Preprocessing Special kind of Branch. If a Preprocessing pipe is added to a PHOTONAI Hyperpipe, all transformers are applied to the data ONCE BEFORE cross validation starts in order to prepare the data. Every added element should be a transformer PipelineElement. __iadd__ ( self , pipe_element ) special Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The transformer object to add. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The transformer object to add. \"\"\" if hasattr ( pipe_element , \"transform\" ): super ( Preprocessing , self ) . __iadd__ ( pipe_element ) if len ( pipe_element . hyperparameters ) > 0 : raise ValueError ( \"A preprocessing transformer must not have any hyperparameter \" \"because it is not part of the optimization and cross validation procedure\" ) else : raise ValueError ( \"Pipeline Element must have transform function\" ) return self __init__ ( self ) special Initialize the object. Source code in photonai/base/photon_elements.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super () . __init__ ( 'Preprocessing' ) self . has_hyperparameters = False self . needs_y = True self . needs_covariates = True self . _name = 'Preprocessing' self . is_transformer = True self . is_estimator = False","title":"Preprocessing"},{"location":"api/base/preprocessing/#documentation-for-preprocessing","text":"","title":"Documentation for Preprocessing"},{"location":"api/base/preprocessing/#photonai.base.photon_elements.Preprocessing","text":"Special kind of Branch. If a Preprocessing pipe is added to a PHOTONAI Hyperpipe, all transformers are applied to the data ONCE BEFORE cross validation starts in order to prepare the data. Every added element should be a transformer PipelineElement.","title":"photonai.base.photon_elements.Preprocessing"},{"location":"api/base/preprocessing/#photonai.base.photon_elements.Preprocessing.__iadd__","text":"Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The transformer object to add. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The transformer object to add. \"\"\" if hasattr ( pipe_element , \"transform\" ): super ( Preprocessing , self ) . __iadd__ ( pipe_element ) if len ( pipe_element . hyperparameters ) > 0 : raise ValueError ( \"A preprocessing transformer must not have any hyperparameter \" \"because it is not part of the optimization and cross validation procedure\" ) else : raise ValueError ( \"Pipeline Element must have transform function\" ) return self","title":"__iadd__()"},{"location":"api/base/preprocessing/#photonai.base.photon_elements.Preprocessing.__init__","text":"Initialize the object. Source code in photonai/base/photon_elements.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super () . __init__ ( 'Preprocessing' ) self . has_hyperparameters = False self . needs_y = True self . needs_covariates = True self . _name = 'Preprocessing' self . is_transformer = True self . is_estimator = False","title":"__init__()"},{"location":"api/base/registry/","text":"Documentation for PhotonRegistry Helper class to manage the PHOTONAI Element Register. Use it to add and remove items into the register. You can also retrieve information about items and its hyperparameters. Every item in the register is encoded by a string literal that points to a python class and its namespace. You can access the python class via the string literal. The class PhotonElement imports and instantiates the class for you. Examples: import os from photonai.base import PhotonRegistry # REGISTER ELEMENT saved in folder custom_elements_folder base_folder = os.path.dirname(os.path.abspath(__file__)) custom_elements_folder = os.path.join(base_folder, 'custom_elements') registry = PhotonRegistry(custom_elements_folder=custom_elements_folder) registry.register(photon_name='MyCustomEstimator', class_str='custom_estimator.CustomEstimator', element_type='Estimator') registry.activate() registry.info('MyCustomEstimator') # get informations of other available elements registry.info('SVC') __init__ ( self , custom_elements_folder = None ) special Initialize the object. Parameters: Name Type Description Default custom_elements_folder str Path to folder with custom element in it. None Source code in photonai/base/registry/registry.py def __init__ ( self , custom_elements_folder : str = None ): \"\"\" Initialize the object. Parameters: custom_elements_folder: Path to folder with custom element in it. \"\"\" self . current_folder = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) self . module_path = os . path . join ( self . current_folder , \"modules\" ) if not os . path . isdir ( self . module_path ): os . mkdir ( self . module_path ) # update list with available sub_elements self . _list_available_modules () PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = custom_elements_folder self . _load_custom_folder ( custom_elements_folder ) if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : PhotonRegistry . ELEMENT_DICTIONARY = self . get_package_info () delete ( self , photon_name ) Delete Element from JSON file. Parameters: Name Type Description Default photon_name str The string literal encoding the class. required Source code in photonai/base/registry/registry.py def delete ( self , photon_name : str ): \"\"\" Delete Element from JSON file. Parameters: photon_name: The string literal encoding the class. \"\"\" if photon_name in PhotonRegistry . CUSTOM_ELEMENTS : del PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Removing the PipelineElement named \" {0} \" from CustomElements.json.' . format ( photon_name )) else : logger . info ( 'Cannot remove \" {0} \" from CustomElements.json. Element has not been registered before.' . format ( photon_name )) get_package_info ( self , photon_package = [ 'PhotonCore' ]) Collect all registered elements from JSON file. Parameters: Name Type Description Default photon_package list The names of the PHOTONAI submodules for which the elements should be retrieved. ['PhotonCore'] Returns: Type Description dict Dict of registered elements Source code in photonai/base/registry/registry.py def get_package_info ( self , photon_package : list = PHOTON_REGISTRIES ) -> dict : \"\"\" Collect all registered elements from JSON file. Parameters: photon_package: The names of the PHOTONAI submodules for which the elements should be retrieved. Returns: Dict of registered elements \"\"\" class_info = dict () for package in photon_package : content = self . _load_json ( package ) for idx , key in enumerate ( content ): class_path , class_name = os . path . splitext ( content [ key ][ 0 ]) if idx == 0 and package not in [ \"PhotonCore\" , \"CustomElements\" ]: # try to import something from module. # if that fails. drop this shit. try : imported_module = importlib . import_module ( class_path ) desired_class = getattr ( imported_module , class_name [ 1 :]) custom_element = desired_class () except ( AttributeError , ModuleNotFoundError ) as e : logger . error ( e ) logger . error ( \"Could not import from package {} . Deleting json.\" . format ( package )) self . delete_module ( package ) class_info [ key ] = class_path , class_name [ 1 :] return class_info info ( self , photon_name ) Show information for object that is encoded by this name. Parameters: Name Type Description Default photon_name str The string literal which accesses the class. required Source code in photonai/base/registry/registry.py def info ( self , photon_name : str ): \"\"\" Show information for object that is encoded by this name. Parameters: photon_name: The string literal which accesses the class. \"\"\" content = self . get_package_info () # load existing json if photon_name in content : element_namespace , element_name = content [ photon_name ] print ( \"----------------------------------\" ) print ( \"Name: \" + element_name ) print ( \"Namespace: \" + element_namespace ) print ( \"----------------------------------\" ) try : imported_module = __import__ ( element_namespace , globals (), locals (), element_name , 0 ) desired_class = getattr ( imported_module , element_name ) base_element = desired_class () print ( \"Possible Hyperparameters as derived from constructor:\" ) class_args = inspect . signature ( base_element . __init__ ) for item , more_info in class_args . parameters . items (): print ( \" {:<35} {:<75} \" . format ( item , str ( more_info ))) print ( \"----------------------------------\" ) except Exception as e : logger . error ( e ) logger . error ( \"Could not instantiate class \" + element_namespace + \".\" + element_name ) else : logger . error ( \"Could not find element \" + photon_name ) list_available_elements ( self , photon_package = [ 'PhotonCore' ]) Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: Name Type Description Default photon_package list The names of the PHOTON submodules for which the elements should be retrieved. ['PhotonCore'] Source code in photonai/base/registry/registry.py def list_available_elements ( self , photon_package : list = PHOTON_REGISTRIES ): \"\"\" Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: photon_package: The names of the PHOTON submodules for which the elements should be retrieved. \"\"\" if isinstance ( photon_package , str ): photon_package = [ photon_package ] for package in photon_package : content = self . _load_json ( package ) if len ( content ) > 0 : print ( ' \\n ' + package ) for k , v in sorted ( content . items ()): class_info , package_type = v print ( \" {:<35} {:<75} {:<5} \" . format ( k , class_info , package_type )) register ( self , photon_name , class_str , element_type ) Save element information to the JSON file. Parameters: Name Type Description Default photon_name str The string literal with which you want to access the class. required class_str str The namespace of the class, like in the import statement. required element_type str Can be 'Estimator' or 'Transformer' required Source code in photonai/base/registry/registry.py def register ( self , photon_name : str , class_str : str , element_type : str ): \"\"\" Save element information to the JSON file. Parameters: photon_name: The string literal with which you want to access the class. class_str: The namespace of the class, like in the import statement. element_type: Can be 'Estimator' or 'Transformer' \"\"\" # check if folder exists if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To register an element, specify a custom elements folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER)\" ) if not element_type == \"Estimator\" and not element_type == \"Transformer\" : raise ValueError ( \"Variable element_type must be 'Estimator' or 'Transformer'\" ) duplicate = self . _check_duplicate ( photon_name = photon_name , class_str = class_str , content = PhotonRegistry . CUSTOM_ELEMENTS ) if not duplicate : python_file = os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , class_str . split ( '.' )[ 0 ] + '.py' ) if not os . path . isfile ( python_file ): raise FileNotFoundError ( \"Couldn't find python file {} in your custom elements folder. \" \"Please copy your file into this folder first!\" . format ( python_file )) # add new element PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] = class_str , element_type # write back to file self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Adding PipelineElement ' + class_str + ' to CustomElements.json as \"' + photon_name + '\".' ) # activate custom elements self . activate () # check custom element logger . info ( \"Running tests on custom element...\" ) return self . _run_tests ( photon_name , element_type ) else : logger . error ( 'Could not register element!' )","title":"Registry"},{"location":"api/base/registry/#documentation-for-photonregistry","text":"","title":"Documentation for PhotonRegistry"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry","text":"Helper class to manage the PHOTONAI Element Register. Use it to add and remove items into the register. You can also retrieve information about items and its hyperparameters. Every item in the register is encoded by a string literal that points to a python class and its namespace. You can access the python class via the string literal. The class PhotonElement imports and instantiates the class for you. Examples: import os from photonai.base import PhotonRegistry # REGISTER ELEMENT saved in folder custom_elements_folder base_folder = os.path.dirname(os.path.abspath(__file__)) custom_elements_folder = os.path.join(base_folder, 'custom_elements') registry = PhotonRegistry(custom_elements_folder=custom_elements_folder) registry.register(photon_name='MyCustomEstimator', class_str='custom_estimator.CustomEstimator', element_type='Estimator') registry.activate() registry.info('MyCustomEstimator') # get informations of other available elements registry.info('SVC')","title":"photonai.base.registry.registry.PhotonRegistry"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.__init__","text":"Initialize the object. Parameters: Name Type Description Default custom_elements_folder str Path to folder with custom element in it. None Source code in photonai/base/registry/registry.py def __init__ ( self , custom_elements_folder : str = None ): \"\"\" Initialize the object. Parameters: custom_elements_folder: Path to folder with custom element in it. \"\"\" self . current_folder = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) self . module_path = os . path . join ( self . current_folder , \"modules\" ) if not os . path . isdir ( self . module_path ): os . mkdir ( self . module_path ) # update list with available sub_elements self . _list_available_modules () PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = custom_elements_folder self . _load_custom_folder ( custom_elements_folder ) if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : PhotonRegistry . ELEMENT_DICTIONARY = self . get_package_info ()","title":"__init__()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.delete","text":"Delete Element from JSON file. Parameters: Name Type Description Default photon_name str The string literal encoding the class. required Source code in photonai/base/registry/registry.py def delete ( self , photon_name : str ): \"\"\" Delete Element from JSON file. Parameters: photon_name: The string literal encoding the class. \"\"\" if photon_name in PhotonRegistry . CUSTOM_ELEMENTS : del PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Removing the PipelineElement named \" {0} \" from CustomElements.json.' . format ( photon_name )) else : logger . info ( 'Cannot remove \" {0} \" from CustomElements.json. Element has not been registered before.' . format ( photon_name ))","title":"delete()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.get_package_info","text":"Collect all registered elements from JSON file. Parameters: Name Type Description Default photon_package list The names of the PHOTONAI submodules for which the elements should be retrieved. ['PhotonCore'] Returns: Type Description dict Dict of registered elements Source code in photonai/base/registry/registry.py def get_package_info ( self , photon_package : list = PHOTON_REGISTRIES ) -> dict : \"\"\" Collect all registered elements from JSON file. Parameters: photon_package: The names of the PHOTONAI submodules for which the elements should be retrieved. Returns: Dict of registered elements \"\"\" class_info = dict () for package in photon_package : content = self . _load_json ( package ) for idx , key in enumerate ( content ): class_path , class_name = os . path . splitext ( content [ key ][ 0 ]) if idx == 0 and package not in [ \"PhotonCore\" , \"CustomElements\" ]: # try to import something from module. # if that fails. drop this shit. try : imported_module = importlib . import_module ( class_path ) desired_class = getattr ( imported_module , class_name [ 1 :]) custom_element = desired_class () except ( AttributeError , ModuleNotFoundError ) as e : logger . error ( e ) logger . error ( \"Could not import from package {} . Deleting json.\" . format ( package )) self . delete_module ( package ) class_info [ key ] = class_path , class_name [ 1 :] return class_info","title":"get_package_info()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.info","text":"Show information for object that is encoded by this name. Parameters: Name Type Description Default photon_name str The string literal which accesses the class. required Source code in photonai/base/registry/registry.py def info ( self , photon_name : str ): \"\"\" Show information for object that is encoded by this name. Parameters: photon_name: The string literal which accesses the class. \"\"\" content = self . get_package_info () # load existing json if photon_name in content : element_namespace , element_name = content [ photon_name ] print ( \"----------------------------------\" ) print ( \"Name: \" + element_name ) print ( \"Namespace: \" + element_namespace ) print ( \"----------------------------------\" ) try : imported_module = __import__ ( element_namespace , globals (), locals (), element_name , 0 ) desired_class = getattr ( imported_module , element_name ) base_element = desired_class () print ( \"Possible Hyperparameters as derived from constructor:\" ) class_args = inspect . signature ( base_element . __init__ ) for item , more_info in class_args . parameters . items (): print ( \" {:<35} {:<75} \" . format ( item , str ( more_info ))) print ( \"----------------------------------\" ) except Exception as e : logger . error ( e ) logger . error ( \"Could not instantiate class \" + element_namespace + \".\" + element_name ) else : logger . error ( \"Could not find element \" + photon_name )","title":"info()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.list_available_elements","text":"Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: Name Type Description Default photon_package list The names of the PHOTON submodules for which the elements should be retrieved. ['PhotonCore'] Source code in photonai/base/registry/registry.py def list_available_elements ( self , photon_package : list = PHOTON_REGISTRIES ): \"\"\" Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: photon_package: The names of the PHOTON submodules for which the elements should be retrieved. \"\"\" if isinstance ( photon_package , str ): photon_package = [ photon_package ] for package in photon_package : content = self . _load_json ( package ) if len ( content ) > 0 : print ( ' \\n ' + package ) for k , v in sorted ( content . items ()): class_info , package_type = v print ( \" {:<35} {:<75} {:<5} \" . format ( k , class_info , package_type ))","title":"list_available_elements()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.register","text":"Save element information to the JSON file. Parameters: Name Type Description Default photon_name str The string literal with which you want to access the class. required class_str str The namespace of the class, like in the import statement. required element_type str Can be 'Estimator' or 'Transformer' required Source code in photonai/base/registry/registry.py def register ( self , photon_name : str , class_str : str , element_type : str ): \"\"\" Save element information to the JSON file. Parameters: photon_name: The string literal with which you want to access the class. class_str: The namespace of the class, like in the import statement. element_type: Can be 'Estimator' or 'Transformer' \"\"\" # check if folder exists if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To register an element, specify a custom elements folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER)\" ) if not element_type == \"Estimator\" and not element_type == \"Transformer\" : raise ValueError ( \"Variable element_type must be 'Estimator' or 'Transformer'\" ) duplicate = self . _check_duplicate ( photon_name = photon_name , class_str = class_str , content = PhotonRegistry . CUSTOM_ELEMENTS ) if not duplicate : python_file = os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , class_str . split ( '.' )[ 0 ] + '.py' ) if not os . path . isfile ( python_file ): raise FileNotFoundError ( \"Couldn't find python file {} in your custom elements folder. \" \"Please copy your file into this folder first!\" . format ( python_file )) # add new element PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] = class_str , element_type # write back to file self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Adding PipelineElement ' + class_str + ' to CustomElements.json as \"' + photon_name + '\".' ) # activate custom elements self . activate () # check custom element logger . info ( \"Running tests on custom element...\" ) return self . _run_tests ( photon_name , element_type ) else : logger . error ( 'Could not register element!' )","title":"register()"},{"location":"api/base/stack/","text":"Documentation for Stack Creates a vertical stacking/parallelization of pipeline items. The object acts as single pipeline element and encapsulates several vertically stacked other pipeline elements, each child receiving the same input data. The data is iteratively distributed to all children, the results are collected and horizontally concatenated. Examples: tree = PipelineElement('DecisionTreeClassifier') svc = PipelineElement('LinearSVC') my_pipe += Stack('final_stack', [tree, svc], use_probabilities=True) __iadd__ ( self , item ) special Adds a new element to the stack. Generates sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , item : PipelineElement ): \"\"\" Adds a new element to the stack. Generates sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . check_if_needs_y ( item ) super ( Stack , self ) . __iadd__ ( item ) # for each configuration tmp_dict = dict ( item . hyperparameters ) for key , element in tmp_dict . items (): self . _hyperparameters [ self . name + '__' + key ] = tmp_dict [ key ] return self __init__ ( self , name , elements = None , use_probabilities = False ) special Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: Name Type Description Default name str Give the pipeline element a name. required elements List[photonai.base.photon_elements.PipelineElement] List of pipeline elements that should run in parallel. None use_probabilities bool For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. False Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , use_probabilities : bool = False ): \"\"\" Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: name: Give the pipeline element a name. elements: List of pipeline elements that should run in parallel. use_probabilities: For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. \"\"\" super ( Stack , self ) . __init__ ( name , hyperparameters = {}, test_disabled = False , disabled = False , base_element = True ) self . _hyperparameters = {} self . elements = list () if elements is not None : for item_to_stack in elements : self . __iadd__ ( item_to_stack ) # todo: Stack should not be allowed to change y, only covariates self . needs_y = False self . needs_covariates = True self . identifier = \"STACK:\" self . use_probabilities = use_probabilities add ( self , item ) Adds a new element to the stack. Generates sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def add ( self , item : PipelineElement ): \"\"\" Adds a new element to the stack. Generates sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . __iadd__ ( item ) fit ( self , X , y = None , ** kwargs ) Calls fit iteratively on every child. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls fit iteratively on every child. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" for element in self . elements : # Todo: parallellize fitting element . fit ( X , y , ** kwargs ) return self predict ( self , X , ** kwargs ) Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y The truth array-like values with shape=[N], where N is the number of samples. required kwargs Keyword arguments, passed to base_elements predict. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements predict. Returns: Prediction values. \"\"\" if not self . use_probabilities : return self . _predict ( X , ** kwargs ) else : return self . predict_proba ( X , ** kwargs ) predict_proba ( self , X , y = None , ** kwargs ) Predict probabilities for every pipe element and stack them together. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, not used yet. {} Returns: Type Description ndarray Probabilites. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities for every pipe element and stack them together. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, not used yet. Returns: Probabilites. \"\"\" predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict_proba ( X ) if element_transform is None : element_transform = element . predict ( X ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data transform ( self , X , y = None , ** kwargs ) Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: Name Type Description Default X ndarray The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_elements transform. {} Returns: Type Description Prediction. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: X: The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements transform. Returns: Prediction. \"\"\" transformed_data = np . array ([]) for element in self . elements : # if it is a hyperpipe with a final estimator, we want to use predict: element_transform , _ , _ = element . transform ( X , y , ** kwargs ) transformed_data = PhotonDataHelper . stack_data_horizontally ( transformed_data , element_transform ) return transformed_data , y , kwargs","title":"Stack"},{"location":"api/base/stack/#documentation-for-stack","text":"","title":"Documentation for Stack"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack","text":"Creates a vertical stacking/parallelization of pipeline items. The object acts as single pipeline element and encapsulates several vertically stacked other pipeline elements, each child receiving the same input data. The data is iteratively distributed to all children, the results are collected and horizontally concatenated. Examples: tree = PipelineElement('DecisionTreeClassifier') svc = PipelineElement('LinearSVC') my_pipe += Stack('final_stack', [tree, svc], use_probabilities=True)","title":"photonai.base.photon_elements.Stack"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.__iadd__","text":"Adds a new element to the stack. Generates sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , item : PipelineElement ): \"\"\" Adds a new element to the stack. Generates sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . check_if_needs_y ( item ) super ( Stack , self ) . __iadd__ ( item ) # for each configuration tmp_dict = dict ( item . hyperparameters ) for key , element in tmp_dict . items (): self . _hyperparameters [ self . name + '__' + key ] = tmp_dict [ key ] return self","title":"__iadd__()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.__init__","text":"Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: Name Type Description Default name str Give the pipeline element a name. required elements List[photonai.base.photon_elements.PipelineElement] List of pipeline elements that should run in parallel. None use_probabilities bool For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. False Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , use_probabilities : bool = False ): \"\"\" Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: name: Give the pipeline element a name. elements: List of pipeline elements that should run in parallel. use_probabilities: For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. \"\"\" super ( Stack , self ) . __init__ ( name , hyperparameters = {}, test_disabled = False , disabled = False , base_element = True ) self . _hyperparameters = {} self . elements = list () if elements is not None : for item_to_stack in elements : self . __iadd__ ( item_to_stack ) # todo: Stack should not be allowed to change y, only covariates self . needs_y = False self . needs_covariates = True self . identifier = \"STACK:\" self . use_probabilities = use_probabilities","title":"__init__()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.add","text":"Adds a new element to the stack. Generates sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def add ( self , item : PipelineElement ): \"\"\" Adds a new element to the stack. Generates sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . __iadd__ ( item )","title":"add()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.fit","text":"Calls fit iteratively on every child. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls fit iteratively on every child. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" for element in self . elements : # Todo: parallellize fitting element . fit ( X , y , ** kwargs ) return self","title":"fit()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.predict","text":"Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y The truth array-like values with shape=[N], where N is the number of samples. required kwargs Keyword arguments, passed to base_elements predict. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements predict. Returns: Prediction values. \"\"\" if not self . use_probabilities : return self . _predict ( X , ** kwargs ) else : return self . predict_proba ( X , ** kwargs )","title":"predict()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.predict_proba","text":"Predict probabilities for every pipe element and stack them together. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, not used yet. {} Returns: Type Description ndarray Probabilites. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities for every pipe element and stack them together. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, not used yet. Returns: Probabilites. \"\"\" predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict_proba ( X ) if element_transform is None : element_transform = element . predict ( X ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data","title":"predict_proba()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.transform","text":"Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: Name Type Description Default X ndarray The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None kwargs Keyword arguments, passed to base_elements transform. {} Returns: Type Description Prediction. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: X: The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. kwargs: Keyword arguments, passed to base_elements transform. Returns: Prediction. \"\"\" transformed_data = np . array ([]) for element in self . elements : # if it is a hyperpipe with a final estimator, we want to use predict: element_transform , _ , _ = element . transform ( X , y , ** kwargs ) transformed_data = PhotonDataHelper . stack_data_horizontally ( transformed_data , element_transform ) return transformed_data , y , kwargs","title":"transform()"},{"location":"api/base/switch/","text":"Documentation for Switch This class encapsulates several PipelineElements that belong at the same step of the pipeline, competing for being the best choice. If for example you want to find out if Preprocessing A or Preprocessing B is better at this position in the pipe. Or you want to test if a tree outperforms the good old SVM. ATTENTION: This class is a construct that may be convenient but is not suitable for any complex optimizations. Currently it only works for grid_search and the derived optimization strategies. USE THIS ONLY FOR RAPID PROTOTYPING AND PRELIMINARY RESULTS. The class acts as if it is a single entity. Tt joins the hyperparamater combinations of each encapsulated element to a single, big combination grid. Each hyperparameter combination from that grid gets a number. Then the Switch object publishes the numbers to be chosen as the object's hyperparameter. When a new number is chosen from the optimizer, it internally activates the belonging element and sets the element's parameter to the hyperparameter combination. In that way, each of the elements is tested in all its configurations at the same position in the pipeline. From the outside, the process and the optimizer only sees one parameter of the Switch, that is the an integer indicating which item of the hyperparameter combination grid is currently active. Examples: from photonai.base import PipelineElement, Switch from photonai.optimization import IntegerRange # Estimator Switch svm = PipelineElement('SVC', hyperparameters={'kernel': ['rbf', 'linear']}) tree = PipelineElement('DecisionTreeClassifier', hyperparameters={'min_samples_split': IntegerRange(2, 5), 'min_samples_leaf': IntegerRange(1, 5), 'criterion': ['gini', 'entropy']}) my_pipe += Switch('EstimatorSwitch', [svm, tree]) __iadd__ ( self , pipeline_element ) special Add a new estimator or transformer object to the switch container. All items change positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" super ( Switch , self ) . __iadd__ ( pipeline_element ) self . elements_dict [ pipeline_element . name ] = pipeline_element self . generate_private_config_grid () return self __init__ ( self , name , elements = None , estimator_name = '' ) special Creates a new Switch object and generated the hyperparameter combination grid. Parameters: Name Type Description Default name str How the element is called in the pipeline. required elements List[photonai.base.photon_elements.PipelineElement] The competing pipeline elements. None Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , estimator_name : str = '' ): \"\"\" Creates a new Switch object and generated the hyperparameter combination grid. Parameters: name: How the element is called in the pipeline. elements: The competing pipeline elements. \"\"\" self . _name = name self . initial_name = self . _name self . sklearn_name = self . name + \"__current_element\" self . _hyperparameters = {} self . _current_element = ( 1 , 1 ) self . pipeline_element_configurations = [] self . base_element = None self . disabled = False self . test_disabled = False self . batch_size = 0 self . estimator_name = estimator_name self . needs_y = True self . needs_covariates = True # we assume we test models against each other, but only guessing self . is_estimator = True self . is_transformer = True self . identifier = \"SWITCH:\" self . _random_state = False self . elements_dict = {} if elements : self . elements = elements self . generate_private_config_grid () for p_element in elements : self . elements_dict [ p_element . name ] = p_element else : self . elements = [] add ( self , pipeline_element ) Add a new estimator or transformer object to the switch container. All items change positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def add ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" self . __iadd__ ( pipeline_element ) predict_proba ( self , X , ** kwargs ) Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required kwargs Keyword arguments, not in use yet. {} Returns: Type Description ndarray Probabilities. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. kwargs: Keyword arguments, not in use yet. Returns: Probabilities. \"\"\" if not self . disabled : if hasattr ( self . base_element . base_element , 'predict_proba' ): return self . base_element . predict_proba ( X ) else : return None return X","title":"Switch"},{"location":"api/base/switch/#documentation-for-switch","text":"","title":"Documentation for Switch"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch","text":"This class encapsulates several PipelineElements that belong at the same step of the pipeline, competing for being the best choice. If for example you want to find out if Preprocessing A or Preprocessing B is better at this position in the pipe. Or you want to test if a tree outperforms the good old SVM. ATTENTION: This class is a construct that may be convenient but is not suitable for any complex optimizations. Currently it only works for grid_search and the derived optimization strategies. USE THIS ONLY FOR RAPID PROTOTYPING AND PRELIMINARY RESULTS. The class acts as if it is a single entity. Tt joins the hyperparamater combinations of each encapsulated element to a single, big combination grid. Each hyperparameter combination from that grid gets a number. Then the Switch object publishes the numbers to be chosen as the object's hyperparameter. When a new number is chosen from the optimizer, it internally activates the belonging element and sets the element's parameter to the hyperparameter combination. In that way, each of the elements is tested in all its configurations at the same position in the pipeline. From the outside, the process and the optimizer only sees one parameter of the Switch, that is the an integer indicating which item of the hyperparameter combination grid is currently active. Examples: from photonai.base import PipelineElement, Switch from photonai.optimization import IntegerRange # Estimator Switch svm = PipelineElement('SVC', hyperparameters={'kernel': ['rbf', 'linear']}) tree = PipelineElement('DecisionTreeClassifier', hyperparameters={'min_samples_split': IntegerRange(2, 5), 'min_samples_leaf': IntegerRange(1, 5), 'criterion': ['gini', 'entropy']}) my_pipe += Switch('EstimatorSwitch', [svm, tree])","title":"photonai.base.photon_elements.Switch"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.__iadd__","text":"Add a new estimator or transformer object to the switch container. All items change positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" super ( Switch , self ) . __iadd__ ( pipeline_element ) self . elements_dict [ pipeline_element . name ] = pipeline_element self . generate_private_config_grid () return self","title":"__iadd__()"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.__init__","text":"Creates a new Switch object and generated the hyperparameter combination grid. Parameters: Name Type Description Default name str How the element is called in the pipeline. required elements List[photonai.base.photon_elements.PipelineElement] The competing pipeline elements. None Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , estimator_name : str = '' ): \"\"\" Creates a new Switch object and generated the hyperparameter combination grid. Parameters: name: How the element is called in the pipeline. elements: The competing pipeline elements. \"\"\" self . _name = name self . initial_name = self . _name self . sklearn_name = self . name + \"__current_element\" self . _hyperparameters = {} self . _current_element = ( 1 , 1 ) self . pipeline_element_configurations = [] self . base_element = None self . disabled = False self . test_disabled = False self . batch_size = 0 self . estimator_name = estimator_name self . needs_y = True self . needs_covariates = True # we assume we test models against each other, but only guessing self . is_estimator = True self . is_transformer = True self . identifier = \"SWITCH:\" self . _random_state = False self . elements_dict = {} if elements : self . elements = elements self . generate_private_config_grid () for p_element in elements : self . elements_dict [ p_element . name ] = p_element else : self . elements = []","title":"__init__()"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.add","text":"Add a new estimator or transformer object to the switch container. All items change positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def add ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" self . __iadd__ ( pipeline_element )","title":"add()"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.predict_proba","text":"Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required kwargs Keyword arguments, not in use yet. {} Returns: Type Description ndarray Probabilities. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. kwargs: Keyword arguments, not in use yet. Returns: Probabilities. \"\"\" if not self . disabled : if hasattr ( self . base_element . base_element , 'predict_proba' ): return self . base_element . predict_proba ( X ) else : return None return X","title":"predict_proba()"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/","text":"Documentation for FClassifSelectPercentile Feature Selection for classification data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_classif and parameter percentile. Parameters: Name Type Description Default percentile int, default=10 Percent of features to keep. required inverse_transform ( self , X ) Reverse to original dimension. SelectPercentile.inverse_transform VarianceThreshold.inverse_transform Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_selected_features]. required Returns: Type Description Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. Source code in photonai/modelwrapper/feature_selection.py def inverse_transform ( self , X : np . ndarray ): \"\"\"Reverse to original dimension. 1. SelectPercentile.inverse_transform 2. VarianceThreshold.inverse_transform Parameters: X: The input samples of shape [n_samples, n_selected_features]. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" Xt = self . my_fs . inverse_transform ( X ) return self . var_thres . inverse_transform ( Xt ) selection: members: - init","title":"FClassifSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/#documentation-for-fclassifselectpercentile","text":"","title":"Documentation for FClassifSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/#photonai.modelwrapper.feature_selection.FClassifSelectPercentile","text":"Feature Selection for classification data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_classif and parameter percentile. Parameters: Name Type Description Default percentile int, default=10 Percent of features to keep. required","title":"photonai.modelwrapper.feature_selection.FClassifSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/#photonai.modelwrapper.feature_selection.FClassifSelectPercentile.inverse_transform","text":"Reverse to original dimension. SelectPercentile.inverse_transform VarianceThreshold.inverse_transform Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_selected_features]. required Returns: Type Description Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. Source code in photonai/modelwrapper/feature_selection.py def inverse_transform ( self , X : np . ndarray ): \"\"\"Reverse to original dimension. 1. SelectPercentile.inverse_transform 2. VarianceThreshold.inverse_transform Parameters: X: The input samples of shape [n_samples, n_selected_features]. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" Xt = self . my_fs . inverse_transform ( X ) return self . var_thres . inverse_transform ( Xt ) selection: members: - init","title":"inverse_transform()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/","text":"Documentation for FRegressionFilterPValue Feature Selection for Regression - p-value based. Fit f_regression and select all columns when p_value of column < p_threshold. __init__ ( self , p_threshold = 0.05 ) special Initialize the object. Parameters: Name Type Description Default p_threshold float Upper bound for p_values. 0.05 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , p_threshold : float = . 05 ): \"\"\" Initialize the object. Parameters: p_threshold: Upper bound for p_values. \"\"\" self . p_threshold = p_threshold self . selected_indices = [] self . n_original_features = None fit ( self , X , y ) Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_original_features] required y ndarray The input targets of shape [n_samples, 1] required Source code in photonai/modelwrapper/feature_selection.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: X: The input samples of shape [n_samples, n_original_features] y: The input targets of shape [n_samples, 1] \"\"\" self . n_original_features = X . shape [ 1 ] _ , p_values = f_regression ( X , y ) self . selected_indices = np . where ( p_values < self . p_threshold )[ 0 ] return self inverse_transform ( self , X ) Reverse to original dimension. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_selected_features]. required Exceptions: Type Description ValueError If input X has a different shape than during fitting. Returns: Type Description ndarray Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. Source code in photonai/modelwrapper/feature_selection.py def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. Parameters: X: The input samples of shape [n_samples, n_selected_features]. Raises: ValueError: If input X has a different shape than during fitting. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt transform ( self , X ) Reduced input X to selected_columns. Returns: Type Description ndarray Column-filtered array of shape [n_samples, n_selected_features]. Source code in photonai/modelwrapper/feature_selection.py def transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reduced input X to selected_columns. Parameters: X The input samples of shape [n_samples, n_original_features] Returns: Column-filtered array of shape [n_samples, n_selected_features]. \"\"\" return X [:, self . selected_indices ] selection: members: - init - fit - transform - inverse_transform","title":"FRegressionFilterPValue"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#documentation-for-fregressionfilterpvalue","text":"","title":"Documentation for FRegressionFilterPValue"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue","text":"Feature Selection for Regression - p-value based. Fit f_regression and select all columns when p_value of column < p_threshold.","title":"photonai.modelwrapper.feature_selection.FRegressionFilterPValue"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.__init__","text":"Initialize the object. Parameters: Name Type Description Default p_threshold float Upper bound for p_values. 0.05 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , p_threshold : float = . 05 ): \"\"\" Initialize the object. Parameters: p_threshold: Upper bound for p_values. \"\"\" self . p_threshold = p_threshold self . selected_indices = [] self . n_original_features = None","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.fit","text":"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_original_features] required y ndarray The input targets of shape [n_samples, 1] required Source code in photonai/modelwrapper/feature_selection.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: X: The input samples of shape [n_samples, n_original_features] y: The input targets of shape [n_samples, 1] \"\"\" self . n_original_features = X . shape [ 1 ] _ , p_values = f_regression ( X , y ) self . selected_indices = np . where ( p_values < self . p_threshold )[ 0 ] return self","title":"fit()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.inverse_transform","text":"Reverse to original dimension. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_selected_features]. required Exceptions: Type Description ValueError If input X has a different shape than during fitting. Returns: Type Description ndarray Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. Source code in photonai/modelwrapper/feature_selection.py def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. Parameters: X: The input samples of shape [n_samples, n_selected_features]. Raises: ValueError: If input X has a different shape than during fitting. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt","title":"inverse_transform()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.transform","text":"Reduced input X to selected_columns. Returns: Type Description ndarray Column-filtered array of shape [n_samples, n_selected_features]. Source code in photonai/modelwrapper/feature_selection.py def transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reduced input X to selected_columns. Parameters: X The input samples of shape [n_samples, n_original_features] Returns: Column-filtered array of shape [n_samples, n_selected_features]. \"\"\" return X [:, self . selected_indices ] selection: members: - init - fit - transform - inverse_transform","title":"transform()"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/","text":"Documentation for FRegressionSelectPercentile Feature Selection for regression data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_regression and parameter percentile. __init__ ( self , percentile = 10 ) special Initialize the object. Parameters: Name Type Description Default percentile (<class 'float'>, <class 'int'>) Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : ( float , int ) = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None selection: members: - init","title":"FRegressionSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/#documentation-for-fregressionselectpercentile","text":"","title":"Documentation for FRegressionSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/#photonai.modelwrapper.feature_selection.FRegressionSelectPercentile","text":"Feature Selection for regression data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_regression and parameter percentile.","title":"photonai.modelwrapper.feature_selection.FRegressionSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/#photonai.modelwrapper.feature_selection.FRegressionSelectPercentile.__init__","text":"Initialize the object. Parameters: Name Type Description Default percentile (<class 'float'>, <class 'int'>) Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : ( float , int ) = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None selection: members: - init","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/","text":"Documentation for LassoFeatureSelection Lasso based feature selection - based on feature_importance. Apply Lasso to ModelSelection. __init__ ( self , percentile = 0.3 , alpha = 1.0 , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default percentile float bool, default=False Percent of features to keep. 0.3 alpha float float, default=1. Weighting parameter for Lasso. 1.0 kwargs Passed to Lasso object. {} Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 0.3 , alpha : float = 1. , ** kwargs ): \"\"\" Initialize the object. Parameters: percentile: bool, default=False Percent of features to keep. alpha: float, default=1. Weighting parameter for Lasso. kwargs: Passed to Lasso object. \"\"\" self . percentile = percentile self . alpha = alpha self . model_selector = None self . Lasso_kwargs = kwargs self . needs_covariates = False self . needs_y = False set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. Source code in photonai/modelwrapper/feature_selection.py def set_params ( self , ** params ): super ( LassoFeatureSelection , self ) . set_params ( ** params ) selection: members: - init","title":"LassoFeatureSelection"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#documentation-for-lassofeatureselection","text":"","title":"Documentation for LassoFeatureSelection"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#photonai.modelwrapper.feature_selection.LassoFeatureSelection","text":"Lasso based feature selection - based on feature_importance. Apply Lasso to ModelSelection.","title":"photonai.modelwrapper.feature_selection.LassoFeatureSelection"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#photonai.modelwrapper.feature_selection.LassoFeatureSelection.__init__","text":"Initialize the object. Parameters: Name Type Description Default percentile float bool, default=False Percent of features to keep. 0.3 alpha float float, default=1. Weighting parameter for Lasso. 1.0 kwargs Passed to Lasso object. {} Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 0.3 , alpha : float = 1. , ** kwargs ): \"\"\" Initialize the object. Parameters: percentile: bool, default=False Percent of features to keep. alpha: float, default=1. Weighting parameter for Lasso. kwargs: Passed to Lasso object. \"\"\" self . percentile = percentile self . alpha = alpha self . model_selector = None self . Lasso_kwargs = kwargs self . needs_covariates = False self . needs_y = False","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#photonai.modelwrapper.feature_selection.LassoFeatureSelection.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#photonai.modelwrapper.feature_selection.LassoFeatureSelection.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#photonai.modelwrapper.feature_selection.LassoFeatureSelection.set_params--returns","text":"self : estimator instance Estimator instance. Source code in photonai/modelwrapper/feature_selection.py def set_params ( self , ** params ): super ( LassoFeatureSelection , self ) . set_params ( ** params ) selection: members: - init","title":"Returns"},{"location":"api/modelwrapper/feature_selection/ModelSelector/","text":"Documentation for ModelSelector Model Selector - based on feature_importance. Apply feature selection on specific estimator and its importance scores. __init__ ( self , estimator_obj , threshold = 1e-05 , percentile = False ) special Initialize the object. Parameters: Name Type Description Default estimator_obj Estimator with fit/tranform and possibility of feature_importance. required threshold float If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) 1e-05 percentile bool Percent of features to keep. False Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , estimator_obj , threshold : float = 1e-5 , percentile : bool = False ): \"\"\" Initialize the object. Parameters: estimator_obj: Estimator with fit/tranform and possibility of feature_importance. threshold: If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) percentile: Percent of features to keep. \"\"\" self . threshold = threshold self . estimator_obj = estimator_obj self . selected_indices = [] self . percentile = percentile self . importance_scores = [] self . n_original_features = None get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. Source code in photonai/modelwrapper/feature_selection.py def get_params ( self , deep = True ): return self . estimator_obj . get_params ( deep ) set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. Source code in photonai/modelwrapper/feature_selection.py def set_params ( self , ** params ): if 'threshold' in params : self . threshold = params [ 'threshold' ] params . pop ( 'threshold' ) self . estimator_obj . set_params ( ** params ) selection: members: - init","title":"ModelSelector"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#documentation-for-modelselector","text":"","title":"Documentation for ModelSelector"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector","text":"Model Selector - based on feature_importance. Apply feature selection on specific estimator and its importance scores.","title":"photonai.modelwrapper.feature_selection.ModelSelector"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector.__init__","text":"Initialize the object. Parameters: Name Type Description Default estimator_obj Estimator with fit/tranform and possibility of feature_importance. required threshold float If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) 1e-05 percentile bool Percent of features to keep. False Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , estimator_obj , threshold : float = 1e-5 , percentile : bool = False ): \"\"\" Initialize the object. Parameters: estimator_obj: Estimator with fit/tranform and possibility of feature_importance. threshold: If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) percentile: Percent of features to keep. \"\"\" self . threshold = threshold self . estimator_obj = estimator_obj self . selected_indices = [] self . percentile = percentile self . importance_scores = [] self . n_original_features = None","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector.get_params--returns","text":"params : dict Parameter names mapped to their values. Source code in photonai/modelwrapper/feature_selection.py def get_params ( self , deep = True ): return self . estimator_obj . get_params ( deep )","title":"Returns"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector.set_params--returns","text":"self : estimator instance Estimator instance. Source code in photonai/modelwrapper/feature_selection.py def set_params ( self , ** params ): if 'threshold' in params : self . threshold = params [ 'threshold' ] params . pop ( 'threshold' ) self . estimator_obj . set_params ( ** params ) selection: members: - init","title":"Returns"},{"location":"api/optimization/grid_search/","text":"Documentation for GridSearchOptimizer Grid search optimizer. Searches for the best configuration by iteratively testing all possible hyperparameter combinations. Examples: my_pipe = Hyperpipe(name='grid_based_pipe', optimizer='grid_search', ... ) my_pipe.fit(X, y) __init__ ( self ) special Initialize the object. Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . param_grid = [] self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () next_config_generator ( self ) Generator for new configs - ask method. Returns: Type Description Generator next_config: Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: next_config: Yields the next config. \"\"\" for parameters in self . param_grid : yield parameters prepare ( self , pipeline_elements , maximize_metric ) Prepare grid based hyperparameter search. Creates a grid from input pipeline_elements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\"Prepare grid based hyperparameter search. Creates a grid from input pipeline_elements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. \"\"\" self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () self . param_grid = create_global_config_grid ( self . pipeline_elements ) logger . info ( \"Grid Search generated \" + str ( len ( self . param_grid )) + \" configurations\" )","title":"GridSearch"},{"location":"api/optimization/grid_search/#documentation-for-gridsearchoptimizer","text":"","title":"Documentation for GridSearchOptimizer"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer","text":"Grid search optimizer. Searches for the best configuration by iteratively testing all possible hyperparameter combinations. Examples: my_pipe = Hyperpipe(name='grid_based_pipe', optimizer='grid_search', ... ) my_pipe.fit(X, y)","title":"photonai.optimization.grid_search.grid_search.GridSearchOptimizer"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer.__init__","text":"Initialize the object. Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . param_grid = [] self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator ()","title":"__init__()"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer.next_config_generator","text":"Generator for new configs - ask method. Returns: Type Description Generator next_config: Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: next_config: Yields the next config. \"\"\" for parameters in self . param_grid : yield parameters","title":"next_config_generator()"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer.prepare","text":"Prepare grid based hyperparameter search. Creates a grid from input pipeline_elements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\"Prepare grid based hyperparameter search. Creates a grid from input pipeline_elements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. \"\"\" self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () self . param_grid = create_global_config_grid ( self . pipeline_elements ) logger . info ( \"Grid Search generated \" + str ( len ( self . param_grid )) + \" configurations\" )","title":"prepare()"},{"location":"api/optimization/nevergrad/","text":"Documentation for NevergradOptimizer Nevergrad Wrapper for PHOTONAI. Nevergrad is a gradient-free optimization platform. Nevergrad usage and implementation details Examples: import nevergrad as ng # list of all available nevergrad optimizer print(list(ng.optimizers.registry.values())) my_pipe = Hyperpipe('nevergrad_example', optimizer='nevergrad', optimizer_params={'facade': 'NGO', 'n_configurations': 30}, ... ) __init__ ( self , facade = 'NGO' , n_configurations = 100 , rng = 42 ) special Initialize the object. Parameters: Name Type Description Default facade Choice of Nevergrad backend strategy, [NGO, ...]. 'NGO' n_configurations int Number of runs. 100 rng int Random Seed. 42 Source code in photonai/optimization/nevergrad/nevergrad.py def __init__ ( self , facade = 'NGO' , n_configurations : int = 100 , rng : int = 42 ): \"\"\" Initialize the object. Parameters: facade: Choice of Nevergrad backend strategy, [NGO, ...]. n_configurations: Number of runs. rng: Random Seed. \"\"\" if not __found__ : msg = \"Module nevergrad not found or not installed as expected. \" \\ \"Please install the nevergrad/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) if facade in list ( ng . optimizers . registry . values ()): self . facade = facade elif facade in list ( ng . optimizers . registry . keys ()): self . facade = ng . optimizers . registry [ facade ] else : msg = \"nevergrad.optimizer {} not known. Check out all available nevergrad optimizers \" \\ \"by nevergrad.optimizers.registry.keys()\" . format ( str ( facade )) logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . n_configurations = n_configurations self . space = None # Hyperparameter space for nevergrad self . switch_optiones = {} self . hyperparameters = [] self . rng = rng self . maximize_metric = False self . constant_dictionary = {} self . objective = None self . optimizer = None optimize ( self ) Start optimization over objective_function. Source code in photonai/optimization/nevergrad/nevergrad.py def optimize ( self ) -> None : self . optimizer . minimize ( self . objective ) prepare ( self , pipeline_elements , maximize_metric , objective_function ) Prepare Nevergrad Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/nevergrad/nevergrad.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ) -> None : \"\"\"Prepare Nevergrad Optimizer. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . space = self . _build_nevergrad_space ( pipeline_elements ) self . space . random_state . seed ( self . rng ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric def nevergrad_objective_function ( ** current_config ): return objective_function ( current_config ) self . objective = nevergrad_objective_function self . optimizer = self . facade ( parametrization = self . space , budget = self . n_configurations )","title":"Nevergrad"},{"location":"api/optimization/nevergrad/#documentation-for-nevergradoptimizer","text":"","title":"Documentation for NevergradOptimizer"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer","text":"Nevergrad Wrapper for PHOTONAI. Nevergrad is a gradient-free optimization platform. Nevergrad usage and implementation details Examples: import nevergrad as ng # list of all available nevergrad optimizer print(list(ng.optimizers.registry.values())) my_pipe = Hyperpipe('nevergrad_example', optimizer='nevergrad', optimizer_params={'facade': 'NGO', 'n_configurations': 30}, ... )","title":"photonai.optimization.nevergrad.nevergrad.NevergradOptimizer"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default facade Choice of Nevergrad backend strategy, [NGO, ...]. 'NGO' n_configurations int Number of runs. 100 rng int Random Seed. 42 Source code in photonai/optimization/nevergrad/nevergrad.py def __init__ ( self , facade = 'NGO' , n_configurations : int = 100 , rng : int = 42 ): \"\"\" Initialize the object. Parameters: facade: Choice of Nevergrad backend strategy, [NGO, ...]. n_configurations: Number of runs. rng: Random Seed. \"\"\" if not __found__ : msg = \"Module nevergrad not found or not installed as expected. \" \\ \"Please install the nevergrad/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) if facade in list ( ng . optimizers . registry . values ()): self . facade = facade elif facade in list ( ng . optimizers . registry . keys ()): self . facade = ng . optimizers . registry [ facade ] else : msg = \"nevergrad.optimizer {} not known. Check out all available nevergrad optimizers \" \\ \"by nevergrad.optimizers.registry.keys()\" . format ( str ( facade )) logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . n_configurations = n_configurations self . space = None # Hyperparameter space for nevergrad self . switch_optiones = {} self . hyperparameters = [] self . rng = rng self . maximize_metric = False self . constant_dictionary = {} self . objective = None self . optimizer = None","title":"__init__()"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer.optimize","text":"Start optimization over objective_function. Source code in photonai/optimization/nevergrad/nevergrad.py def optimize ( self ) -> None : self . optimizer . minimize ( self . objective )","title":"optimize()"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer.prepare","text":"Prepare Nevergrad Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/nevergrad/nevergrad.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ) -> None : \"\"\"Prepare Nevergrad Optimizer. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . space = self . _build_nevergrad_space ( pipeline_elements ) self . space . random_state . seed ( self . rng ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric def nevergrad_objective_function ( ** current_config ): return objective_function ( current_config ) self . objective = nevergrad_objective_function self . optimizer = self . facade ( parametrization = self . space , budget = self . n_configurations )","title":"prepare()"},{"location":"api/optimization/random_grid_search/","text":"Documentation for RandomGridSearchOptimizer Random grid search optimizer. Searches for the best configuration by randomly testing n possible hyperparameter combinations. Examples: my_pipe = Hyperpipe(name='rgrid_based_pipe', optimizer='random_grid_search', optimizer_params={'n_configurations': 50}, ... ) my_pipe.fit(X, y) __init__ ( self , n_configurations = 25 ) special Initialize the object. Parameters: Name Type Description Default n_configurations Optional[int] int or None, default=25 Number of configurations to be calculated. 25 Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self , n_configurations : Union [ int , None ] = 25 ): \"\"\" Initialize the object. Parameters: n_configurations: int or None, default=25 Number of configurations to be calculated. \"\"\" super ( RandomGridSearchOptimizer , self ) . __init__ () self . _k = n_configurations self . n_configurations = self . _k prepare ( self , pipeline_elements , maximize_metric ) Prepare hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\"Prepare hyperparameter search. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. \"\"\" super ( RandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . n_configurations = self . _k self . param_grid = list ( self . param_grid ) # create random order in list np . random . shuffle ( self . param_grid ) if self . n_configurations is not None : # k is maximal all grid items if self . n_configurations > len ( self . param_grid ): self . n_configurations = len ( self . param_grid ) self . param_grid = self . param_grid [ 0 : self . n_configurations ]","title":"RandomGridSearch"},{"location":"api/optimization/random_grid_search/#documentation-for-randomgridsearchoptimizer","text":"","title":"Documentation for RandomGridSearchOptimizer"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer","text":"Random grid search optimizer. Searches for the best configuration by randomly testing n possible hyperparameter combinations. Examples: my_pipe = Hyperpipe(name='rgrid_based_pipe', optimizer='random_grid_search', optimizer_params={'n_configurations': 50}, ... ) my_pipe.fit(X, y)","title":"photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default n_configurations Optional[int] int or None, default=25 Number of configurations to be calculated. 25 Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self , n_configurations : Union [ int , None ] = 25 ): \"\"\" Initialize the object. Parameters: n_configurations: int or None, default=25 Number of configurations to be calculated. \"\"\" super ( RandomGridSearchOptimizer , self ) . __init__ () self . _k = n_configurations self . n_configurations = self . _k","title":"__init__()"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer.prepare","text":"Prepare hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\"Prepare hyperparameter search. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. \"\"\" super ( RandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . n_configurations = self . _k self . param_grid = list ( self . param_grid ) # create random order in list np . random . shuffle ( self . param_grid ) if self . n_configurations is not None : # k is maximal all grid items if self . n_configurations > len ( self . param_grid ): self . n_configurations = len ( self . param_grid ) self . param_grid = self . param_grid [ 0 : self . n_configurations ]","title":"prepare()"},{"location":"api/optimization/skopt/","text":"Documentation for SkOptOptimizer Wrapper for Scikit-Optimize with PHOTONAI. Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details A detailed parameter documentation here. Examples: my_pipe = Hyperpipe('skopt_example', optimizer='sk_opt', optimizer_params={'n_configurations': 25, 'acq_func': 'LCB', 'acq_func_kwargs': {'kappa': 1.96}}, ...) __init__ ( self , n_configurations = 20 , n_initial_points = 10 , base_estimator = 'ET' , initial_point_generator = 'random' , acq_func = 'gp_hedge' , acq_func_kwargs = None ) special Initialize the object. Parameters: Name Type Description Default n_configurations int Number of configurations to be calculated. 20 n_initial_points int Number of evaluations with initialization points before approximating it with base_estimator . 10 base_estimator Union[str, sklearn.base.RegressorMixin] Estimator for returning std(Y | x) along with E[Y | x]. 'ET' initial_point_generator str Generator for initial points. 'random' acq_func str Function to minimize over the posterior distribution. 'gp_hedge' acq_func_kwargs dict Additional arguments to be passed to the acquisition function. None Source code in photonai/optimization/scikit_optimize/sk_opt.py def __init__ ( self , n_configurations : int = 20 , n_initial_points : int = 10 , base_estimator : Union [ str , sklearn . base . RegressorMixin ] = \"ET\" , initial_point_generator : str = \"random\" , acq_func : str = 'gp_hedge' , acq_func_kwargs : dict = None ): \"\"\" Initialize the object. Parameters: n_configurations: Number of configurations to be calculated. n_initial_points: Number of evaluations with initialization points before approximating it with `base_estimator`. base_estimator: Estimator for returning std(Y | x) along with E[Y | x]. initial_point_generator: Generator for initial points. acq_func: Function to minimize over the posterior distribution. acq_func_kwargs: Additional arguments to be passed to the acquisition function. \"\"\" self . metric_to_optimize = '' self . n_configurations = n_configurations self . n_initial_points = n_initial_points self . base_estimator = base_estimator self . initial_point_generator = initial_point_generator self . acq_func = acq_func self . acq_func_kwargs = acq_func_kwargs self . optimizer = None self . maximize_metric = None self . hyperparameter_list = [] self . constant_dictionary = {} self . ask = self . ask_generator () prepare ( self , pipeline_elements , maximize_metric ) Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the pipeline_element list in order to prepare the hyperparameter search space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\"Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the pipeline_element list in order to prepare the hyperparameter search space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. \"\"\" self . optimizer = None self . hyperparameter_list = [] self . maximize_metric = maximize_metric # build skopt space space = [] for pipe_element in pipeline_elements : if pipe_element . __class__ . __name__ == 'Switch' : error_msg = 'Scikit-Optimize cannot operate in the specified hyperparameter space with a Switch ' \\ 'element. We recommend the use of SMAC.' logger . error ( error_msg ) raise ValueError ( error_msg ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue skopt_param = self . _convert_photonai_to_skopt_space ( value , name ) if skopt_param is not None : space . append ( skopt_param ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) if len ( space ) == 0 : msg = \"Did not find any hyperparameter to convert into skopt space.\" logger . warning ( msg ) warnings . warn ( msg ) else : self . optimizer = Optimizer ( space , base_estimator = self . base_estimator , n_initial_points = self . n_initial_points , initial_point_generator = self . initial_point_generator , acq_func = self . acq_func , acq_func_kwargs = self . acq_func_kwargs ) self . ask = self . ask_generator () tell ( self , config , performance ) Provide result for skopt optimizer to calculate new ones. Parameters: Name Type Description Default config dict dict The configuration that has been trained and tested. required performance float dict Metrics about the configuration's generalization capabilities. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def tell ( self , config : dict , performance : float ) -> None : \"\"\"Provide result for skopt optimizer to calculate new ones. Parameters: config: dict The configuration that has been trained and tested. performance: dict Metrics about the configuration's generalization capabilities. \"\"\" # convert dictionary to list in correct order if self . optimizer is not None : config_values = [ config [ name ] for name in self . hyperparameter_list ] best_config_metric_performance = performance if self . maximize_metric : best_config_metric_performance = - best_config_metric_performance self . optimizer . tell ( config_values , best_config_metric_performance )","title":"Scikit-Optimize"},{"location":"api/optimization/skopt/#documentation-for-skoptoptimizer","text":"","title":"Documentation for SkOptOptimizer"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer","text":"Wrapper for Scikit-Optimize with PHOTONAI. Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details A detailed parameter documentation here. Examples: my_pipe = Hyperpipe('skopt_example', optimizer='sk_opt', optimizer_params={'n_configurations': 25, 'acq_func': 'LCB', 'acq_func_kwargs': {'kappa': 1.96}}, ...)","title":"photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default n_configurations int Number of configurations to be calculated. 20 n_initial_points int Number of evaluations with initialization points before approximating it with base_estimator . 10 base_estimator Union[str, sklearn.base.RegressorMixin] Estimator for returning std(Y | x) along with E[Y | x]. 'ET' initial_point_generator str Generator for initial points. 'random' acq_func str Function to minimize over the posterior distribution. 'gp_hedge' acq_func_kwargs dict Additional arguments to be passed to the acquisition function. None Source code in photonai/optimization/scikit_optimize/sk_opt.py def __init__ ( self , n_configurations : int = 20 , n_initial_points : int = 10 , base_estimator : Union [ str , sklearn . base . RegressorMixin ] = \"ET\" , initial_point_generator : str = \"random\" , acq_func : str = 'gp_hedge' , acq_func_kwargs : dict = None ): \"\"\" Initialize the object. Parameters: n_configurations: Number of configurations to be calculated. n_initial_points: Number of evaluations with initialization points before approximating it with `base_estimator`. base_estimator: Estimator for returning std(Y | x) along with E[Y | x]. initial_point_generator: Generator for initial points. acq_func: Function to minimize over the posterior distribution. acq_func_kwargs: Additional arguments to be passed to the acquisition function. \"\"\" self . metric_to_optimize = '' self . n_configurations = n_configurations self . n_initial_points = n_initial_points self . base_estimator = base_estimator self . initial_point_generator = initial_point_generator self . acq_func = acq_func self . acq_func_kwargs = acq_func_kwargs self . optimizer = None self . maximize_metric = None self . hyperparameter_list = [] self . constant_dictionary = {} self . ask = self . ask_generator ()","title":"__init__()"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer.prepare","text":"Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the pipeline_element list in order to prepare the hyperparameter search space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\"Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the pipeline_element list in order to prepare the hyperparameter search space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. \"\"\" self . optimizer = None self . hyperparameter_list = [] self . maximize_metric = maximize_metric # build skopt space space = [] for pipe_element in pipeline_elements : if pipe_element . __class__ . __name__ == 'Switch' : error_msg = 'Scikit-Optimize cannot operate in the specified hyperparameter space with a Switch ' \\ 'element. We recommend the use of SMAC.' logger . error ( error_msg ) raise ValueError ( error_msg ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue skopt_param = self . _convert_photonai_to_skopt_space ( value , name ) if skopt_param is not None : space . append ( skopt_param ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) if len ( space ) == 0 : msg = \"Did not find any hyperparameter to convert into skopt space.\" logger . warning ( msg ) warnings . warn ( msg ) else : self . optimizer = Optimizer ( space , base_estimator = self . base_estimator , n_initial_points = self . n_initial_points , initial_point_generator = self . initial_point_generator , acq_func = self . acq_func , acq_func_kwargs = self . acq_func_kwargs ) self . ask = self . ask_generator ()","title":"prepare()"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer.tell","text":"Provide result for skopt optimizer to calculate new ones. Parameters: Name Type Description Default config dict dict The configuration that has been trained and tested. required performance float dict Metrics about the configuration's generalization capabilities. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def tell ( self , config : dict , performance : float ) -> None : \"\"\"Provide result for skopt optimizer to calculate new ones. Parameters: config: dict The configuration that has been trained and tested. performance: dict Metrics about the configuration's generalization capabilities. \"\"\" # convert dictionary to list in correct order if self . optimizer is not None : config_values = [ config [ name ] for name in self . hyperparameter_list ] best_config_metric_performance = performance if self . maximize_metric : best_config_metric_performance = - best_config_metric_performance self . optimizer . tell ( config_values , best_config_metric_performance )","title":"tell()"},{"location":"api/optimization/smac/","text":"Documentation for SMACOptimizer SMAC Wrapper for PHOTONAI. SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details here . References: Hutter, F. and Hoos, H. H. and Leyton-Brown, K. Sequential Model-Based Optimization for General Algorithm Configuration In: Proceedings of the conference on Learning and Intelligent OptimizatioN (LION 5) Examples: my_pipe = Hyperpipe('smac_example', optimizer='smac', optimizer_params={\"facade\": \"SMAC4BO\", \"wallclock_limit\": 60.0*10, # seconds \"ta_run_limit\": 100}, # limit of configurations ...) __init__ ( self , facade = 'SMAC4HPO' , run_obj = 'quality' , deterministic = 'true' , wallclock_limit = 60.0 , intensifier_kwargs = None , rng = 42 , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default facade Choice of SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. 'SMAC4HPO' run_obj str Defines what metric to optimize. When optimizing runtime, cutoff_time is required as well. 'quality' wallclock_limit float Maximum amount of wallclock-time used for optimization. 60.0 deterministic str If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. 'true' intensifier_kwargs dict Dict for intensifier settings. None rng int random seed of SMAC.facade 42 kwargs All initial kwargs are passed to SMACs scenario. List of all a vailable parameters . {} Source code in photonai/optimization/smac/smac.py def __init__ ( self , facade = 'SMAC4HPO' , run_obj : str = \"quality\" , deterministic : str = \"true\" , wallclock_limit : float = 60.0 , intensifier_kwargs : dict = None , rng : int = 42 , ** kwargs ): \"\"\" Initialize the object. Parameters: facade: Choice of SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. run_obj: Defines what metric to optimize. When optimizing runtime, cutoff_time is required as well. wallclock_limit: Maximum amount of wallclock-time used for optimization. deterministic: If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. intensifier_kwargs: Dict for intensifier settings. rng: random seed of SMAC.facade kwargs: All initial kwargs are passed to SMACs scenario. [List of all a vailable parameters]( https://automl.github.io/SMAC3/master/options.html#scenario). \"\"\" super ( SMACOptimizer , self ) . __init__ () if not __found__ : msg = \"Module smac not found or not installed as expected. \" \\ \"Please install the smac/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) self . run_obj = run_obj self . deterministic = deterministic self . wallclock_limit = wallclock_limit self . kwargs = kwargs if facade in [ \"SMAC4BO\" , SMAC4BO , \"SMAC4AC\" , SMAC4AC , \"SMAC4HPO\" , SMAC4HPO , \"BOHB4HPO\" , BOHB4HPO ]: if type ( facade ) == str : self . facade = eval ( facade ) else : self . facade = facade else : msg = \"SMAC.facade {} not known. Please use one of ['SMAC4BO', 'SMAC4AC', 'SMAC4HPO'].\" logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . rng = rng if not intensifier_kwargs : self . intensifier_kwargs = {} else : self . intensifier_kwargs = intensifier_kwargs self . cspace = ConfigurationSpace () # hyperparameter space for SMAC self . switch_optiones = {} self . hyperparameters = [] self . maximize_metric = False self . constant_dictionary = {} optimize ( self ) Start optimization process. Source code in photonai/optimization/smac/smac.py def optimize ( self ): \"\"\"Start optimization process.\"\"\" self . smac . optimize () prepare ( self , pipeline_elements , maximize_metric , objective_function ) Initializes SMAC Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter space. required maximize_metric bool Boolean for distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/smac/smac.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ): \"\"\"Initializes SMAC Optimizer. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter space. maximize_metric: Boolean for distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . cspace = ConfigurationSpace () # build space self . _build_smac_space ( pipeline_elements ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric scenario_dict = self . kwargs scenario_dict . update ({ \"run_obj\" : self . run_obj , \"deterministic\" : self . deterministic , \"wallclock_limit\" : self . wallclock_limit , \"cs\" : self . cspace , \"limit_resources\" : False }) scenario = Scenario ( scenario_dict ) def smac_objective_function ( current_config ): current_config = { k : current_config [ k ] for k in current_config if ( current_config [ k ] and 'algos' not in k )} return objective_function ( current_config ) self . smac = self . facade ( scenario = scenario , intensifier_kwargs = self . intensifier_kwargs , rng = self . rng , tae_runner = smac_objective_function )","title":"SMAC"},{"location":"api/optimization/smac/#documentation-for-smacoptimizer","text":"","title":"Documentation for SMACOptimizer"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer","text":"SMAC Wrapper for PHOTONAI. SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details here . References: Hutter, F. and Hoos, H. H. and Leyton-Brown, K. Sequential Model-Based Optimization for General Algorithm Configuration In: Proceedings of the conference on Learning and Intelligent OptimizatioN (LION 5) Examples: my_pipe = Hyperpipe('smac_example', optimizer='smac', optimizer_params={\"facade\": \"SMAC4BO\", \"wallclock_limit\": 60.0*10, # seconds \"ta_run_limit\": 100}, # limit of configurations ...)","title":"photonai.optimization.smac.smac.SMACOptimizer"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default facade Choice of SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. 'SMAC4HPO' run_obj str Defines what metric to optimize. When optimizing runtime, cutoff_time is required as well. 'quality' wallclock_limit float Maximum amount of wallclock-time used for optimization. 60.0 deterministic str If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. 'true' intensifier_kwargs dict Dict for intensifier settings. None rng int random seed of SMAC.facade 42 kwargs All initial kwargs are passed to SMACs scenario. List of all a vailable parameters . {} Source code in photonai/optimization/smac/smac.py def __init__ ( self , facade = 'SMAC4HPO' , run_obj : str = \"quality\" , deterministic : str = \"true\" , wallclock_limit : float = 60.0 , intensifier_kwargs : dict = None , rng : int = 42 , ** kwargs ): \"\"\" Initialize the object. Parameters: facade: Choice of SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. run_obj: Defines what metric to optimize. When optimizing runtime, cutoff_time is required as well. wallclock_limit: Maximum amount of wallclock-time used for optimization. deterministic: If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. intensifier_kwargs: Dict for intensifier settings. rng: random seed of SMAC.facade kwargs: All initial kwargs are passed to SMACs scenario. [List of all a vailable parameters]( https://automl.github.io/SMAC3/master/options.html#scenario). \"\"\" super ( SMACOptimizer , self ) . __init__ () if not __found__ : msg = \"Module smac not found or not installed as expected. \" \\ \"Please install the smac/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) self . run_obj = run_obj self . deterministic = deterministic self . wallclock_limit = wallclock_limit self . kwargs = kwargs if facade in [ \"SMAC4BO\" , SMAC4BO , \"SMAC4AC\" , SMAC4AC , \"SMAC4HPO\" , SMAC4HPO , \"BOHB4HPO\" , BOHB4HPO ]: if type ( facade ) == str : self . facade = eval ( facade ) else : self . facade = facade else : msg = \"SMAC.facade {} not known. Please use one of ['SMAC4BO', 'SMAC4AC', 'SMAC4HPO'].\" logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . rng = rng if not intensifier_kwargs : self . intensifier_kwargs = {} else : self . intensifier_kwargs = intensifier_kwargs self . cspace = ConfigurationSpace () # hyperparameter space for SMAC self . switch_optiones = {} self . hyperparameters = [] self . maximize_metric = False self . constant_dictionary = {}","title":"__init__()"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer.optimize","text":"Start optimization process. Source code in photonai/optimization/smac/smac.py def optimize ( self ): \"\"\"Start optimization process.\"\"\" self . smac . optimize ()","title":"optimize()"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer.prepare","text":"Initializes SMAC Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter space. required maximize_metric bool Boolean for distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/smac/smac.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ): \"\"\"Initializes SMAC Optimizer. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter space. maximize_metric: Boolean for distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . cspace = ConfigurationSpace () # build space self . _build_smac_space ( pipeline_elements ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric scenario_dict = self . kwargs scenario_dict . update ({ \"run_obj\" : self . run_obj , \"deterministic\" : self . deterministic , \"wallclock_limit\" : self . wallclock_limit , \"cs\" : self . cspace , \"limit_resources\" : False }) scenario = Scenario ( scenario_dict ) def smac_objective_function ( current_config ): current_config = { k : current_config [ k ] for k in current_config if ( current_config [ k ] and 'algos' not in k )} return objective_function ( current_config ) self . smac = self . facade ( scenario = scenario , intensifier_kwargs = self . intensifier_kwargs , rng = self . rng , tae_runner = smac_objective_function )","title":"prepare()"},{"location":"api/optimization/time_boxed_random_grid_search/","text":"Documentation for TimeBoxedRandomGridSearchOptimizer Time boxed random grid serach. Iteratively tests n possible hyperparameter configurations until a certain time limit is reached. Examples: my_pipe = Hyperpipe(name='trgrid_based_pipe', optimizer='timeboxed_random_grid_search', optimizer_params={'n_configurations': 50, 'limit_in_minutes': 10}, ... ) my_pipe.fit(X, y) __init__ ( self , limit_in_minutes = 60 , n_configurations = None ) special Initialize the object. Parameters: Name Type Description Default limit_in_minutes float Total time in minutes. 60 n_configurations Optional[int] Number of configurations to be calculated. None Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self , limit_in_minutes : float = 60 , n_configurations : Union [ int , None ] = None ): \"\"\" Initialize the object. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" super ( TimeBoxedRandomGridSearchOptimizer , self ) . __init__ ( n_configurations ) self . limit_in_minutes = limit_in_minutes self . start_time = None self . end_time = None next_config_generator ( self ) Generator for new configs - ask method. Returns: Type Description Generator next_config: Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: next_config: Yields the next config. \"\"\" if self . start_time is None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) for parameters in super ( TimeBoxedRandomGridSearchOptimizer , self ) . next_config_generator (): if datetime . datetime . now () < self . end_time : yield parameters prepare ( self , pipeline_elements , maximize_metric ) Prepare hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\"Prepare hyperparameter search. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. \"\"\" super ( TimeBoxedRandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . start_time = None","title":"TimeBoxedRandom_GS"},{"location":"api/optimization/time_boxed_random_grid_search/#documentation-for-timeboxedrandomgridsearchoptimizer","text":"","title":"Documentation for TimeBoxedRandomGridSearchOptimizer"},{"location":"api/optimization/time_boxed_random_grid_search/#photonai.optimization.grid_search.grid_search.TimeBoxedRandomGridSearchOptimizer","text":"Time boxed random grid serach. Iteratively tests n possible hyperparameter configurations until a certain time limit is reached. Examples: my_pipe = Hyperpipe(name='trgrid_based_pipe', optimizer='timeboxed_random_grid_search', optimizer_params={'n_configurations': 50, 'limit_in_minutes': 10}, ... ) my_pipe.fit(X, y)","title":"photonai.optimization.grid_search.grid_search.TimeBoxedRandomGridSearchOptimizer"},{"location":"api/optimization/time_boxed_random_grid_search/#photonai.optimization.grid_search.grid_search.TimeBoxedRandomGridSearchOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default limit_in_minutes float Total time in minutes. 60 n_configurations Optional[int] Number of configurations to be calculated. None Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self , limit_in_minutes : float = 60 , n_configurations : Union [ int , None ] = None ): \"\"\" Initialize the object. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" super ( TimeBoxedRandomGridSearchOptimizer , self ) . __init__ ( n_configurations ) self . limit_in_minutes = limit_in_minutes self . start_time = None self . end_time = None","title":"__init__()"},{"location":"api/optimization/time_boxed_random_grid_search/#photonai.optimization.grid_search.grid_search.TimeBoxedRandomGridSearchOptimizer.next_config_generator","text":"Generator for new configs - ask method. Returns: Type Description Generator next_config: Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: next_config: Yields the next config. \"\"\" if self . start_time is None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) for parameters in super ( TimeBoxedRandomGridSearchOptimizer , self ) . next_config_generator (): if datetime . datetime . now () < self . end_time : yield parameters","title":"next_config_generator()"},{"location":"api/optimization/time_boxed_random_grid_search/#photonai.optimization.grid_search.grid_search.TimeBoxedRandomGridSearchOptimizer.prepare","text":"Prepare hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all pipeline_elements to create hyperparameter_space. required maximize_metric bool Boolean for distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\"Prepare hyperparameter search. Parameters: pipeline_elements: List of all pipeline_elements to create hyperparameter_space. maximize_metric: Boolean for distinguish between score and error. \"\"\" super ( TimeBoxedRandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . start_time = None","title":"prepare()"},{"location":"api/optimization/hyperparameter/boolean_switch/","text":"Documentation for BooleanSwitch Boolean switch. Class for defining a boolean hyperparameter, When both options de- and activation should be tested in hyperparameter optimization. __init__ ( self ) special Initialize the object. Source code in photonai/optimization/hyperparameters.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( BooleanSwitch , self ) . __init__ ([ True , False ])","title":"BooleanSwitch"},{"location":"api/optimization/hyperparameter/boolean_switch/#documentation-for-booleanswitch","text":"","title":"Documentation for BooleanSwitch"},{"location":"api/optimization/hyperparameter/boolean_switch/#photonai.optimization.hyperparameters.BooleanSwitch","text":"Boolean switch. Class for defining a boolean hyperparameter, When both options de- and activation should be tested in hyperparameter optimization.","title":"photonai.optimization.hyperparameters.BooleanSwitch"},{"location":"api/optimization/hyperparameter/boolean_switch/#photonai.optimization.hyperparameters.BooleanSwitch.__init__","text":"Initialize the object. Source code in photonai/optimization/hyperparameters.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( BooleanSwitch , self ) . __init__ ([ True , False ])","title":"__init__()"},{"location":"api/optimization/hyperparameter/categorical/","text":"Documentation for Categorical Categorical. Class for defining a definite list of hyperparameter values. Can be used for categorical values, but also for numbers. __init__ ( self , values ) special Initialize the object. Parameters: Name Type Description Default values list Definite list of hyperparameter values. required Source code in photonai/optimization/hyperparameters.py def __init__ ( self , values : list ): \"\"\" Initialize the object. Parameters: values: Definite list of hyperparameter values. \"\"\" super ( Categorical , self ) . __init__ ( values )","title":"Categorical"},{"location":"api/optimization/hyperparameter/categorical/#documentation-for-categorical","text":"","title":"Documentation for Categorical"},{"location":"api/optimization/hyperparameter/categorical/#photonai.optimization.hyperparameters.Categorical","text":"Categorical. Class for defining a definite list of hyperparameter values. Can be used for categorical values, but also for numbers.","title":"photonai.optimization.hyperparameters.Categorical"},{"location":"api/optimization/hyperparameter/categorical/#photonai.optimization.hyperparameters.Categorical.__init__","text":"Initialize the object. Parameters: Name Type Description Default values list Definite list of hyperparameter values. required Source code in photonai/optimization/hyperparameters.py def __init__ ( self , values : list ): \"\"\" Initialize the object. Parameters: values: Definite list of hyperparameter values. \"\"\" super ( Categorical , self ) . __init__ ( values )","title":"__init__()"},{"location":"api/optimization/hyperparameter/float_range/","text":"Documentation for FloatRange Float range. Class for easily creating a range of integer numbers to be tested in hyperparameter optimization. __init__ ( self , start , stop , range_type = 'linspace' , step = None , num = None , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default start float The start value for generating the number interval. The resulting interval includes the value, default is 0. required stop float The stop value for generating the number interval. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options: - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression) 'linspace' step float if range_type == 'range', the spacing between values. None num int if range_type == 'linspace' or range_type == 'logspace' or range_type == 'geomspace', the number of samples to generate. None kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'linspace' , step : float = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the number interval. The resulting interval includes the value, default is 0. stop: The stop value for generating the number interval. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options: - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression) step: if range_type == 'range', the spacing between values. num: if range_type == 'linspace' or range_type == 'logspace' or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super ( FloatRange , self ) . __init__ ( start , stop , range_type , step , num , np . float64 , ** kwargs ) get_random_value ( self , definite_list = False ) Method for random search to get random parameter based on its domain. Parameters: Name Type Description Default definite_list bool Choice of transform param to discret list or not. In some cases, certain settings such as the step size may otherwise be lost. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\"Method for random search to get random parameter based on its domain. Parameters: definite_list: Choice of transform param to discret list or not. In some cases, certain settings such as the step size may otherwise be lost. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . uniform ( self . start , self . stop )","title":"FloatRange"},{"location":"api/optimization/hyperparameter/float_range/#documentation-for-floatrange","text":"","title":"Documentation for FloatRange"},{"location":"api/optimization/hyperparameter/float_range/#photonai.optimization.hyperparameters.FloatRange","text":"Float range. Class for easily creating a range of integer numbers to be tested in hyperparameter optimization.","title":"photonai.optimization.hyperparameters.FloatRange"},{"location":"api/optimization/hyperparameter/float_range/#photonai.optimization.hyperparameters.FloatRange.__init__","text":"Initialize the object. Parameters: Name Type Description Default start float The start value for generating the number interval. The resulting interval includes the value, default is 0. required stop float The stop value for generating the number interval. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options: - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression) 'linspace' step float if range_type == 'range', the spacing between values. None num int if range_type == 'linspace' or range_type == 'logspace' or range_type == 'geomspace', the number of samples to generate. None kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'linspace' , step : float = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the number interval. The resulting interval includes the value, default is 0. stop: The stop value for generating the number interval. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options: - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression) step: if range_type == 'range', the spacing between values. num: if range_type == 'linspace' or range_type == 'logspace' or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super ( FloatRange , self ) . __init__ ( start , stop , range_type , step , num , np . float64 , ** kwargs )","title":"__init__()"},{"location":"api/optimization/hyperparameter/float_range/#photonai.optimization.hyperparameters.FloatRange.get_random_value","text":"Method for random search to get random parameter based on its domain. Parameters: Name Type Description Default definite_list bool Choice of transform param to discret list or not. In some cases, certain settings such as the step size may otherwise be lost. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\"Method for random search to get random parameter based on its domain. Parameters: definite_list: Choice of transform param to discret list or not. In some cases, certain settings such as the step size may otherwise be lost. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . uniform ( self . start , self . stop )","title":"get_random_value()"},{"location":"api/optimization/hyperparameter/integer_range/","text":"Documentation for IntegerRange Integer range. Class for easily creating a range of integer numbers to be tested in hyperparameter optimization. __init__ ( self , start , stop , range_type = 'range' , step = None , num = None , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default start float The start value for generating the number interval. The resulting interval includes the value, default is 0. required stop float The stop value for generating the number interval. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options: - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression) 'range' step int if range_type == 'range', the spacing between values. None num int if range_type == 'linspace' or range_type == 'logspace' or range_type == 'geomspace', the number of samples to generate. None kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'range' , step : int = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the number interval. The resulting interval includes the value, default is 0. stop: The stop value for generating the number interval. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options: - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression) step: if range_type == 'range', the spacing between values. num: if range_type == 'linspace' or range_type == 'logspace' or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super () . __init__ ( start , stop , range_type , step , num , np . int32 , ** kwargs ) get_random_value ( self , definite_list = False ) Method for random search to get random parameter based on its domain. Parameters: Name Type Description Default definite_list bool Choice of transform param to discret list or not. In some cases, certain settings such as the step size may otherwise be lost. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\"Method for random search to get random parameter based on its domain. Parameters: definite_list: Choice of transform param to discret list or not. In some cases, certain settings such as the step size may otherwise be lost. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . randint ( self . start , self . stop - 1 )","title":"IntegerRange"},{"location":"api/optimization/hyperparameter/integer_range/#documentation-for-integerrange","text":"","title":"Documentation for IntegerRange"},{"location":"api/optimization/hyperparameter/integer_range/#photonai.optimization.hyperparameters.IntegerRange","text":"Integer range. Class for easily creating a range of integer numbers to be tested in hyperparameter optimization.","title":"photonai.optimization.hyperparameters.IntegerRange"},{"location":"api/optimization/hyperparameter/integer_range/#photonai.optimization.hyperparameters.IntegerRange.__init__","text":"Initialize the object. Parameters: Name Type Description Default start float The start value for generating the number interval. The resulting interval includes the value, default is 0. required stop float The stop value for generating the number interval. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options: - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression) 'range' step int if range_type == 'range', the spacing between values. None num int if range_type == 'linspace' or range_type == 'logspace' or range_type == 'geomspace', the number of samples to generate. None kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'range' , step : int = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the number interval. The resulting interval includes the value, default is 0. stop: The stop value for generating the number interval. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options: - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression) step: if range_type == 'range', the spacing between values. num: if range_type == 'linspace' or range_type == 'logspace' or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super () . __init__ ( start , stop , range_type , step , num , np . int32 , ** kwargs )","title":"__init__()"},{"location":"api/optimization/hyperparameter/integer_range/#photonai.optimization.hyperparameters.IntegerRange.get_random_value","text":"Method for random search to get random parameter based on its domain. Parameters: Name Type Description Default definite_list bool Choice of transform param to discret list or not. In some cases, certain settings such as the step size may otherwise be lost. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\"Method for random search to get random parameter based on its domain. Parameters: definite_list: Choice of transform param to discret list or not. In some cases, certain settings such as the step size may otherwise be lost. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . randint ( self . start , self . stop - 1 )","title":"get_random_value()"},{"location":"api/processing/results_handler/","text":"Provides all functions that operate on calculated results. As IO for the results object the ResultsHandler is able to handle results on its own. __init__ ( self , results_object = None , output_settings = None ) special Initialize the object. Parameters: Name Type Description Default results_object MDBHyperpipe All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. None output_settings Setting for creation and storage of the results_object. None Source code in photonai/processing/results_handler.py def __init__ ( self , results_object : MDBHyperpipe = None , output_settings = None ): \"\"\" Initialize the object. Parameters: results_object: All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. output_settings: Setting for creation and storage of the results_object. \"\"\" self . results = results_object self . output_settings = output_settings get_config_evaluations ( self ) Return the test performance of every tested configuration in every outer fold. Source code in photonai/processing/results_handler.py def get_config_evaluations ( self ): \"\"\"Return the test performance of every tested configuration in every outer fold. \"\"\" config_performances = list () maximum_fold = None for outer_fold in self . results . outer_folds : if maximum_fold is None or len ( outer_fold . tested_config_list ) > maximum_fold : maximum_fold = len ( outer_fold . tested_config_list ) for outer_fold in self . results . outer_folds : performance = dict () for metric in self . results . hyperpipe_info . metrics : performance [ metric ] = list () for i in range ( maximum_fold ): # for config in outer_fold.tested_config_list: for metric in self . results . hyperpipe_info . metrics : if i >= len ( outer_fold . tested_config_list ): performance [ metric ] . append ( np . nan ) continue config = outer_fold . tested_config_list [ i ] if config . config_failed : performance [ metric ] . append ( np . nan ) else : for item in config . metrics_test : if ( item . operation == 'FoldOperations.MEAN' ) and ( item . metric_name == metric ): performance [ metric ] . append ( item . value ) config_performances . append ( performance ) config_performances_dict = dict () for metric in self . results . hyperpipe_info . metrics : config_performances_dict [ metric ] = list () for fold in config_performances : config_performances_dict [ metric ] . append ( fold [ metric ]) return config_performances_dict get_methods () staticmethod This function returns a list of all methods available for ResultsHandler. Returns: Type Description list List of all available methods. Source code in photonai/processing/results_handler.py @staticmethod def get_methods () -> list : \"\"\"This function returns a list of all methods available for ResultsHandler. Returns: List of all available methods. \"\"\" methods_list = [ s for s in dir ( ResultsHandler ) if '__' not in s ] return methods_list get_performance_table ( self ) This function returns a summary table of the overall results. ToDo: add best_config information! Source code in photonai/processing/results_handler.py def get_performance_table ( self ): \"\"\"This function returns a summary table of the overall results. ToDo: add best_config information! \"\"\" res_tab = pd . DataFrame () for i , folds in enumerate ( self . results . outer_folds ): # add best config infos try : res_tab . loc [ i , 'best_config' ] = folds . best_config . human_readable_config except : res_tab . loc [ i , 'best_config' ] = str ( folds . best_config . human_readable_config ) # add fold index res_tab . loc [ i , 'fold' ] = folds . fold_nr # add sample size infos res_tab . loc [ i , 'n_train' ] = folds . best_config . best_config_score . number_samples_training res_tab . loc [ i , 'n_validation' ] = folds . best_config . best_config_score . number_samples_validation # add performance metrics d = folds . best_config . best_config_score . validation . metrics for key , value in d . items (): res_tab . loc [ i , key ] = value # add row with overall info res_tab . loc [ i + 1 , 'n_validation' ] = np . sum ( res_tab [ 'n_validation' ]) for key , value in d . items (): m = res_tab . loc [:, key ] res_tab . loc [ i + 1 , key ] = np . mean ( m ) res_tab . loc [ i + 1 , key + '_sem' ] = sem ( m ) # standard error of the mean res_tab . loc [ i + 1 , 'best_config' ] = 'Overall' return res_tab load_from_file ( self , results_file ) Reads results_file from json into MDBHyperpipe object self.results. Parameters: Name Type Description Default results_file str Full path to json file. required Source code in photonai/processing/results_handler.py def load_from_file ( self , results_file : str ): \"\"\"Reads results_file from json into MDBHyperpipe object self.results. Parameters: results_file: Full path to json file. \"\"\" self . results = MDBHyperpipe . from_document ( json . load ( open ( results_file , 'r' ))) load_from_mongodb ( self , mongodb_connect_url , pipe_name ) Reads results_file from MongoDB into MDBHyperpipe object self.results. Parameters: Name Type Description Default mongodb_connect_url str MongoDB connection string. required pipe_name str Name of the stored hyperpipe. required Source code in photonai/processing/results_handler.py def load_from_mongodb ( self , mongodb_connect_url : str , pipe_name : str ): \"\"\"Reads results_file from MongoDB into MDBHyperpipe object self.results. Parameters: mongodb_connect_url: MongoDB connection string. pipe_name: Name of the stored hyperpipe. \"\"\" connect ( mongodb_connect_url , alias = \"photon_core\" ) results = list ( MDBHyperpipe . objects . raw ({ 'name' : pipe_name })) if len ( results ) == 1 : self . results = results [ 0 ] elif len ( results ) > 1 : self . results = MDBHyperpipe . objects . order_by ([( \"computation_start_time\" , DESCENDING )]) . raw ({ 'name' : pipe_name }) . first () warn_text = 'Found multiple hyperpipes with that name. Returning most recent one.' logger . warning ( warn_text ) warnings . warn ( warn_text ) else : raise FileNotFoundError ( 'Could not load hyperpipe from MongoDB.' )","title":"Results Handler"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler","text":"Provides all functions that operate on calculated results. As IO for the results object the ResultsHandler is able to handle results on its own.","title":"photonai.processing.results_handler.ResultsHandler"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.__init__","text":"Initialize the object. Parameters: Name Type Description Default results_object MDBHyperpipe All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. None output_settings Setting for creation and storage of the results_object. None Source code in photonai/processing/results_handler.py def __init__ ( self , results_object : MDBHyperpipe = None , output_settings = None ): \"\"\" Initialize the object. Parameters: results_object: All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. output_settings: Setting for creation and storage of the results_object. \"\"\" self . results = results_object self . output_settings = output_settings","title":"__init__()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.get_config_evaluations","text":"Return the test performance of every tested configuration in every outer fold. Source code in photonai/processing/results_handler.py def get_config_evaluations ( self ): \"\"\"Return the test performance of every tested configuration in every outer fold. \"\"\" config_performances = list () maximum_fold = None for outer_fold in self . results . outer_folds : if maximum_fold is None or len ( outer_fold . tested_config_list ) > maximum_fold : maximum_fold = len ( outer_fold . tested_config_list ) for outer_fold in self . results . outer_folds : performance = dict () for metric in self . results . hyperpipe_info . metrics : performance [ metric ] = list () for i in range ( maximum_fold ): # for config in outer_fold.tested_config_list: for metric in self . results . hyperpipe_info . metrics : if i >= len ( outer_fold . tested_config_list ): performance [ metric ] . append ( np . nan ) continue config = outer_fold . tested_config_list [ i ] if config . config_failed : performance [ metric ] . append ( np . nan ) else : for item in config . metrics_test : if ( item . operation == 'FoldOperations.MEAN' ) and ( item . metric_name == metric ): performance [ metric ] . append ( item . value ) config_performances . append ( performance ) config_performances_dict = dict () for metric in self . results . hyperpipe_info . metrics : config_performances_dict [ metric ] = list () for fold in config_performances : config_performances_dict [ metric ] . append ( fold [ metric ]) return config_performances_dict","title":"get_config_evaluations()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.get_methods","text":"This function returns a list of all methods available for ResultsHandler. Returns: Type Description list List of all available methods. Source code in photonai/processing/results_handler.py @staticmethod def get_methods () -> list : \"\"\"This function returns a list of all methods available for ResultsHandler. Returns: List of all available methods. \"\"\" methods_list = [ s for s in dir ( ResultsHandler ) if '__' not in s ] return methods_list","title":"get_methods()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.get_performance_table","text":"This function returns a summary table of the overall results. ToDo: add best_config information! Source code in photonai/processing/results_handler.py def get_performance_table ( self ): \"\"\"This function returns a summary table of the overall results. ToDo: add best_config information! \"\"\" res_tab = pd . DataFrame () for i , folds in enumerate ( self . results . outer_folds ): # add best config infos try : res_tab . loc [ i , 'best_config' ] = folds . best_config . human_readable_config except : res_tab . loc [ i , 'best_config' ] = str ( folds . best_config . human_readable_config ) # add fold index res_tab . loc [ i , 'fold' ] = folds . fold_nr # add sample size infos res_tab . loc [ i , 'n_train' ] = folds . best_config . best_config_score . number_samples_training res_tab . loc [ i , 'n_validation' ] = folds . best_config . best_config_score . number_samples_validation # add performance metrics d = folds . best_config . best_config_score . validation . metrics for key , value in d . items (): res_tab . loc [ i , key ] = value # add row with overall info res_tab . loc [ i + 1 , 'n_validation' ] = np . sum ( res_tab [ 'n_validation' ]) for key , value in d . items (): m = res_tab . loc [:, key ] res_tab . loc [ i + 1 , key ] = np . mean ( m ) res_tab . loc [ i + 1 , key + '_sem' ] = sem ( m ) # standard error of the mean res_tab . loc [ i + 1 , 'best_config' ] = 'Overall' return res_tab","title":"get_performance_table()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.load_from_file","text":"Reads results_file from json into MDBHyperpipe object self.results. Parameters: Name Type Description Default results_file str Full path to json file. required Source code in photonai/processing/results_handler.py def load_from_file ( self , results_file : str ): \"\"\"Reads results_file from json into MDBHyperpipe object self.results. Parameters: results_file: Full path to json file. \"\"\" self . results = MDBHyperpipe . from_document ( json . load ( open ( results_file , 'r' )))","title":"load_from_file()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.load_from_mongodb","text":"Reads results_file from MongoDB into MDBHyperpipe object self.results. Parameters: Name Type Description Default mongodb_connect_url str MongoDB connection string. required pipe_name str Name of the stored hyperpipe. required Source code in photonai/processing/results_handler.py def load_from_mongodb ( self , mongodb_connect_url : str , pipe_name : str ): \"\"\"Reads results_file from MongoDB into MDBHyperpipe object self.results. Parameters: mongodb_connect_url: MongoDB connection string. pipe_name: Name of the stored hyperpipe. \"\"\" connect ( mongodb_connect_url , alias = \"photon_core\" ) results = list ( MDBHyperpipe . objects . raw ({ 'name' : pipe_name })) if len ( results ) == 1 : self . results = results [ 0 ] elif len ( results ) > 1 : self . results = MDBHyperpipe . objects . order_by ([( \"computation_start_time\" , DESCENDING )]) . raw ({ 'name' : pipe_name }) . first () warn_text = 'Found multiple hyperpipes with that name. Returning most recent one.' logger . warning ( warn_text ) warnings . warn ( warn_text ) else : raise FileNotFoundError ( 'Could not load hyperpipe from MongoDB.' )","title":"load_from_mongodb()"},{"location":"examples/classifier_ensemble/","text":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import StratifiedKFold from photonai.base import Hyperpipe , PipelineElement , Stack from photonai.optimization import FloatRange X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( name = 'Estimator_pipe' , optimizer = 'random_grid_search' , metrics = [ 'balanced_accuracy' ], best_config_metric = 'balanced_accuracy' , outer_cv = StratifiedKFold ( n_splits = 2 , shuffle = True , random_state = 42 ), inner_cv = StratifiedKFold ( n_splits = 2 , shuffle = True , random_state = 42 ), project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE # first normalize all features my_pipe += PipelineElement ( 'StandardScaler' ) # some feature selection my_pipe += PipelineElement ( 'LassoFeatureSelection' , hyperparameters = { 'percentile' : FloatRange ( start = 0.1 , step = 0.1 , stop = 0.7 , range_type = 'range' ), 'alpha' : FloatRange ( 0.5 , 1 )}, test_disabled = True ) # add imbalanced group handling my_pipe += PipelineElement ( 'ImbalancedDataTransformer' , method_name = 'SMOTE' , test_disabled = False ) # setup estimator stack est_stack = Stack ( name = 'classifier_stack' ) clf_list = [ 'RandomForestClassifier' , 'LinearSVC' , 'NuSVC' , \"SVC\" , \"MLPClassifier\" , \"KNeighborsClassifier\" , \"Lasso\" , \"PassiveAggressiveClassifier\" , \"LogisticRegression\" , \"Perceptron\" , \"RidgeClassifier\" , \"SGDClassifier\" , \"GaussianProcessClassifier\" , \"AdaBoostClassifier\" , \"BaggingClassifier\" , \"GradientBoostingClassifier\" ] for clf in clf_list : est_stack += PipelineElement ( clf ) my_pipe += est_stack my_pipe += PipelineElement ( 'PhotonVotingClassifier' ) my_pipe . fit ( X , y )","title":"Classifier Ensemble"},{"location":"examples/confounder_removal/","text":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement # WE USE THE BREAST CANCER SET FROM SKLEARN data = load_breast_cancer () y = data . target # now let's assume we want to regress out the effect of mean_radius and mean_texture X = data . data [:, 2 :] mean_radius = data . data [:, 0 ] mean_texture = data . data [:, 1 ] # BUILD HYPERPIPE pipe = Hyperpipe ( 'confounder_removal_example' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 5 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) # # there are two ways of specifying multiple confounders # # first, you can simply pass a dictionary with \"confounder\" as key and a data matrix or list as value # pipe += PipelineElement('ConfounderRemoval', {}, standardize_covariates=True, test_disabled=False) # pipe.fit(X, y, **{'confounder': [mean_radius, mean_texture]}) # pipe += PipelineElement('SVC') # second, you can also specify the names of the variables that should be used in the confounder removal step pipe += PipelineElement ( 'ConfounderRemoval' , {}, standardize_covariates = True , test_disabled = True , confounder_names = [ 'mean_radius' , 'mean_texture' ]) pipe += PipelineElement ( 'SVC' ) # those names must be keys in the kwargs dictionary pipe . fit ( X , y , ** { 'mean_radius' : mean_radius , 'mean_texture' : mean_texture })","title":"Confounder Removal"},{"location":"examples/custom_estimator/","text":"Custom Estimator You can combine your own learning algorithm, bet it a neural net or anything else, by simply adhering to the scikit-learn interface as shown below. Then register your class with the Register module and you're done! import numpy as np from sklearn.base import BaseEstimator , ClassifierMixin class CustomEstimator ( BaseEstimator , ClassifierMixin ): def __init__ ( self , param1 = 0 , param2 = None ): # it is important that you name your params the same in the constructor # stub as well as in your class variables! self . param1 = param1 self . param2 = param2 def fit ( self , X , y = None , ** kwargs ): \"\"\" Adjust the underlying model or method to the data. Returns ------- IMPORTANT: must return self! \"\"\" return self def predict ( self , X ): \"\"\" Use the learned model to make predictions. \"\"\" return np . random . randint ( 0 , 2 , X . shape [ 0 ])","title":"Custom Estimator"},{"location":"examples/custom_transformer/","text":"Custom Transformer You can add your own method, be it preprocessing, feature selection or dimensionality reduction, by simply adhering to the scikit-learn interface as shown below. Then register your class with the Register module and you're good to go. You can then combine it with any optimizer and metric and design your custom pipeline layout. from sklearn.base import BaseEstimator class CustomTransformer ( BaseEstimator ): def __init__ ( self , param1 = 0 , param2 = None ): # it is important that you name your params the same in the constructor # stub as well as in your class variables! self . param1 = param1 self . param2 = param2 def fit ( self , data , targets = None , ** kwargs ): \"\"\" Adjust the underlying model or method to the data. Returns ------- IMPORTANT: must return self! \"\"\" return self def transform ( self , data , targets = None , ** kwargs ): \"\"\" Apply the method's logic to the data. \"\"\" return data","title":"Custom Transformer"},{"location":"examples/dnn_multiclass_prediction/","text":"from sklearn.datasets import load_digits from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , OutputSettings from photonai.optimization import Categorical # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_digits ( n_class = 5 , return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'basic_keras_multiclass_pipe' , optimizer = 'grid_search' , optimizer_params = {}, metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 2 ), inner_cv = KFold ( n_splits = 2 ), verbosity = 1 , output_settings = OutputSettings ( project_folder = './tmp/' )) # ADD ELEMENTS TO YOUR PIPELINE my_pipe . add ( PipelineElement ( 'StandardScaler' )) # attention: hidden_layer count == activation size. So if you want to choose a function in every layer, # grid_search does not forbid combinations with len(hidden_layer_size) != len(activations) my_pipe += PipelineElement ( 'KerasDnnClassifier' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 5 ]]), 'dropout_rate' : Categorical ([ 0.5 , [ 0.5 , 0.2 ]]) }, activations = 'relu' , nn_batch_size = 32 , multi_class = True , verbosity = 1 ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"DNN with multiclass prediction"},{"location":"examples/feature_subset_pipelines/","text":"import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack , Branch , Switch , DataFilter from photonai.optimization import FloatRange , IntegerRange , Categorical # LOAD DATA FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'data_integration' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 20 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'f1_score' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 0 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'SimpleImputer' ) my_pipe += PipelineElement ( 'StandardScaler' , {}, with_mean = True ) # Use only \"mean\" features: [mean_radius, mean_texture, mean_perimeter, mean_area, mean_smoothness, mean_compactness, # mean_concavity, mean_concave_points, mean_symmetry, mean_fractal_dimension mean_branch = Branch ( 'MeanFeature' ) mean_branch += DataFilter ( indices = np . arange ( 10 )) mean_branch += PipelineElement ( 'SVC' , { 'C' : FloatRange ( 0.1 , 10 )}, kernel = 'linear' ) # Use only \"error\" features error_branch = Branch ( 'ErrorFeature' ) error_branch += DataFilter ( indices = np . arange ( 10 , 20 )) error_branch += PipelineElement ( 'SVC' , { 'C' : Categorical ([ 100 , 1000 , 1000 ])}, kernel = 'linear' ) # use only \"worst\" features: [worst_radius, worst_texture, ..., worst_fractal_dimension] worst_branch = Branch ( 'WorstFeature' ) worst_branch += DataFilter ( indices = np . arange ( 20 , 30 )) worst_branch += PipelineElement ( 'SVC' ) my_pipe += Stack ( 'SourceStack' , [ mean_branch , error_branch , worst_branch ]) my_pipe += Switch ( 'EstimatorSwitch' , [ PipelineElement ( 'RandomForestClassifier' , { 'n_estimators' : IntegerRange ( 2 , 5 )}), PipelineElement ( 'SVC' )]) my_pipe . fit ( X , y )","title":"Feature Subset Pipelines"},{"location":"examples/group_driven_cv_split/","text":"import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import GroupKFold , GroupShuffleSplit from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange , Categorical # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) groups = np . random . random_integers ( 0 , 3 , ( len ( y ), )) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'group_split_pipe' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = GroupKFold ( n_splits = 4 ), inner_cv = GroupShuffleSplit ( n_splits = 10 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE # first normalize all features my_pipe += PipelineElement ( 'StandardScaler' ) # then do feature selection using a PCA, specify which values to try in the hyperparameter search my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : [ 5 , 10 , None ]}, test_disabled = True ) # engage and optimize the good old SVM for Classification my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 0.5 , 2 , \"linspace\" , num = 5 )}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y , groups = groups )","title":"Group driven CV split"},{"location":"examples/imbalanced_data/","text":"Imbalanced Data Transform We have a simple solution for imbalanced classes in a classification problem. Based on the imblearn package, you can choose between over-, under- and combinesampling. Have a look at the Developer Website for details about the balancing data algorithms. from sklearn.model_selection import KFold from imblearn.datasets import fetch_datasets from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange , Categorical , IntegerRange # example of imbalanced dataset dataset = fetch_datasets ()[ 'coil_2000' ] X , y = dataset . data , dataset . target # ratio class 0: 0.06%, class 1: 0.94% my_pipe = Hyperpipe ( 'basic_svm_pipe_no_performance' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 10 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'recall' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 5 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) my_pipe += PipelineElement ( 'ImbalancedDataTransformer' , hyperparameters = { 'method_name' : Categorical ([ 'RandomUnderSampler' , 'SMOTE' ])}, test_disabled = True ) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 0.5 , 2 )}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y ) # Possible values for method_name: # imbalance_type = OVERSAMPLING: # - ADASYN # - BorderlineSMOTE # - KMeansSMOTE # - RandomOverSampler # - SMOTE # - SMOTENC # - SVMSMOTE # # imbalance_type = UNDERSAMPLING: # - ClusterCentroids, # - RandomUnderSampler, # - NearMiss, # - InstanceHardnessThreshold, # - CondensedNearestNeighbour, # - EditedNearestNeighbours, # - RepeatedEditedNearestNeighbours, # - AllKNN, # - NeighbourhoodCleaningRule, # - OneSidedSelection # # imbalance_type = COMBINE: # - SMOTEENN, # - SMOTETomek","title":"Over- and Undersampling"},{"location":"examples/no_outer_cv/","text":"import numpy as np from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement X , y = load_boston ( return_X_y = True ) my_pipe = Hyperpipe ( name = 'default_pipe' , metrics = [ 'mean_absolute_error' , 'mean_squared_error' , 'pearson_correlation' ], best_config_metric = 'mean_absolute_error' , use_test_set = False , inner_cv = KFold ( n_splits = 10 , shuffle = True , random_state = 42 ), verbosity = 0 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'SimpleImputer' , missing_values = np . nan , strategy = 'median' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'GaussianProcessRegressor' ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y ) # find mean and std of all metrics here test_metrics = my_pipe . results . best_config . metrics_test train_metrics = my_pipe . results . best_config . metrics_train","title":"No Outer CV"},{"location":"examples/permutation_test/","text":"import uuid import numpy as np from sklearn.datasets import load_breast_cancer from photonai.processing.permutation_test import PermutationTest def create_hyperpipe (): # this is needed here for the parallelisation from photonai.base import Hyperpipe , PipelineElement , OutputSettings from sklearn.model_selection import GroupKFold from sklearn.model_selection import KFold settings = OutputSettings ( mongodb_connect_url = 'mongodb://localhost:27017/photon_results' ) my_pipe = Hyperpipe ( 'permutation_test_1' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = GroupKFold ( n_splits = 2 ), inner_cv = KFold ( n_splits = 2 ), calculate_metrics_across_folds = True , use_test_set = True , verbosity = 1 , project_folder = './tmp/' , output_settings = settings ) # Add transformer elements my_pipe += PipelineElement ( \"StandardScaler\" , hyperparameters = {}, test_disabled = True , with_mean = True , with_std = True ) my_pipe += PipelineElement ( \"PCA\" , # hyperparameters={'n_components': IntegerRange(5, 15)}, test_disabled = False ) # Add estimator my_pipe += PipelineElement ( \"SVC\" , hyperparameters = { 'kernel' : [ 'linear' , 'rbf' ]}, #C': FloatRange(0.1, 5), gamma = 'scale' , max_iter = 1000000 ) return my_pipe X , y = load_breast_cancer ( return_X_y = True ) my_perm_id = str ( uuid . uuid4 ()) groups = np . random . random_integers ( 0 , 3 , ( len ( y ), )) # in case the permutation test for this specific hyperpipe has already been calculated, PHOTON will skip the permutation # runs and load existing results perm_tester = PermutationTest ( create_hyperpipe , n_perms = 2 , n_processes = 1 , random_state = 11 , permutation_id = my_perm_id ) perm_tester . fit ( X , y , groups = groups ) results = PermutationTest . _calculate_results ( my_perm_id , mongodb_path = 'mongodb://localhost:27017/photon_results' ) print ( results . p_values )","title":"Permutation Test"},{"location":"examples/sample_pairing/","text":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import Categorical # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'sample_pairing_example_classification' , optimizer = 'grid_search' , metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' , random_seed = 42123 ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SamplePairingClassification' , hyperparameters = { 'draw_limit' : [ 500 , 1000 , 10000 ], 'generator' : Categorical ([ 'nearest_pair' ])}, distance_metric = 'euclidean' , test_disabled = True ) my_pipe += PipelineElement ( 'RandomForestClassifier' , hyperparameters = { 'n_estimators' : [ 10 ]}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Sample Pairing"},{"location":"examples/scikit_learn_mlp/","text":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , OutputSettings from photonai.optimization import IntegerRange # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'basic_svm_pipe' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , output_settings = OutputSettings ( project_folder = './tmp/' )) # ADD ELEMENTS TO YOUR PIPELINE my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PhotonMLPClassifier' , hyperparameters = { 'layer_1' : IntegerRange ( 1 , 5 ), 'layer_2' : IntegerRange ( 0 , 5 ), 'layer_3' : IntegerRange ( 0 , 5 )}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Scikit-learn MLP"},{"location":"examples/subpipelines/","text":"Subpipelines If the user wants to parallelize a complete sequence of transformations, that is not only singular PipelineElements but an ordered number of PipelineElements, the class PHOTONAI Branch offers a way to create parallel subpipelines. The branch in turn, can be used in combination with the AND- and OR- Elements in order to design complex pipeline architectures. from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack , Branch from photonai.optimization import IntegerRange , Categorical , FloatRange X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'basic_stacking' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'f1_score' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 10 ), verbosity = 1 , project_folder = './tmp/' ) # BRANCH WITH QUANTILTRANSFORMER AND DECISIONTREECLASSIFIER tree_qua_branch = Branch ( 'tree_branch' ) tree_qua_branch += PipelineElement ( 'QuantileTransformer' , n_quantiles = 100 ) tree_qua_branch += PipelineElement ( 'DecisionTreeClassifier' , { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) # BRANCH WITH MinMaxScaler AND DecisionTreeClassifier svm_mima_branch = Branch ( 'svm_branch' ) svm_mima_branch += PipelineElement ( 'MinMaxScaler' ) svm_mima_branch += PipelineElement ( 'SVC' , { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 0.01 , 2.0 )}, gamma = 'auto' ) # BRANCH WITH StandardScaler AND KNeighborsClassifier knn_sta_branch = Branch ( 'neighbour_branch' ) knn_sta_branch += PipelineElement ( 'StandardScaler' ) knn_sta_branch += PipelineElement ( 'KNeighborsClassifier' ) # voting = True to mean the result of every branch my_pipe += Stack ( 'final_stack' , [ tree_qua_branch , svm_mima_branch , knn_sta_branch ]) my_pipe += PipelineElement ( 'LogisticRegression' , solver = 'lbfgs' ) my_pipe . fit ( X , y )","title":"Subpipelines"},{"location":"features/performance_constraints/","text":"Performance Constraints Integrating performance baselines and performance expectations in the hyperparameter optimization process is furthermore helpful to increase the overall speed and efficiency. The user can specify to skip the testing of a specific hyperparameter configuration in further inner-cross-validation folds if the given configuration performs worse than a given static or dynamic threshold. from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , OutputSettings from photonai.optimization import MinimumPerformanceConstraint , IntegerRange X , y = load_boston ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( name = 'basic_svm_pipe_no_performance' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'mean_squared_error' , 'pearson_correlation' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 , shuffle = True ), inner_cv = KFold ( n_splits = 3 ), use_test_set = True , verbosity = 1 , project_folder = './result_folder' , output_settings = OutputSettings ( mongodb_connect_url = \"mongodb://localhost:27017/photon_results\" , save_output = True ), performance_constraints = [ MinimumPerformanceConstraint ( 'mean_squared_error' , 35 , 'first' ), MinimumPerformanceConstraint ( 'pearson_correlation' , 0.7 , 'any' )]) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 5 , 50 )}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y ) # AND SHOW THE RESULTS IN THE WEBBASED PHOTON INVESTIGATOR TOOL # Investigator.show(my_pipe) # YOU CAN ALSO SAVE THE BEST PERFORMING PIPELINE FOR FURTHER USE # my_pipe.save_optimum_pipe('/home/photon_user/photon_test/optimum_pipe.photon')","title":"Hyperparameter Optimization Shortcuts"},{"location":"features/permutation_test/","text":"import uuid import numpy as np from sklearn.datasets import load_breast_cancer from photonai.processing.permutation_test import PermutationTest def create_hyperpipe (): # this is needed here for the parallelisation from photonai.base import Hyperpipe , PipelineElement , OutputSettings from sklearn.model_selection import GroupKFold from sklearn.model_selection import KFold settings = OutputSettings ( mongodb_connect_url = 'mongodb://localhost:27017/photon_results' ) my_pipe = Hyperpipe ( 'permutation_test_1' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = GroupKFold ( n_splits = 2 ), inner_cv = KFold ( n_splits = 2 ), calculate_metrics_across_folds = True , use_test_set = True , verbosity = 1 , project_folder = './tmp/' , output_settings = settings ) # Add transformer elements my_pipe += PipelineElement ( \"StandardScaler\" , hyperparameters = {}, test_disabled = True , with_mean = True , with_std = True ) my_pipe += PipelineElement ( \"PCA\" , # hyperparameters={'n_components': IntegerRange(5, 15)}, test_disabled = False ) # Add estimator my_pipe += PipelineElement ( \"SVC\" , hyperparameters = { 'kernel' : [ 'linear' , 'rbf' ]}, #C': FloatRange(0.1, 5), gamma = 'scale' , max_iter = 1000000 ) return my_pipe X , y = load_breast_cancer ( return_X_y = True ) my_perm_id = str ( uuid . uuid4 ()) groups = np . random . random_integers ( 0 , 3 , ( len ( y ), )) # in case the permutation test for this specific hyperpipe has already been calculated, PHOTON will skip the permutation # runs and load existing results perm_tester = PermutationTest ( create_hyperpipe , n_perms = 2 , n_processes = 1 , random_state = 11 , permutation_id = my_perm_id ) perm_tester . fit ( X , y , groups = groups ) results = PermutationTest . _calculate_results ( my_perm_id , mongodb_path = 'mongodb://localhost:27017/photon_results' ) print ( results . p_values )","title":"Permutation Test"},{"location":"features/stack/","text":"Stack You want to do stacking if more than one algorithm shall be applied, which equals to an AND-Operation. The PHOTONAI Stack delivers the data to all of the entailed PipelineElements and the transformations or predictions are afterwards horizontally concatenated. In this way you can preprocess data in different ways and collect the resulting information to create a new feature matrix. Additionally, you can train several learning algorithms with the same data in an ensemble-like fashion and concatenate their predictions to a prediction matrix on which you can apply further processing like voting strategies. from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack from photonai.optimization import FloatRange , IntegerRange X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'basic_stack_pipe' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 0 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) svc = PipelineElement ( 'LinearSVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 25 )}) # for a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators # in case only some implement predict_proba, predict is called for the remaining estimators my_pipe += Stack ( 'final_stack' , [ tree , svc ], use_probabilities = True ) my_pipe += PipelineElement ( 'LinearSVC' ) my_pipe . fit ( X , y )","title":"Stack"},{"location":"features/switch/","text":"Switch The PipelineSwitch element acts like an OR-Operator and decides which element performs best. Currently, you can only optimize the PipelineSwitch using Grid Search, Random Grid Search and smac3 In this example, we add two different transformer elements and two different estimators, and PHOTONAI will evaluate the best choices including the respective hyperparameters. from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import IntegerRange # GET DATA X , y = load_breast_cancer ( return_X_y = True ) # CREATE HYPERPIPE my_pipe = Hyperpipe ( 'basic_switch_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 5 ), verbosity = 1 , project_folder = './tmp/' ) # Transformer Switch my_pipe += Switch ( 'StandardizationSwitch' , [ PipelineElement ( 'StandardScaler' ), PipelineElement ( 'MinMaxScaler' )]) # Estimator Switch svm = PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : [ 'rbf' , 'linear' ]}) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 5 ), 'min_samples_leaf' : IntegerRange ( 1 , 5 ), 'criterion' : [ 'gini' , 'entropy' ]}) my_pipe += Switch ( 'EstimatorSwitch' , [ svm , tree ]) my_pipe . fit ( X , y )","title":"Switch"},{"location":"getting_started/classification/","text":"Classification Classification is one of the main machine Learning task in these days. PHOTONAI provides all tools for a fast and convenient design of classification machine learning pipeline layouts. from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , OutputSettings from photonai.optimization import FloatRange , Categorical , IntegerRange # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'basic_svm_pipe' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 5 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' ) # my_pipe.add(PipelineElement('StandardScaler')) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 10 , 30 )}, test_disabled = True ) # # my_pipe += PipelineElement('SVC', # hyperparameters={'kernel': Categorical(['rbf', 'linear']), # 'C': FloatRange(1, 6)}, # gamma='scale') my_pipe += PipelineElement ( 'RandomForestClassifier' ) # import numpy as np # np.random.shuffle(y) my_pipe . fit ( X , y )","title":"Classification"},{"location":"getting_started/output/","text":"Output Folder After executing the script a Result Folder is created. In there you find six files with different information about your pipeline and the results. best_config_predictions.csv This file saves the predicted value for every tested configuration. hyperpipe_config.json Here is the initial Setup for your Pipeline saved. photon_best_model.photon This file stores the best model. You can share or reload it later. photon_output.log Saves the console output from every Fold, including the time, the current testing configurations and the results. photon_result_file.json You can visualize this file with our Explorer . Visualized information: Best Hyperparameter Configuration Performance Fold information Tested Configuration Optimization Progress photon_summary.txt A text file including a summary of the results.","title":"Output"},{"location":"getting_started/registry/","text":"Registry The PHOTONAI Register class lets you register your class with a key, so that you can access it conveniently in your PHOTONAI Hyperpipe setup via the PipelineElement class. import os from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , PhotonRegistry from photonai.optimization import IntegerRange # REGISTER ELEMENT base_folder = os . path . dirname ( os . path . abspath ( __file__ )) custom_elements_folder = os . path . join ( base_folder , 'custom_elements' ) registry = PhotonRegistry ( custom_elements_folder = custom_elements_folder ) registry . register ( photon_name = 'MyCustomEstimator' , class_str = 'custom_estimator.CustomEstimator' , element_type = 'Estimator' ) registry . register ( photon_name = 'MyCustomTransformer' , class_str = 'custom_transformer.CustomTransformer' , element_type = 'Transformer' ) registry . activate () # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'custom_estimator_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 2 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) # SHOW WHAT IS POSSIBLE IN THE CONSOLE registry . list_available_elements () # NOW FIND OUT MORE ABOUT A SPECIFIC ELEMENT registry . info ( 'MyCustomEstimator' ) registry . info ( 'MyCustomTransformer' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) my_pipe += PipelineElement ( 'MyCustomEstimator' ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Registry"},{"location":"getting_started/regression/","text":"Regression Next to learning a distinct classification task, a regression model learns to predict continuous target values. from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import IntegerRange X , y = load_boston ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'basic_svm_pipe_no_performance' , optimizer = 'random_grid_search' , metrics = [ 'mean_squared_error' , 'pearson_correlation' , 'mean_absolute_error' , 'explained_variance' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE # first normalize all features my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : [ 0.5 , 0.8 , 0.3 ]}) # engage and optimize the good old SVM for Classification my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 10 , 50 )}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Regression"},{"location":"getting_started/save_load/","text":"Regression Every successful trained pipeline saves the best model in the result folder as photon_best_model.photon. To collaborate with other people share your trained model saved in this file. If you want to reload it have a look at the following example script: from photonai.base import Hyperpipe from sklearn.datasets import load_breast_cancer X , _ = load_breast_cancer ( True ) my_pipe = Hyperpipe . load_optimum_pipe ( \"full_path/to/photon_best_model.photon\" ) predictions = my_pipe . predict ( X )","title":"Saving and loading"}]}