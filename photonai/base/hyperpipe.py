import traceback
import re
import __main__
import os
import glob
import shutil
import pickle
import inspect
import zipfile
import importlib
import importlib.util
from sklearn.externals import joblib

import datetime
import pandas as pd
import numpy as np

from bson.objectid import ObjectId

from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.model_selection._split import BaseCrossValidator
from sklearn.base import BaseEstimator

from photonai.base.photon_elements import Stack, Switch, Preprocessing, CallbackElement, Branch, PipelineElement, PhotonNative
from photonai.base.photon_pipeline import PhotonPipeline
from photonai.base.cache_manager import CacheManager

from photonai.processing import ResultsHandler
from photonai.processing.outer_folds import OuterFoldManager
from photonai.processing.photon_folds import FoldInfo
from photonai.processing.metrics import Scorer
from photonai.processing.results_structure import MDBHyperpipe, MDBHyperpipeInfo, MDBDummyResults, MDBHelper, \
    FoldOperations, MDBConfig, MDBOuterFold

from photonai.optimization import GridSearchOptimizer, TimeBoxedRandomGridSearchOptimizer, RandomGridSearchOptimizer, \
    SkOptOptimizer, IntegerRange, FloatRange, Categorical

from photonai.photonlogger import Logger


class OutputSettings:
    """
    Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB
    or a simple son-file. You can also choose whether to save predictions and/or feature importances.

    Parameters
    ----------
    * `mongodb_connect_url` [str]:
        Valid mongodb connection url that specifies a database for storing the results

    * `save_predictions` [str, default='best']:
        Possible options are 'best' to save only the predictions of the best configuration for every outer fold, 'all'
        to save all predictions or 'None' to not save any predictions at all.

    * `save_feature_importances` [str, default='best']:
        Possible options are 'best' to save only the feature importances of the best configuration for every outer fold,
        'all' to save all feature importances or 'None' to not save any at all. Feature importances can only be saved
        if the chosen estimators have an attribute 'coef_' or 'feature_importances_'.

    * `project_folder` [bool, default=True]:
        If True, PHOTON writes a summary_file, the results of the hyperparameter optimization, the best model and the
        console output to the filesystem into the given project folder.

    * `project_folder` [str, default='']:
        The output folder in which all files generated by the PHOTON project are saved to.

    * `user_id` [str]:
       The user name of the according PHOTON Wizard login

    * `wizard_object_id` [str]:
       The object id to map the designed pipeline in the PHOTON Wizard to the results in the PHOTON CORE Database

    * `wizard_project_name` [str]:
       How the project is titled in the PHOTON Wizard
    """
    def __init__(self, mongodb_connect_url: str = None,
                 save_predictions: str = 'best',
                 save_feature_importances: str = 'best',
                 save_output: bool = True,
                 plots: bool = True,
                 overwrite_results: bool = False,
                 project_folder = '',
                 user_id: str = '',
                 wizard_object_id: str = '',
                 wizard_project_name: str = ''):

        self.mongodb_connect_url = mongodb_connect_url
        self.overwrite_results = overwrite_results

        self.save_best_config_predictions, self.save_predictions = self._set_save_options(save_predictions)
        self.save_best_config_feature_importances, self.save_feature_importances = self._set_save_options(save_feature_importances)

        self.__main_file__ = __main__.__file__
        if project_folder == '':
            self.project_folder = os.path.dirname(self.__main_file__)
        else:
            self.project_folder = project_folder

        self.results_folder = None
        self.log_file = os.path.join(self.project_folder, 'photon_output.log')
        self.save_output = save_output
        self.plots = plots

        # in case eval final performance is false, we have no outer fold predictions
        self.save_predictions_from_best_config_inner_folds = False

        self.user_id = user_id
        self.wizard_object_id = wizard_object_id
        self.wizard_project_name = wizard_project_name

    def _set_save_options(self, specifier):
        if specifier == 'best':
            save_best = True
            save_all = False
        elif specifier == 'all':
            save_best = True
            save_all = True
        elif specifier == 'None':
            save_best = False
            save_all = False
        else:
            raise ValueError('Possible options for saving predictions or feature importances are: "best", "all", "None"')
        return save_best, save_all

    def _update_settings(self, name, timestamp):
        if self.save_output:
            # Todo: give rights to user if this is done by docker container
            if self.overwrite_results:
                self.results_folder = os.path.join(self.project_folder, name + '_results')
            else:
                self.results_folder = os.path.join(self.project_folder, name + '_results_' + timestamp)

            if not os.path.exists(self.results_folder):
                os.makedirs(self.results_folder)
            shutil.copy(self.__main_file__, os.path.join(self.results_folder, 'photon_code.py'))

            self.log_file = self._add_timestamp(self.log_file)
            Logger().set_custom_log_file(self.log_file)

    def _add_timestamp(self, file):
        return os.path.join(self.results_folder, os.path.basename(file))


class Hyperpipe(BaseEstimator):
    """
    Wrapper class for machine learning pipeline, holding all pipeline elements
    and managing the optimization of the hyperparameters

    Parameters
    ----------
    * `name` [str]:
        Name of hyperpipe instance

    * `inner_cv` [BaseCrossValidator]:
        Cross validation strategy to test hyperparameter configurations, generates the validation set

    * `outer_cv` [BaseCrossValidator]:
        Cross validation strategy to use for the hyperparameter search itself, generates the test set

    * `optimizer` [str or object, default="grid_search"]:
        Hyperparameter optimization algorithm

        - In case a string literal is given:
            - "grid_search": optimizer that iteratively tests all possible hyperparameter combinations
            - "random_grid_search": a variation of the grid search optimization that randomly picks hyperparameter
               combinations from all possible hyperparameter combinations
            - "timeboxed_random_grid_search": randomly chooses hyperparameter combinations from the set of all
               possible hyperparameter combinations and tests until the given time limit is reached
               - `limit_in_minutes`: int

        - In case an object is given:
          expects the object to have the following methods:
           - `next_config_generator`: returns a hyperparameter configuration in form of an dictionary containing
              key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value`
           - `prepare`: takes a list of pipeline elements and their particular hyperparameters to test
           - `evaluate_recent_performance`: gets a tested config and the respective performance in order to
              calculate a smart next configuration to process

    * `metrics` [list of metric names as str]:
        Metrics that should be calculated for both training, validation and test set
        Use the preimported metrics from sklearn and photonai, or register your own

        - Metrics for `classification`:
            - `accuracy`: sklearn.metrics.accuracy_score
            - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef
            - `confusion_matrix`: sklearn.metrics.confusion_matrix,
            - `f1_score`: sklearn.metrics.f1_score
            - `hamming_loss`: sklearn.metrics.hamming_loss
            - `log_loss`: sklearn.metrics.log_loss
            - `precision`: sklearn.metrics.precision_score
            - `recall`: sklearn.metrics.recall_score
        - Metrics for `regression`:
            - `mean_squared_error`: sklearn.metrics.mean_squared_error
            - `mean_absolute_error`: sklearn.metrics.mean_absolute_error
            - `explained_variance`: sklearn.metrics.explained_variance_score
            - `r2`: sklearn.metrics.r2_score
        - Other metrics
            - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation
            - `variance_explained`:  photon_core.framework.Metrics.variance_explained_score
            - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score

    * `best_config_metric` [str]:
        The metric that should be maximized or minimized in order to choose the best hyperparameter configuration

    * `eval_final_performance` [bool, default=True]:
        If the metrics should be calculated for the test set, otherwise the test set is seperated but not used

    * `test_size` [float, default=0.2]:
        the amount of the data that should be left out if no outer_cv is given and
        eval_final_perfomance is set to True

    * `set_random_seed` [bool, default=False]:
        If True sets the random seed to 42

    * `verbosity` [int, default=0]:
        The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug

    * `groups` [array-like, default=None]:
        Info for advanced cross validation strategies, such as LeaveOneSiteOut-CV about the affiliation
        of the rows in the data. Also works with continuous values and StratifiedKFoldRegression. In case a group
        variable and a StratifiedCV is passed, the targets will be ignored and only the group variable will be used
        for the stratification.

    Attributes
    ----------
    * `optimum_pipe` [Pipeline]:
        An sklearn pipeline object that is fitted to the training data according to the best hyperparameter
        configuration found. Currently, we don't create an ensemble of all best hyperparameter configs over all folds.
        We find the best config by comparing the test error across outer folds. The hyperparameter config of the best
        fold is used as the optimal model and is then trained on the complete set.

    * `best_config` [dict]:
        Dictionary containing the hyperparameters of the best configuration.
        Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value

    * `results` [MDBHyperpipe]:
        Object containing all information about the for the performed hyperparameter search.
        Holds the training and test metrics for all outer folds, inner folds and configurations, as well as
        additional information.

    * `elements` [list]:
        Contains all PipelineElement or Hyperpipe objects that are added to the pipeline.

    Example
    -------
        manager = Hyperpipe('test_manager',
                            optimizer='timeboxed_random_grid_search', optimizer_params={'limit_in_minutes': 1},
                            outer_cv=ShuffleSplit(test_size=0.2, n_splits=1),
                            inner_cv=KFold(n_splits=10, shuffle=True),
                            metrics=['accuracy', 'precision', 'recall', "f1_score"],
                            best_config_metric='accuracy', eval_final_performance=True,
                            verbose=2)

   """
    def __init__(self, name, inner_cv: BaseCrossValidator, outer_cv=None,
                 optimizer='grid_search', optimizer_params: dict = {}, metrics=None,
                 best_config_metric=None, eval_final_performance=True, test_size: float = 0.2,
                 calculate_metrics_per_fold: bool = True, calculate_metrics_across_folds: bool = False,
                 set_random_seed: bool=False,
                 verbosity=0,
                 output_settings=None,
                 performance_constraints=None,
                 permutation_id: str=None,
                 cache_folder: str=None):

        self.name = re.sub(r'\W+', '', name)
        self.permutation_id = permutation_id
        if cache_folder:
            self.cache_folder = os.path.join(cache_folder, self.name)
        else:
            self.cache_folder = None
        # ====================== Cross Validation ===========================
        # check if both calculate_metrics_per_folds and calculate_metrics_across_folds is False
        if not calculate_metrics_across_folds and not calculate_metrics_per_fold:
            raise NotImplementedError("Apparently, you've set calculate_metrics_across_folds=False and "
                                      "calculate_metrics_per_fold=False. In this case PHOTON does not calculate "
                                      "any metrics which doesn't make any sense. Set at least one to True.")
        self.cross_validation = Hyperpipe.CrossValidation(inner_cv=inner_cv,
                                                          outer_cv=outer_cv,
                                                          eval_final_performance=eval_final_performance,
                                                          test_size=test_size,
                                                          calculate_metrics_per_fold=calculate_metrics_per_fold,
                                                          calculate_metrics_across_folds=calculate_metrics_across_folds)

        # ====================== Data ===========================
        self.data = Hyperpipe.Data()

        # ====================== Result Logging ===========================
        if output_settings:
            self.output_settings = output_settings
        else:
            self.output_settings = OutputSettings()
        self.verbosity = verbosity
        self.results_handler = None
        self.results = None
        self.best_config = None
        self.estimation_type = None

        # ====================== Pipeline ===========================
        self.elements = []
        self._pipe = None
        self.optimum_pipe = None
        self.preprocessing = None

        # ====================== Perfomance Optimization ===========================

        self.optimization = Hyperpipe.Optimization(metrics=metrics,
                                                   best_config_metric=best_config_metric,
                                                   optimizer_input=optimizer,
                                                   optimizer_params=optimizer_params,
                                                   performance_constraints=performance_constraints)

        self.optimization.sanity_check_metrics()

        # ====================== Internals ===========================

        self.is_final_fit = False

        if set_random_seed:
            import random
            random.seed(42)
            print('set random seed to 42')

    # Helper Classes
    #
    #
    #
    # ============= Cross Validation ==================================================================
    class CrossValidation:

        def __init__(self, inner_cv, outer_cv,
                     eval_final_performance, test_size,
                     calculate_metrics_per_fold,
                     calculate_metrics_across_folds):
            self.inner_cv = inner_cv
            self.outer_cv = outer_cv
            self.eval_final_performance = eval_final_performance
            self.test_size = test_size
            self.calculate_metrics_per_fold = calculate_metrics_per_fold
            # Todo: if self.outer_cv is LeaveOneOut: Set calculate metrics across folds to True -> Print
            self.calculate_metrics_across_folds = calculate_metrics_across_folds

            self.outer_folds = None
            self.inner_folds = dict()


    # ============= Data ==================================================================
    class Data:

        def __init__(self, X=None, y=None, kwargs=None, groups=None):
            self.X = X
            self.y = y
            self.kwargs = kwargs
            self.groups = groups

    # ============= Performance Optimization ==================================================================
    class Optimization:

        OPTIMIZER_DICTIONARY = {'grid_search': GridSearchOptimizer,
                                'random_grid_search': RandomGridSearchOptimizer,
                                'timeboxed_random_grid_search': TimeBoxedRandomGridSearchOptimizer,
                                'sk_opt': SkOptOptimizer}  # ,

        # 'fabolas': FabolasOptimizer}

        def __init__(self, optimizer_input, optimizer_params,
                     metrics, best_config_metric, performance_constraints):

            self.optimizer_input = optimizer_input
            self.optimizer_params = optimizer_params
            self.metrics = metrics
            self.best_config_metric = best_config_metric
            self.maximize_metric = True
            self.inner_cv_callback_functions = performance_constraints

        def sanity_check_metrics(self):

            # --------------------- Validity of metrics ----------------
            if isinstance(self.best_config_metric, list) or not isinstance(self.best_config_metric, str):

                if self.metrics is not None:
                    warning_text = "Best Config Metric must be a single metric given as string, no list. " \
                                   "PHOTON chose the first one from the list of metrics to calculate."

                    self.best_config_metric = self.metrics[0]
                    Logger().warn(warning_text)
                    raise Warning(warning_text)
                else:
                    error_msg = "No metrics were chosen. Please choose metrics to quantify your performance and set " \
                                "the best_config_metric so that PHOTON which optimizes for"
                    Logger().error(error_msg)
                    raise ValueError(error_msg)

            if self.best_config_metric is not None:
                if self.metrics is None:
                    self.metrics = [self.best_config_metric]
                else:
                    if self.best_config_metric not in self.metrics:
                        self.metrics.append(self.best_config_metric)

            if self.best_config_metric is None and len(self.metrics) > 0:
                self.best_config_metric = self.metrics[0]
                warning_text = "No best config metric was given, so PHOTON chose the first in the list of metrics as " \
                               "criteria for choosing the best configuration."
                Logger().warn(warning_text)
                raise Warning(warning_text)
            else:
                if self.metrics is None or len(self.metrics) == 0:
                    metric_error_text = "List of Metrics to calculate should not be empty"
                    Logger().error(metric_error_text)
                    raise ValueError(metric_error_text)

        def get_optimizer(self):
            if isinstance(self.optimizer_input, str):
                # instantiate optimizer from string
                #  Todo: check if optimizer strategy is already implemented
                optimizer_class = self.OPTIMIZER_DICTIONARY[self.optimizer_input]
                optimizer_instance = optimizer_class(**self.optimizer_params)
                return optimizer_instance
            else:
                # Todo: check if object has the right interface
                return self.optimizer_input

        def get_optimum_config(self, tested_configs):
            """
            Looks for the best configuration according to the metric with which the configurations are compared -> best config metric
            :param tested_configs: the list of tested configurations and their performances
            :return: MDBConfiguration that has performed best
            """

            list_of_config_vals = []
            list_of_non_failed_configs = [conf for conf in tested_configs if not conf.config_failed]

            if len(list_of_non_failed_configs) == 0:
                raise Warning("No Configs found which did not fail.")
            try:

                if len(list_of_non_failed_configs) == 1:
                    best_config_outer_fold = list_of_non_failed_configs[0]
                else:
                    for config in list_of_non_failed_configs:
                        list_of_config_vals.append(
                            MDBHelper.get_metric(config, FoldOperations.MEAN, self.best_config_metric, train=False))

                    if self.maximize_metric:
                        # max metric
                        best_config_metric_nr = np.argmax(list_of_config_vals)
                    else:
                        # min metric
                        best_config_metric_nr = np.argmin(list_of_config_vals)

                    best_config_outer_fold = list_of_non_failed_configs[best_config_metric_nr]

                # inform user
                Logger().verbose('Number of tested configurations:' + str(len(tested_configs)))
                Logger().verbose('Optimizer metric: ' + self.best_config_metric + '\n' +
                                 '   --> Greater is better: ' + str(self.maximize_metric))
                Logger().info('Best config: ' + str(best_config_outer_fold.human_readable_config))

                return best_config_outer_fold
            except BaseException as e:
                Logger().error(str(e))

        def get_optimum_config_outer_folds(self, outer_folds):
            list_of_scores = list()
            for outer_fold in outer_folds:
                metrics = outer_fold.best_config.best_config_score.validation.metrics
                list_of_scores.append(metrics[self.best_config_metric])

            if self.maximize_metric:
                # max metric
                best_config_metric_nr = np.argmax(list_of_scores)
            else:
                # min metric
                best_config_metric_nr = np.argmin(list_of_scores)

            best_config = outer_folds[best_config_metric_nr].best_config
            return best_config

        def define_optimizer_metric(self):
            """
            Analyse and prepare the best config metric.
            Derive if it is better when the value increases or decreases.
            """
            if isinstance(self.best_config_metric, str):
                self.maximize_metric = Scorer.greater_is_better_distinction(self.best_config_metric)

    # Setters
    #
    #
    def _set_verbosity(self, verbosity):
        """
        Set verbosity level manually
        Returns None

        Parameters
        ----------
        * `verbosity` [Integer]:
            Verbosity level can be 0, 1, or 2.

        """
        Logger().set_verbosity(verbosity)

    def _set_persist_options(self, persist_options):
        """
        Set persist options manually
        Returns None

        Parameters
        ----------
        * `persist_options` [OutputSettings]:

        """
        self.output_settings = persist_options

    # Pipeline Management & Interface
    #
    #
    def __iadd__(self, pipe_element):
        """
        Add an element to the machine learning pipeline
        Returns self

        Parameters
        ----------
        * 'pipe_element' [PipelineElement]:
            The object to add to the machine learning pipeline, being either a transformer or an estimator.

        """
        if isinstance(pipe_element, Preprocessing):
            self.preprocessing = pipe_element
        elif isinstance(pipe_element, CallbackElement):
            pipe_element.needs_y = True
            self.elements.append(pipe_element)
        else:
            if isinstance(pipe_element, PipelineElement) or issubclass(type(pipe_element), PhotonNative):
                self.elements.append(pipe_element)
                # Todo: is repeated each time element is added....
                self._prepare_pipeline()
            else:
                raise TypeError("Element must be of type Pipeline Element")
        return self

    def add(self, pipe_element):
        """
           Add an element to the machine learning pipeline
           Returns self

           Parameters
           ----------
           * `pipe_element` [PipelineElement or Hyperpipe]:
               The object to add to the machine learning pipeline, being either a transformer or an estimator.

           """
        self.__iadd__(pipe_element)

    def _prepare_dummy_estimator(self):
        Logger().info("Running Dummy Estimator.")
        est_type = self.estimation_type

        # Run Dummy Estimator
        self.results.dummy_estimator = MDBDummyResults()

        if est_type == 'regressor':
            self.results.dummy_estimator.strategy = 'mean'
            return DummyRegressor(strategy=self.results.dummy_estimator.strategy)
        elif est_type == 'classifier':
            self.results.dummy_estimator.strategy = 'most_frequent'
            return DummyClassifier(strategy=self.results.dummy_estimator.strategy)
        else:
            Logger().info('Estimator does not specify whether it is a regressor or classifier. DummyEstimator '
                          'step skipped.')
            return

    def _evaluate_dummy_estimator(self, fold_list):
        config_item = MDBConfig()
        config_item.inner_folds = [f for f in fold_list if f is not None]
        if len(config_item.inner_folds) > 0:
            self.results.dummy_estimator.train, self.results.dummy_estimator.test = MDBHelper.aggregate_metrics_for_inner_folds(config_item.inner_folds,
                                                                                                                 self.optimization.metrics)

    def _prepare_result_logging(self, start_time):
        results_object_name = self.name

        self.results = MDBHyperpipe(name=results_object_name)
        self.results.hyperpipe_info = MDBHyperpipeInfo()
        # in case eval final performance is false, we have no outer fold predictions
        if not self.cross_validation.eval_final_performance:
            self.output_settings.save_predictions_from_best_config_inner_folds = True
        self.results_handler = ResultsHandler(self.results, self.output_settings)

        self.results.computation_start_time = start_time
        self.results.hyperpipe_info.estimation_type = self.estimation_type

        if self.permutation_id is not None:
            self.results.permutation_id = self.permutation_id

        # save wizard information to photon db in order to map results to the wizard design object
        if self.output_settings and hasattr(self.output_settings, 'wizard_object_id'):
            if self.output_settings.wizard_object_id:
                self.name = self.output_settings.wizard_object_id
                self.results.name = self.output_settings.wizard_object_id
                self.results.wizard_object_id = ObjectId(self.output_settings.wizard_object_id)
                self.results.wizard_system_name = self.output_settings.wizard_project_name
                self.results.user_id = self.output_settings.user_id
        self.results.outer_folds = []
        self.results.hyperpipe_info.eval_final_performance = self.cross_validation.eval_final_performance
        self.results.hyperpipe_info.best_config_metric = self.optimization.best_config_metric
        self.results.hyperpipe_info.metrics = self.optimization.metrics

        # optimization
        def _format_cross_validation(cv):
            if cv:
                string = "{}(".format(cv.__class__.__name__)
                for key, val in cv.__dict__.items():
                    string += "{}={}, ".format(key, val)
                return string[:-2] + ")"
            else:
                return "None"

        self.results.hyperpipe_info.cross_validation = {'OuterCV': _format_cross_validation(self.cross_validation.outer_cv),
                                                        'InnerCV': _format_cross_validation(self.cross_validation.inner_cv)}
        self.results.hyperpipe_info.data = {'X.shape': self.data.X.shape, 'y.shape': self.data.y.shape}
        self.results.hyperpipe_info.optimization = {'Optimizer': self.optimization.optimizer_input,
                                                        'OptimizerParams': str(self.optimization.optimizer_params),
                                                        'BestConfigMetric': self.optimization.best_config_metric}

        # add flowchart to results
        try:
            flowchart = FlowchartCreator(self.elements)
            self.results.hyperpipe_info.flowchart = flowchart.create_str()
        except:
            self.results.hyperpipe_info.flowchart = ""

    def _finalize_optimization(self):
        # ==================== EVALUATING RESULTS OF HYPERPARAMETER OPTIMIZATION ===============================
        # 1. computing average metrics
        # 2. finding overall best config
        # 3. training model with best config
        # 4. persisting best model

        # Compute all final metrics
        self.results.metrics_train, self.results.metrics_test = MDBHelper.aggregate_metrics_for_outer_folds(self.results.outer_folds,
                                                                                                            self.optimization.metrics)

        # save result tree to db or file or both
        Logger().info('Finished hyperparameter optimization!')
        self.results_handler.save()

        # Find best config across outer folds
        self.best_config = self.optimization.get_optimum_config_outer_folds(self.results.outer_folds)
        self.results.best_config = self.best_config
        Logger().info('OVERALL BEST CONFIGURATION')
        Logger().info('--------------------------')
        Logger().info(self.best_config.human_readable_config)

        # save results again
        self.results.time_of_results = datetime.datetime.now()
        self.results.computation_completed = True
        self.results_handler.save()
        Logger().info("Saved overall best config to database ")

        # write all convenience files (summary, predictions_file and plots)
        self.results_handler.write_convenience_files()

        # set self to best config
        self.optimum_pipe = self._pipe
        self.optimum_pipe.set_params(**self.best_config.config_dict)

        # set caching
        # we want caching disabled in general but still want to do single subject caching
        self.recursive_cache_folder_propagation(self.optimum_pipe, self.cache_folder, 'fixed_fold_id')
        self.optimum_pipe.caching = False

        # disable multiprocessing when fitting optimum pipe and save_feature_importances is true
        # (otherwise inverse_transform won't work for BrainAtlas/Mask)
        if self.output_settings.save_best_config_feature_importances:
            self.disable_multiprocessing_recursively(self.optimum_pipe)

        Logger().info("Fitting best model...")
        self.optimum_pipe.fit(self.data.X, self.data.y, **self.data.kwargs)

        # Before saving the optimum pipe, add preprocessing
        self.optimum_pipe._add_preprocessing(self.preprocessing)

        # Now truly set to no caching (including single_subject_caching)
        self.recursive_cache_folder_propagation(self.optimum_pipe, None, None)

        if self.output_settings.save_output:
            Logger().info("Saving best model...")
            try:
                pretrained_model_filename = os.path.join(self.output_settings.results_folder, 'photon_best_model.photon')
                PhotonModelPersistor.save_optimum_pipe(self, pretrained_model_filename)
                Logger().info("Saved optimum pipe model to file")
            except FileNotFoundError as e:
                Logger().info("Could not save optimum pipe model to file")
                Logger().error(str(e))

        if self.output_settings.save_best_config_feature_importances and self.output_settings.save_output:
            # get feature importances of optimum pipe
            Logger().info("Mapping back feature importances...")
            feature_importances = OuterFoldManager.extract_feature_importances(self.optimum_pipe)
            if not feature_importances:
                Logger().info("No feature importances available for {}!".format(self.optimum_pipe.elements[-1][0]))
                return

            self.results.best_config_feature_importances = feature_importances

            # get backmapping
            backmapping, _, _ = self.optimum_pipe.inverse_transform(feature_importances, None)

            # save backmapping
            self.results_handler.save_backmapping(filename='optimum_pipe_feature_importances_backmapped',
                                 backmapping=backmapping)

    @staticmethod
    def disable_multiprocessing_recursively(pipe):
        if isinstance(pipe, (Stack, Branch, Switch, Preprocessing)):
            if hasattr(pipe, 'nr_of_processes'):
                pipe.nr_of_processes = 1
            for child in pipe.elements:
                Hyperpipe.disable_multiprocessing_recursively(child.base_element)
        elif isinstance(pipe, PhotonPipeline):
            for name, child in pipe.named_steps.items():
                Hyperpipe.disable_multiprocessing_recursively(child)
        else:
            if hasattr(pipe, 'nr_of_processes'):
                pipe.nr_of_processes = 1

    def _input_data_sanity_checks(self, data, targets, **kwargs):
        # ==================== SANITY CHECKS ===============================
        # 1. Make to numpy arrays
        # 2. erase all Nan targets

        self.data.X = data
        self.data.y = targets
        self.data.kwargs = kwargs

        try:
            if self.data.X is None:
                raise ValueError("(Input-)data is a NoneType.")
            if self.data.y is None:
                raise ValueError("(Input-)target is a NoneType.")

            shape_X = np.shape(self.data.X)
            shape_y = np.shape(self.data.y)
            if len(shape_y) != 1:
                raise ValueError("Target is not one-dimensional.")
            if not shape_X[0] == shape_y[0]:
                raise IndexError(
                    "Size of targets mismatch to the size of the data: " + str(shape_X[0]) + " - " + str(shape_y[0]))
        except IndexError as ie:
            Logger().error("IndexError: " + str(ie))
            raise ie
        except ValueError as ve:
            Logger().error("ValueError: " + str(ve))
            raise ve
        except Exception as e:
            Logger().error("Error: " + str(e))
            raise e

        # be compatible to list of (image-) files
        if isinstance(self.data.X, list):
            self.data.X = np.asarray(self.data.X)
        elif isinstance(self.data.X, pd.DataFrame):
            self.data.X = self.data.X.to_numpy()
        if isinstance(self.data.y, list):
            self.data.y = np.asarray(self.data.y)
        elif isinstance(self.data.y, pd.Series) or isinstance(self.data.y, pd.DataFrame):
            self.data.y = self.data.y.to_numpy()

        # at first first, erase all rows where y is Nan if preprocessing has not done it already
        try:
            nans_in_y = np.isnan(self.data.y)
            nr_of_nans = len(np.where(nans_in_y == 1)[0])
            if nr_of_nans > 0:
                Logger().info("You have " + str(nr_of_nans) + " Nans in your target vector, "
                                                              "PHOTON erases every data item that has a Nan Target")
                self.data.X = self.data.X[~nans_in_y]
                self.data.y = self.data.y[~nans_in_y]
        except Exception as e:
            # This is only for convenience so if it fails then never mind
            Logger().error("Removing Nans in target vector failed: " + str(e))
            pass

        Logger().info("Hyperpipe is training with " + str(self.data.y.shape[0]) + " samples.")

    # @staticmethod
    # def prepare_caching(cache_folder):
    #     if cache_folder and not os.path.isdir(cache_folder):
    #         os.makedirs(cache_folder, exist_ok=True)

    @staticmethod
    def recursive_cache_folder_propagation(element, cache_folder, inner_fold_id):
        if isinstance(element, (Switch, Stack, Preprocessing)):
            for child in element.elements:
                Hyperpipe.recursive_cache_folder_propagation(child.base_element, cache_folder, inner_fold_id)

        elif isinstance(element, Branch):
            # in case it's a Branch, we create a cache subfolder and propagate it to every child
            if cache_folder:
                cache_folder = os.path.join(cache_folder, element.name)
            Hyperpipe.recursive_cache_folder_propagation(element.base_element, cache_folder, inner_fold_id)
            # Hyperpipe.prepare_caching(element.base_element.cache_folder)

        elif isinstance(element, PhotonPipeline):
            element.fold_id = inner_fold_id
            element.cache_folder = cache_folder

            # pipe.caching is automatically set to True or False by .cache_folder setter

            for name, child in element.named_steps.items():
                # we need to check if any element is Branch, Stack or Swtich
                Hyperpipe.recursive_cache_folder_propagation(child, cache_folder, inner_fold_id)

        # if it's a simple PipelineElement, then we just don't do anything

    def preprocess_data(self):
        # if there is a preprocessing pipeline, we apply it first.
        if self.preprocessing is not None:
            Logger().info("Applying preprocessing.")
            self.preprocessing.fit(self.data.X, self.data.y, **self.data.kwargs)
            self.data.X, self.data.y, self.data.kwargs = self.preprocessing.transform(self.data.X, self.data.y,
                                                                                      **self.data.kwargs)

    def _check_for_estimator(self, last_element=None):
        if not last_element:
            last_element = self.elements[-1]
        if isinstance(last_element, (Switch, Stack, Branch)):
            # if Switch, just take the first element within the switch; this should be a regressor or classifier
            self._check_for_estimator(last_element.elements[-1])
        else:
            if not hasattr(last_element.base_element, '_estimator_type'):
                raise NotImplementedError("Last pipeline element has to be an estimator. Your estimator does not specify"
                                          " whether it is a regressor or classifier. Make sure to inherit from sklearn's "
                                          "ClassifierMixin or RegressorMixin or set _estimator_type explicitly.")
            estimator_type = last_element.base_element._estimator_type
            last_name = last_element.name
            self.estimation_type = estimator_type
            if not (estimator_type == 'classifier' or estimator_type == 'regressor'):
                raise NotImplementedError("Last pipeline element has to be an estimator. {} is a {}.".format(last_name,
                                                                                                         estimator_type))

    def fit(self, data, targets, **kwargs):
        """
        Starts the hyperparameter search and/or fits the pipeline to the data and targets

        Manages the nested cross validated hyperparameter search:

        1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2)
        2. requests new configurations from the hyperparameter search strategy, the optimizer,
        3. initializes the testing of a specific configuration,
        4. communicates the result to the optimizer,
        5. repeats 2-4 until optimizer delivers no more configurations to test
        6. finally searches for the best config in all tested configs,
        7. trains the pipeline with the best config and evaluates the performance on the test set

        Parameters
        ----------
         * `data` [array-like, shape=[N, D]]:
            the training and test data, where N is the number of samples and D is the number of features.

         * `targets` [array-like, shape=[N]]:
            the truth values, where N is the number of samples.


        Returns
        -------
         * 'self'
            Returns self

        """
        try:

            self._input_data_sanity_checks(data, targets, **kwargs)
            self._check_for_estimator()
            self.preprocess_data()
            Logger().set_verbosity(self.verbosity)

            if not self.is_final_fit:

                # first check if correct optimizer metric has been chosen
                # pass elements so that OptimizerMetric can look for last
                # element and use the corresponding score method
                self.optimization.define_optimizer_metric()

                start = datetime.datetime.now()
                self._prepare_result_logging(start)

                # update output options to add pipe name and timestamp to results folder
                self.output_settings._update_settings(self.name, start.strftime("%Y-%m-%d_%H-%M-%S"))

                # Outer Folds
                outer_folds = FoldInfo.generate_folds(self.cross_validation.outer_cv,
                                                      self.data.X, self.data.y, self.data.groups,
                                                      self.cross_validation.eval_final_performance,
                                                      self.cross_validation.test_size)

                self.cross_validation.outer_folds = {f.fold_id: f for f in outer_folds}

                # Run Dummy Estimator
                dummy_estimator = self._prepare_dummy_estimator()
                dummy_results = []

                if self.cache_folder is not None:
                    Logger().info("Removing Cache Files")
                    CacheManager.clear_cache_files(self.cache_folder, force_all=True)


                # loop over outer cross validation
                for i, outer_f in enumerate(outer_folds):
                    Logger().info('HYPERPARAMETER SEARCH OF {0}, Outer Cross validation Fold {1}'
                                  .format(self.name, outer_f.fold_nr))

                    # 1. generate OuterFolds Object

                    outer_fold_computer = OuterFoldManager(self._copy_pipeline,
                                                           self.optimization,
                                                           outer_f.fold_id,
                                                           self.cross_validation,
                                                           save_feature_importances=self.output_settings.save_feature_importances,
                                                           save_predictions=self.output_settings.save_predictions,
                                                           save_best_config_feature_importances=self.output_settings.save_best_config_feature_importances,
                                                           save_best_config_predictions=self.output_settings.save_best_config_predictions,
                                                           cache_folder=self.cache_folder,
                                                           cache_updater=self.recursive_cache_folder_propagation)
                    # 2. prepare
                    outer_fold = MDBOuterFold(fold_nr=outer_f.fold_nr)
                    self.results.outer_folds.append(outer_fold)
                    outer_fold_computer.prepare_optimization(self.elements, outer_fold)
                    dummy_results.append(outer_fold_computer.fit_dummy(self.data.X, self.data.y, dummy_estimator))

                    try:
                        # 3. fit
                        outer_fold_computer.fit(self.data.X, self.data.y, **self.data.kwargs)
                        # 4. save outer fold results
                        self.results_handler.save()
                    finally:
                        # 5. clear cache
                        CacheManager.clear_cache_files(self.cache_folder)

                # evaluate hyperparameter optimization results for best config
                self._evaluate_dummy_estimator(dummy_results)
                self._finalize_optimization()

                # clear complete cache ?
                CacheManager.clear_cache_files(self.cache_folder, force_all=True)

            ###############################################################################################
            else:
                self.preprocess_data()
                self._pipe.fit(self.data.X, self.data.y, **kwargs)

        except Exception as e:
            Logger().error(e)
            Logger().error(traceback.format_exc())
            traceback.print_exc()
            raise e
        return self

    def predict(self, data, **kwargs):
        """
        Use the optimum pipe to predict the data

        Returns
        -------
            predicted targets

        """
        # Todo: if local_search = true then use optimized pipe here?
        if self._pipe:
            return self.optimum_pipe.predict(data, **kwargs)

    def predict_proba(self, data, **kwargs):
        """
        Predict probabilities

        Returns
        -------
        predicted probabilities

        """
        if self._pipe:
            return self.optimum_pipe.predict_proba(data, **kwargs)

    def transform(self, data, **kwargs):
        """
        Use the optimum pipe to transform the data
        """
        if self._pipe:
            X, _, _ = self.optimum_pipe.transform(data, y=None, **kwargs)
            return X

    def get_params(self, deep=True):
        """
        Retrieve parameters from sklearn pipeline
        """
        if self._pipe is not None:
            return self._pipe.get_params(deep)
        else:
            return None

    def set_params(self, **params):
        """
        Give parameter values to the pipeline elements
        """
        if self._pipe is not None:
            self._pipe.set_params(**params)
        return self

    def _prepare_pipeline(self):
        """
        build sklearn pipeline from PipelineElements and
        calculate parameter grid for all combinations of pipeline element hyperparameters
        """
        # prepare pipeline
        pipeline_steps = []
        for item in self.elements:
            # pipeline_steps.append((item.name, item.base_element))
            pipeline_steps.append((item.name, item))

        # build pipeline...
        self._pipe = PhotonPipeline(pipeline_steps)

    def copy_me(self):
        """
        Helper function to copy an entire Hyperpipe
        :return: Hyperpipe
        """
        # create new Hyperpipe instance
        pipe_copy = Hyperpipe(name=self.name, inner_cv=self.cross_validation.inner_cv,
                              best_config_metric=self.optimization.best_config_metric, metrics=self.optimization.metrics)

        signature = inspect.getfullargspec(self.__init__)[0]
        for attr in signature:
            if hasattr(self, attr):
                setattr(pipe_copy, attr, getattr(self, attr))

        if hasattr(self, 'preprocessing') and self.preprocessing:
            preprocessing = Preprocessing()
            for element in self.preprocessing.pipeline_elements:
                preprocessing += element.copy_me()
            pipe_copy += preprocessing
        if hasattr(self, 'elements'):
            for element in self.elements:
                pipe_copy += element.copy_me()
        return pipe_copy

    def _copy_pipeline(self):
        """
        Copy Pipeline by building a new sklearn Pipeline with Pipeline Elements

        Returns
        -------
        new sklearn Pipeline object
        """
        pipeline_steps = []
        for item in self.elements:
            cpy = item.copy_me()
            if isinstance(cpy, list):
                for new_step in cpy:
                    pipeline_steps.append((new_step.name, new_step))
            else:
                pipeline_steps.append((cpy.name, cpy))
        new_pipe = PhotonPipeline(pipeline_steps)
        return new_pipe

    def save_optimum_pipe(self, filename=None, password=None):
        if filename is None:
            filename = "photon_" + self.name + "_best_model.p"
        PhotonModelPersistor.save_optimum_pipe(self, filename, password)

    @staticmethod
    def load_optimum_pipe(file, password=None):
        return PhotonModelPersistor.load_optimum_pipe(file, password)

    def inverse_transform_pipeline(self, hyperparameters: dict, data, targets, data_to_inverse):
        """
        Inverse transform data for a pipeline with specific hyperparameter configuration

        1. Copy Sklearn Pipeline,
        2. Set Parameters
        3. Fit Pipeline to data and targets
        4. Inverse transform data with that pipeline

        Parameters
        ----------
        * `hyperparameters` [dict]:
            The concrete configuration settings for the pipeline elements
        * `data` [array-like]:
            The training data to which the pipeline is fitted
        * `targets` [array-like]:
            The truth values for training
        * `data_to_inverse` [array-like]:
            The data that should be inversed after training

        Returns
        -------
        Inversed data as array
        """
        copied_pipe = self._copy_pipeline()
        copied_pipe.set_params(**hyperparameters)
        copied_pipe.fit(data, targets)
        return copied_pipe.inverse_transform(data_to_inverse)


class FlowchartCreator(object):

    def __init__(self, pipeline_elements):
        self.pipeline_elements = pipeline_elements
        self.chart_str = ""

    def create_str(self):
        header_layout = ""
        header_relate = ""
        old_element = ""
        for pipeline_element in self.pipeline_elements:
            header_layout = header_layout + "[" + pipeline_element.name + "]"
            if old_element:
                header_relate = header_relate + "[" + old_element + "]" + "->" + "[" + pipeline_element.name + "]\n"
            old_element = pipeline_element.name

        self.chart_str = "Layout:\n" + header_layout + "\nRelate:\n" + header_relate + "\n"

        for pipeline_element in self.pipeline_elements:
            self.chart_str = self.chart_str + self.recursive_element(pipeline_element, "")

        return self.chart_str

    @staticmethod
    def format_cross_validation(cv):
        if cv:
            string = "{}(".format(cv.__class__.__name__)
            for key, val in cv.__dict__.items():
                string += "{}={}, ".format(key, val)
            return string[:-2] + ")"
        else:
            return "None"

    @staticmethod
    def format_optimizer(optimizer):
        return optimizer.optimizer_input, optimizer.optimizer_params, optimizer.metrics, optimizer.best_config_metric

    def format_kwargs(self, kwargs):
        pass

    @staticmethod
    def format_hyperparameter(hyperparameter):
        if isinstance(hyperparameter, IntegerRange):
            return """IntegerRange(start: {},
                                   stop: {}, 
                                   step: {}, 
                                   range_type: {})""".format(hyperparameter.start, hyperparameter.stop,
                                                                           hyperparameter.step, hyperparameter.range_type)
        elif isinstance(hyperparameter, FloatRange):
            return """FloatRange(start: {},
                                   stop: {}, 
                                   step: {}, 
                                   range_type: {})""".format(hyperparameter.start,
                                                               hyperparameter.stop,
                                                               hyperparameter.step,
                                                               hyperparameter.range_type)
        elif isinstance(hyperparameter, Categorical):
            return str(hyperparameter.values)
        else:
            return str(hyperparameter)

    def recursive_element(self, pipe_element, parent):

        # PHOTON pipeline
        string = ""

        # Pipeline Stack
        if isinstance(pipe_element, Stack):
            if parent == "":
                string = "[" + pipe_element.name + "]:\n" + "Layout:\n"
            else:
                string = "["+parent[1:] + "." + pipe_element.name + "]:\n" + "Layout:\n"

            # Layout
            for pelement in list(pipe_element.elements):
                string = string + "[" + pelement.name + "]|\n"
            string = string +"\n"
            for pelement in list(pipe_element.elements):
                string = string + "\n" + self.recursive_element(pelement, parent=parent + "." + pipe_element.name)


        # Pipeline Switch
        elif isinstance(pipe_element, Switch):
            if parent == "":
                string = "[" + pipe_element.name + "]:\n" + "Layout:\n"
            else:
                string = "[" + parent[1:] + "." + pipe_element.name + "]:\n" + "Layout:\n"

            # Layout
            for pelement in pipe_element.elements:
                string = string + "[" + pelement.name + "]\n"
            string = string + "\n"

            # relate
            string = string + "\n" + "Relate:\n"
            old_element = ""
            for pelement in pipe_element.elements:
                if old_element:
                    string = string + "[" + old_element + "]" + "<:-:>" + "[" + pelement.name + ']\n'
                old_element = pelement.name
                string = string + "\n"

            for pelement in pipe_element.elements:
                string = string + "\n" + self.recursive_element(pelement, parent=parent + "." + pipe_element.name)


        # Pipeline Branch
        elif isinstance(pipe_element, Branch):
            if parent == "":
                string = "[" + pipe_element.name + "]:\n" + "Layout:\n"
            else:
                string = "[" + parent[1:]+"."+pipe_element.name + "]:\n" + "Layout:\n"

            # Layout
            for pelement in pipe_element.elements:
                string = string + "[" + pelement.name + "]"
            string = string + "\n" + "Relate:\n"
            # Relate
            old_element = ""
            for pelement in pipe_element.elements:
                if old_element:
                    string = string + "[" + old_element + "]" + "->" + "[" + pelement.name + ']\n'
                old_element = pelement.name
                string = string + "\n"

            for pelement in pipe_element.elements:
                string = string + "\n" + self.recursive_element(pelement, parent=parent + "." + pipe_element.name)

        elif isinstance(pipe_element, PipelineElement):
            if parent == "":
                string = "[" + pipe_element.name + "]:\n" + "Define:\n"
            else:
                string = "[" + parent[1:] + "." + pipe_element.name + "]:\n" + "Define:\n"
            hyperparameters = None
            kwargs = None
            if hasattr(pipe_element, "hyperparameters"):
                hyperparameters = pipe_element.hyperparameters
                for name, parameter in pipe_element.hyperparameters.items():
                    string += "{}: {}\n".format(name.split('__')[-1], self.format_hyperparameter(parameter))
            if hasattr(pipe_element, "kwargs"):
                kwargs = pipe_element.kwargs
                for name, parameter in pipe_element.kwargs.items():
                    string += "{}: {}\n".format(name.split('__')[-1], self.format_hyperparameter(parameter))
            if not kwargs and not hyperparameters:
                string += "default\n"

        return string


class PhotonModelPersistor:

    @staticmethod
    def save_optimum_pipe(hyperpipe, file, password=None):
        """
        Save optimal pipeline only. Complete hyperpipe will no not be saved.

        Parameters
        ----------
        * 'file' [str]:
            File path as string specifying file to save pipeline to
        * 'password' [str]:
            Password used to encrypt the pipeline file

        """
        def save_element(element, element_number, element_name, folder, wrapper_files):
            filename = '_optimum_pipe_' + str(element_number) + '_' + element_name
            element_identifier.append({'element_name': element_name,
                                       'filename': filename})
            if hasattr(element, 'base_element'):
                base_element = element.base_element
            else:
                base_element = element
            if hasattr(base_element, 'save'):
                base_element.save(os.path.join(folder, filename))
                element_identifier[-1]['mode'] = 'custom'
                element_identifier[-1]['wrapper_script'] = os.path.basename(inspect.getfile(base_element.__class__))
                wrapper_files.append(inspect.getfile(base_element.__class__))
                element_identifier[-1]['test_disabled'] = element.test_disabled
                element_identifier[-1]['disabled'] = element.disabled
                element_identifier[-1]['hyperparameters'] = element.hyperparameters

            else:
                try:
                    joblib.dump(element, os.path.join(folder, filename) + '.pkl', compress=1)
                    element_identifier[-1]['mode'] = 'pickle'
                except:
                    raise NotImplementedError("Custom pipeline element must implement .save() method or "
                                              "allow pickle.")
            return wrapper_files

        element_number = 0
        element_identifier = list()
        folder = os.path.splitext(file)[0]
        file = os.path.splitext(file)[0] + '.photon'

        if os.path.exists(folder):
            Logger().warn('The file you specified already exists as a folder.')
        else:
            os.makedirs(folder)
        wrapper_files = list()

        for name, element in hyperpipe.optimum_pipe.elements:
            wrapper_files = save_element(element, element_number, name, folder, wrapper_files)
            element_number += 1

        # save pipeline blueprint to make loading of pipeline easier
        with open(os.path.join(folder, '_optimum_pipe_blueprint.pkl'), 'wb') as f:
            pickle.dump(element_identifier, f)

        # get all files
        files = glob.glob(os.path.join(folder, '_optimum_pipe_*'))

        if password is not None:
            # ToDo: Do this without the need for pyminizip's C++ requirements
            import pyminizip
            pyminizip.compress(files, file, password)
        else:
            with zipfile.ZipFile(file, 'w') as myzip:
                for f in files:
                    myzip.write(f, os.path.basename(f))
                    os.remove(f)
                for f in wrapper_files:
                    myzip.write(f, os.path.splitext(os.path.basename(f))[0] + '.py')
        os.removedirs(folder)


    @staticmethod
    def load_optimum_pipe(file, password=None):
        """
        Load optimal pipeline.


        Parameters
        ----------
        * `file` [str]:
            File path specifying .photon file to load optimal pipeline from

        Returns
        -------
        sklearn Pipeline with all trained photon_pipelines
        """
        if file.endswith('.photon'):
            archive_name = os.path.splitext(file)[0]
            folder = archive_name
            zf = zipfile.ZipFile(file)
            zf.extractall(folder, pwd=password)
        else:
            raise FileNotFoundError('Specify .photon file that holds PHOTON optimum pipe.')

        with open(os.path.join(folder, '_optimum_pipe_blueprint.pkl'), 'rb') as f:
            setup_info = pickle.load(f)
            element_list = list()
            for element_info in setup_info:
                if element_info['mode'] == 'custom':
                    spec = importlib.util.spec_from_file_location(element_info['element_name'],
                                                                  os.path.join(folder, element_info['wrapper_script']))
                    imported_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(imported_module)
                    base_element = getattr(imported_module, element_info['element_name'])
                    custom_element = PipelineElement(name=element_info['element_name'], base_element=base_element(),
                                                     hyperparameters=element_info['hyperparameters'],
                                                     test_disabled=element_info['test_disabled'],
                                                     disabled=element_info['disabled'])
                    custom_element.base_element.load(os.path.join(folder, element_info['filename']))
                    element_list.append((element_info['element_name'], custom_element))
                else:

                    loaded_pipeline_element = joblib.load(os.path.join(folder, element_info['filename'] + '.pkl'))

                    # This is only for compatibility with older versions
                    if not hasattr(loaded_pipeline_element, 'needs_y'):
                        if hasattr(loaded_pipeline_element.base_element, 'needs_y'):
                            loaded_pipeline_element.needs_y = loaded_pipeline_element.base_element.needs_y
                        else:
                            loaded_pipeline_element.needs_y = False
                    if not hasattr(loaded_pipeline_element, 'needs_covariates'):
                        if hasattr(loaded_pipeline_element.base_element, 'needs_covariates'):
                            loaded_pipeline_element.needs_covariates = loaded_pipeline_element.base_element.needs_covariates
                        else:
                            loaded_pipeline_element.needs_covariates = False

                    loaded_pipeline_element.is_transformer = hasattr(loaded_pipeline_element.base_element, "transform")
                    loaded_pipeline_element.is_estimator = hasattr(loaded_pipeline_element.base_element, "predict")

                    element_list.append((element_info['element_name'], loaded_pipeline_element))

            # delete unpacked folder to clean up
            # ToDo: Don't unpack at all, but use PHOTON file directly
            from shutil import rmtree
            rmtree(folder, ignore_errors=True)

        return PhotonPipeline(element_list)
